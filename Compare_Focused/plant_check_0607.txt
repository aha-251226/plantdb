import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
import networkx as nx
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import RGCNConv, SAGEConv, GATConv, HeteroConv
from torch_geometric.data import Data, HeteroData
import numpy as np
import pdfplumber
import re
import json
import sqlite3
import redis
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import rdflib
from rdflib import Graph, Namespace, URIRef
import logging
from typing import Dict, List, Tuple, Any, Optional
import io
from datetime import datetime
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
import psycopg2
from sqlalchemy import create_engine, text
import pickle
# FAISS 선택적 임포트
try:
    import faiss
    FAISS_AVAILABLE = True
    # GPU 사용 가능한지 확인
    try:
        faiss.StandardGpuResources()
        FAISS_GPU_AVAILABLE = True
    except:
        FAISS_GPU_AVAILABLE = False
except ImportError:
    FAISS_AVAILABLE = False
    FAISS_GPU_AVAILABLE = False
    logger.warning("⚠️ FAISS not available. Vector search will be slower.")
import threading
from queue import Queue
import multiprocessing as mp
from functools import lru_cache
import hashlib
import os
from pathlib import Path

# GPU 가속 설정
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
torch.backends.cudnn.benchmark = True

# 페이지 설정
st.set_page_config(
    page_title="Enterprise Multi-GNN Knowledge Graph Analyzer",
    page_icon="🚀",
    layout="wide",
    initial_sidebar_state="expanded"
)

# 로깅 설정
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatabaseManager:
    """통합 데이터베이스 관리자 (PostgreSQL + Redis + FAISS)"""
    
    def __init__(self, db_config: Dict[str, str]):
        self.db_config = db_config
        self.pg_engine = None
        self.redis_client = None
        self.faiss_index = None
        self.vector_dimension = 384
        self._init_connections()
    
    def _init_connections(self):
        """데이터베이스 연결 초기화"""
        try:
            # PostgreSQL 연결
            pg_url = f"postgresql://{self.db_config['pg_user']}:{self.db_config['pg_password']}@{self.db_config['pg_host']}:{self.db_config['pg_port']}/{self.db_config['pg_database']}"
            self.pg_engine = create_engine(pg_url, pool_size=10, max_overflow=20)
            
            # Redis 연결
            self.redis_client = redis.Redis(
                host=self.db_config.get('redis_host', 'localhost'),
                port=self.db_config.get('redis_port', 6379),
                db=0,
                decode_responses=True
            )
            
        try:
            # FAISS 인덱스 초기화 (사용 가능한 경우만)
            if FAISS_AVAILABLE:
                if FAISS_GPU_AVAILABLE:
                    # GPU FAISS 사용
                    res = faiss.StandardGpuResources()
                    self.faiss_index = faiss.GpuIndexFlatIP(res, self.vector_dimension)
                    logger.info("✅ FAISS-GPU initialized")
                else:
                    # CPU FAISS 사용
                    self.faiss_index = faiss.IndexFlatIP(self.vector_dimension)
                    logger.info("✅ FAISS-CPU initialized")
            else:
                self.faiss_index = None
                logger.info("ℹ️ FAISS not available, using alternative vector search")
            
            self._create_tables()
            logger.info("✅ Database connections established")
            
        except Exception as e:
            logger.error(f"❌ Database connection failed: {e}")
            # 로컬 SQLite 폴백
            self.pg_engine = create_engine("sqlite:///knowledge_graph.db")
            self._create_tables()
    
    def _create_tables(self):
        """테이블 생성"""
        with self.pg_engine.connect() as conn:
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS documents (
                    id SERIAL PRIMARY KEY,
                    filename VARCHAR(255) NOT NULL,
                    content_hash VARCHAR(64) UNIQUE,
                    fields JSONB,
                    metadata JSONB,
                    quality_score FLOAT,
                    processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    embedding_vector BYTEA
                )
            """))
            
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS field_mappings (
                    id SERIAL PRIMARY KEY,
                    source_field VARCHAR(255),
                    target_field VARCHAR(255),
                    similarity_score FLOAT,
                    mapping_type VARCHAR(50),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """))
            
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS gnn_models (
                    id SERIAL PRIMARY KEY,
                    model_name VARCHAR(100),
                    model_data BYTEA,
                    hyperparameters JSONB,
                    performance_metrics JSONB,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """))
            
            conn.commit()
    
    def save_document(self, doc_data: Dict) -> int:
        """문서 저장"""
        content_hash = hashlib.sha256(str(doc_data['fields']).encode()).hexdigest()
        
        with self.pg_engine.connect() as conn:
            result = conn.execute(text("""
                INSERT INTO documents (filename, content_hash, fields, metadata, quality_score, embedding_vector)
                VALUES (:filename, :hash, :fields, :metadata, :quality, :embedding)
                ON CONFLICT (content_hash) DO UPDATE SET
                    processed_at = CURRENT_TIMESTAMP
                RETURNING id
            """), {
                'filename': doc_data['filename'],
                'hash': content_hash,
                'fields': json.dumps(doc_data['fields']),
                'metadata': json.dumps(doc_data.get('metadata', {})),
                'quality': doc_data['quality_score'],
                'embedding': pickle.dumps(doc_data.get('embedding', []))
            })
            conn.commit()
            return result.fetchone()[0]
    
    def get_all_documents(self) -> List[Dict]:
        """모든 문서 조회"""
        with self.pg_engine.connect() as conn:
            result = conn.execute(text("SELECT * FROM documents ORDER BY processed_at DESC"))
            return [dict(row._mapping) for row in result]
    
    def cache_embeddings(self, key: str, embeddings: np.ndarray, ttl: int = 3600):
        """임베딩 캐싱"""
        if self.redis_client:
            self.redis_client.setex(key, ttl, pickle.dumps(embeddings))
    
    def get_cached_embeddings(self, key: str) -> Optional[np.ndarray]:
        """캐시된 임베딩 조회"""
        if self.redis_client:
            data = self.redis_client.get(key)
            if data:
                return pickle.loads(data)
        return None

class EnhancedOntologyManager:
    """향상된 온톨로지 관리자 - 자동 업데이트 지원"""
    
    def __init__(self, ontology_path: str = "ontology.ttl", auto_update: bool = True):
        self.graph = Graph()
        self.ontology_path = ontology_path
        self.field_mappings = {}
        self.domain_embeddings = {}
        self.auto_update = auto_update
        self.update_queue = Queue()
        self.load_ontology()
        
        if auto_update:
            self._start_update_thread()
    
    def load_ontology(self):
        """온톨로지 로드 및 도메인 특화 임베딩 구축"""
        try:
            self.graph.parse(self.ontology_path, format="ttl")
            self._build_field_mappings()
            self._build_domain_embeddings()
            logger.info(f"✅ Enhanced ontology loaded: {len(self.graph)} triples")
        except Exception as e:
            logger.warning(f"⚠️ Ontology file not found: {e}")
            self._create_enhanced_ontology()
    
    def _build_domain_embeddings(self):
        """도메인 특화 임베딩 구축"""
        engineering_terms = [
            "pressure", "temperature", "flow rate", "capacity", "diameter",
            "design pressure", "operating pressure", "maximum pressure",
            "design temperature", "operating temperature", "maximum temperature",
            "centrifugal pump", "heat exchanger", "reactor", "vessel",
            "piping", "instrumentation", "control valve", "safety valve"
        ]
        
        # 도메인 특화 SentenceTransformer 파인튜닝 (시뮬레이션)
        model = SentenceTransformer('all-MiniLM-L6-v2')
        for term in engineering_terms:
            embedding = model.encode([term])[0]
            self.domain_embeddings[term.lower()] = embedding
    
    def _create_enhanced_ontology(self):
        """향상된 기본 온톨로지 생성"""
        enhanced_mappings = {
            # 압력 관련
            'pressure': 'eng:Pressure',
            'press': 'eng:Pressure',
            'design pressure': 'eng:DesignPressure',
            'design press': 'eng:DesignPressure',
            'operating pressure': 'eng:OperatingPressure',
            'oper pressure': 'eng:OperatingPressure',
            'max pressure': 'eng:MaximumPressure',
            'maximum pressure': 'eng:MaximumPressure',
            
            # 온도 관련
            'temperature': 'eng:Temperature',
            'temp': 'eng:Temperature',
            'design temperature': 'eng:DesignTemperature',
            'design temp': 'eng:DesignTemperature',
            'operating temperature': 'eng:OperatingTemperature',
            'oper temperature': 'eng:OperatingTemperature',
            'max temperature': 'eng:MaximumTemperature',
            'maximum temperature': 'eng:MaximumTemperature',
            
            # 유량 관련
            'flow': 'eng:FlowRate',
            'flow rate': 'eng:FlowRate',
            'capacity': 'eng:Capacity',
            'volume': 'eng:Volume',
            
            # 치수 관련
            'diameter': 'eng:Diameter',
            'length': 'eng:Length',
            'height': 'eng:Height',
            'width': 'eng:Width'
        }
        self.field_mappings = enhanced_mappings
    
    def _start_update_thread(self):
        """자동 업데이트 스레드 시작"""
        def update_worker():
            while True:
                try:
                    update_data = self.update_queue.get(timeout=60)
                    self._process_ontology_update(update_data)
                    self.update_queue.task_done()
                except:
                    continue
        
        update_thread = threading.Thread(target=update_worker, daemon=True)
        update_thread.start()
    
    def _process_ontology_update(self, update_data: Dict):
        """온톨로지 자동 업데이트 처리"""
        new_field = update_data.get('field_name', '').lower()
        standard_field = update_data.get('standard_field')
        
        if new_field and standard_field:
            self.field_mappings[new_field] = standard_field
            logger.info(f"🔄 Ontology updated: {new_field} → {standard_field}")
    
    def find_standard_field(self, field_name: str, use_similarity: bool = True) -> Tuple[str, float]:
        """표준 필드명 찾기 (유사도 포함)"""
        normalized = field_name.lower().strip()
        
        # 직접 매칭
        if normalized in self.field_mappings:
            return self.field_mappings[normalized], 1.0
        
        # 유사도 기반 매칭
        if use_similarity and self.domain_embeddings:
            model = SentenceTransformer('all-MiniLM-L6-v2')
            field_embedding = model.encode([normalized])[0]
            
            best_match = field_name
            best_score = 0.0
            
            for term, term_embedding in self.domain_embeddings.items():
                similarity = cosine_similarity([field_embedding], [term_embedding])[0][0]
                if similarity > best_score and similarity > 0.7:
                    best_score = similarity
                    best_match = self.field_mappings.get(term, term)
            
            return best_match, best_score
        
        return field_name, 0.0

class AdaptiveMultiGNN(nn.Module):
    """적응형 Multi-GNN - 동적 임계값 및 멀티모달 지원"""
    
    def __init__(self, num_nodes, num_relations, hidden_dim=128, out_dim=64, num_heads=8):
        super(AdaptiveMultiGNN, self).__init__()
        self.hidden_dim = hidden_dim
        self.out_dim = out_dim
        self.num_heads = num_heads
        
        # Multi-scale GNN layers
        self.rgcn = RGCNConv(num_nodes, hidden_dim, num_relations)
        self.sage = SAGEConv(hidden_dim, hidden_dim)
        self.gat = GATConv(hidden_dim, out_dim, heads=num_heads, concat=False)
        
        # Heterogeneous graph support
        self.hetero_conv = HeteroConv({
            ('document', 'has_field', 'field'): SAGEConv((-1, -1), hidden_dim),
            ('field', 'has_value', 'value'): SAGEConv((-1, -1), hidden_dim),
            ('field', 'similar_to', 'field'): GATConv((-1, -1), hidden_dim, heads=4, concat=False)
        })
        
        # Adaptive threshold network
        self.threshold_net = nn.Sequential(
            nn.Linear(out_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
        # Classification heads
        self.document_classifier = nn.Linear(out_dim, num_nodes)
        self.field_classifier = nn.Linear(out_dim, num_nodes)
        self.similarity_predictor = nn.Linear(out_dim * 2, 1)
        
        # Dropout for regularization
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x, edge_index, edge_type, node_types=None):
        # Multi-scale feature extraction
        x1 = F.relu(self.rgcn(x, edge_index, edge_type))
        x1 = self.dropout(x1)
        
        x2 = F.relu(self.sage(x1, edge_index))
        x2 = self.dropout(x2)
        
        x3 = self.gat(x2, edge_index)
        
        return x3
    
    def compute_adaptive_threshold(self, emb1, emb2):
        """동적 임계값 계산"""
        combined = torch.cat([emb1, emb2], dim=-1)
        threshold = self.threshold_net(combined)
        return threshold
    
    def predict_similarity(self, emb1, emb2):
        """유사도 예측"""
        combined = torch.cat([emb1, emb2], dim=-1)
        similarity = torch.sigmoid(self.similarity_predictor(combined))
        return similarity

class AdvancedSemanticMatcher:
    """고급 시맨틱 매칭 엔진 - 동적 임계값, 도메인 특화"""
    
    def __init__(self, ontology_manager: EnhancedOntologyManager, db_manager: DatabaseManager):
        self.ontology = ontology_manager
        self.db_manager = db_manager
        self.domain_model = None
        self.general_model = None
        self.field_embeddings_cache = {}
        self._load_models()
    
    @st.cache_resource
    def _load_models(_self):
        """모델 로드 (캐시됨)"""
        # 도메인 특화 모델 (엔지니어링)
        domain_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # 일반 모델
        general_model = SentenceTransformer('all-mpnet-base-v2')
        
        return domain_model, general_model
    
    def match_fields_advanced(self, field1: str, field2: str, 
                            gnn_embeddings=None, context: Dict = None) -> Dict[str, float]:
        """고급 필드 매칭 - 다중 점수 반환"""
        
        if self.domain_model is None or self.general_model is None:
            self.domain_model, self.general_model = self._load_models()
        
        results = {
            'ontology_score': 0.0,
            'domain_semantic_score': 0.0,
            'general_semantic_score': 0.0,
            'gnn_score': 0.0,
            'context_score': 0.0,
            'final_score': 0.0,
            'confidence': 0.0
        }
        
        # Level 1: Ontology 매칭
        std_field1, conf1 = self.ontology.find_standard_field(field1)
        std_field2, conf2 = self.ontology.find_standard_field(field2)
        
        if std_field1 == std_field2 and conf1 > 0.8 and conf2 > 0.8:
            results['ontology_score'] = 1.0
        
        # Level 2: 도메인 특화 임베딩 매칭
        try:
            # 캐시된 임베딩 확인
            cache_key1 = f"domain_emb_{hashlib.md5(field1.encode()).hexdigest()}"
            cache_key2 = f"domain_emb_{hashlib.md5(field2.encode()).hexdigest()}"
            
            emb1 = self.db_manager.get_cached_embeddings(cache_key1)
            emb2 = self.db_manager.get_cached_embeddings(cache_key2)
            
            if emb1 is None:
                emb1 = self.domain_model.encode([field1])[0]
                self.db_manager.cache_embeddings(cache_key1, emb1)
            
            if emb2 is None:
                emb2 = self.domain_model.encode([field2])[0]
                self.db_manager.cache_embeddings(cache_key2, emb2)
            
            results['domain_semantic_score'] = cosine_similarity([emb1], [emb2])[0][0]
            
        except Exception as e:
            logger.warning(f"Domain embedding failed: {e}")
        
        # Level 3: 일반 임베딩 매칭
        try:
            gen_emb1 = self.general_model.encode([field1])[0]
            gen_emb2 = self.general_model.encode([field2])[0]
            results['general_semantic_score'] = cosine_similarity([gen_emb1], [gen_emb2])[0][0]
        except Exception as e:
            logger.warning(f"General embedding failed: {e}")
        
        # Level 4: GNN 임베딩 매칭
        if gnn_embeddings is not None:
            try:
                # GNN 임베딩을 이용한 유사도 계산 (구현 예정)
                results['gnn_score'] = 0.5  # 플레이스홀더
            except Exception as e:
                logger.warning(f"GNN embedding failed: {e}")
        
        # Level 5: 컨텍스트 매칭
        if context:
            try:
                # 문서 내 위치, 주변 필드 등을 고려한 점수
                results['context_score'] = self._calculate_context_score(field1, field2, context)
            except Exception as e:
                logger.warning(f"Context matching failed: {e}")
        
        # 동적 가중치 계산 및 최종 점수
        weights = self._calculate_dynamic_weights(results)
        final_score = (
            weights['ontology'] * results['ontology_score'] +
            weights['domain'] * results['domain_semantic_score'] +
            weights['general'] * results['general_semantic_score'] +
            weights['gnn'] * results['gnn_score'] +
            weights['context'] * results['context_score']
        )
        
        results['final_score'] = final_score
        results['confidence'] = self._calculate_confidence(results)
        
        return results
    
    def _calculate_dynamic_weights(self, scores: Dict[str, float]) -> Dict[str, float]:
        """동적 가중치 계산"""
        base_weights = {
            'ontology': 0.4,
            'domain': 0.3,
            'general': 0.15,
            'gnn': 0.1,
            'context': 0.05
        }
        
        # 온톨로지 점수가 높으면 가중치 증가
        if scores['ontology_score'] > 0.9:
            base_weights['ontology'] = 0.6
            base_weights['domain'] = 0.2
            base_weights['general'] = 0.1
        
        # 도메인 점수가 높으면 가중치 증가
        elif scores['domain_semantic_score'] > 0.8:
            base_weights['domain'] = 0.4
            base_weights['ontology'] = 0.3
        
        return base_weights
    
    def _calculate_context_score(self, field1: str, field2: str, context: Dict) -> float:
        """컨텍스트 기반 점수 계산"""
        score = 0.0
        
        # 문서 내 위치 유사성
        pos1 = context.get('position1', {})
        pos2 = context.get('position2', {})
        
        if pos1 and pos2:
            # 같은 섹션에 있으면 점수 증가
            if pos1.get('section') == pos2.get('section'):
                score += 0.3
            
            # 비슷한 행 번호면 점수 증가
            row_diff = abs(pos1.get('row', 0) - pos2.get('row', 0))
            if row_diff <= 2:
                score += 0.2
        
        return min(score, 1.0)
    
    def _calculate_confidence(self, scores: Dict[str, float]) -> float:
        """신뢰도 계산"""
        # 여러 점수가 일치할수록 신뢰도 증가
        score_values = [
            scores['ontology_score'],
            scores['domain_semantic_score'],
            scores['general_semantic_score'],
            scores['gnn_score']
        ]
        
        # 표준편차가 낮을수록 신뢰도 높음
        std_dev = np.std([s for s in score_values if s > 0])
        confidence = max(0.0, 1.0 - std_dev)
        
        return confidence

class RealTimeProcessor:
    """실시간 처리 엔진"""
    
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.processing_queue = Queue()
        self.result_cache = {}
        
    async def process_document_async(self, pdf_file, filename: str) -> Dict[str, Any]:
        """비동기 문서 처리"""
        loop = asyncio.get_event_loop()
        
        # CPU 집약적 작업을 별도 스레드에서 실행
        result = await loop.run_in_executor(
            self.executor,
            self._process_document_sync,
            pdf_file,
            filename
        )
        
        return result
    
    def _process_document_sync(self, pdf_file, filename: str) -> Dict[str, Any]:
        """동기 문서 처리 (내부용)"""
        # 기존 PDF 처리 로직
        processor = AdvancedPDFProcessor()
        return processor.extract_text_and_tables(pdf_file)

class AdvancedPDFProcessor:
    """고급 PDF 처리기 - 멀티프로세싱, 품질 향상"""
    
    def __init__(self, use_multiprocessing: bool = True):
        self.text_quality_threshold = 0.7
        self.use_multiprocessing = use_multiprocessing
        self.table_detection_confidence = 0.8
    
    def extract_text_and_tables(self, pdf_file) -> Dict[str, Any]:
        """고급 텍스트 및 표 추출"""
        results = {
            'text': '',
            'tables': [],
            'fields': {},
            'quality_score': 0.0,
            'metadata': {},
            'structure_analysis': {}
        }
        
        try:
            with pdfplumber.open(pdf_file) as pdf:
                if self.use_multiprocessing and len(pdf.pages) > 1:
                    # 멀티프로세싱으로 페이지 병렬 처리
                    results = self._process_pages_parallel(pdf.pages)
                else:
                    # 순차 처리
                    results = self._process_pages_sequential(pdf.pages)
                
                # 후처리
                results = self._post_process_results(results)
                
        except Exception as e:
            logger.error(f"❌ Enhanced PDF processing error: {e}")
            
        return results
    
    def _process_pages_parallel(self, pages) -> Dict[str, Any]:
        """병렬 페이지 처리"""
        with mp.Pool(processes=min(4, len(pages))) as pool:
            page_results = pool.map(self._process_single_page, 
                                   [(i, page) for i, page in enumerate(pages)])
        
        # 결과 병합
        return self._merge_page_results(page_results)
    
    def _process_pages_sequential(self, pages) -> Dict[str, Any]:
        """순차 페이지 처리"""
        page_results = []
        for i, page in enumerate(pages):
            result = self._process_single_page((i, page))
            page_results.append(result)
        
        return self._merge_page_results(page_results)
    
    def _process_single_page(self, page_data) -> Dict[str, Any]:
        """단일 페이지 처리"""
        page_num, page = page_data
        
        # 텍스트 추출
        page_text = page.extract_text() or ""
        
        # 표 추출 - 향상된 알고리즘
        tables = self._extract_tables_enhanced(page)
        
        # 필드 추출
        fields = self._extract_fields_enhanced(page_text, tables)
        
        return {
            'page_num': page_num,
            'text': page_text,
            'tables': tables,
            'fields': fields
        }
    
    def _extract_tables_enhanced(self, page) -> List[Dict[str, Any]]:
        """향상된 표 추출"""
        tables = []
        
        # 기본 표 추출
        raw_tables = page.extract_tables()
        
        for table in raw_tables:
            if not table or len(table) < 2:
                continue
            
            processed_table = self._analyze_table_structure(table)
            if processed_table['confidence'] >= self.table_detection_confidence:
                tables.append(processed_table)
        
        # 텍스트 기반 표 감지
        text_tables = self._detect_text_tables(page.extract_text())
        tables.extend(text_tables)
        
        return tables
    
    def _analyze_table_structure(self, table: List[List[str]]) -> Dict[str, Any]:
        """표 구조 분석"""
        if not table or len(table) < 2:
            return {'confidence': 0.0}
        
        # 헤더 분석
        headers = [cell.strip() if cell else "" for cell in table[0]]
        data_rows = table[1:]
        
        # 표 품질 평가
        confidence = self._evaluate_table_quality(headers, data_rows)
        
        # 필드-값 쌍 추출
        field_value_pairs = {}
        structure_type = "unknown"
        
        if confidence >= 0.7:
            if len(headers) == 2 and any("field" in h.lower() or "name" in h.lower() for h in headers):
                # 필드-값 테이블
                structure_type = "field_value_pairs"
                field_value_pairs = self._extract_field_value_pairs(data_rows)
            
            elif any(keyword in ' '.join(headers).lower() 
                    for keyword in ['pressure', 'temperature', 'flow', 'capacity']):
                # 엔지니어링 사양 테이블
                structure_type = "engineering_specification"
                field_value_pairs = self._extract_engineering_fields(headers, data_rows)
        
        return {
            'headers': headers,
            'data': data_rows,
            'field_value_pairs': field_value_pairs,
            'structure_type': structure_type,
            'confidence': confidence,
            'quality_metrics': self._calculate_table_metrics(headers, data_rows)
        }
    
    def _evaluate_table_quality(self, headers: List[str], data_rows: List[List[str]]) -> float:
        """표 품질 평가"""
        score = 0.0
        
        # 헤더 품질
        if headers and any(headers):
            score += 0.3
        
        # 데이터 일관성
        if data_rows:
            avg_cols = sum(len(row) for row in data_rows) / len(data_rows)
            if avg_cols >= len(headers) * 0.8:  # 80% 이상 채워져 있음
                score += 0.4
        
        # 엔지니어링 키워드 존재
        all_text = ' '.join(headers + [cell for row in data_rows for cell in row if cell])
        engineering_keywords = ['pressure', 'temperature', 'flow', 'capacity', 'design', 'operating']
        if any(keyword in all_text.lower() for keyword in engineering_keywords):
            score += 0.3
        
        return min(score, 1.0)
    
    def _extract_field_value_pairs(self, data_rows: List[List[str]]) -> Dict[str, str]:
        """필드-값 쌍 추출"""
        pairs = {}
        
        for row in data_rows:
            if len(row) >= 2 and row[0] and row[1]:
                field_name = str(row[0]).strip()
                field_value = str(row[1]).strip()
                
                # 필드명 정규화
                field_name = re.sub(r'\s+', ' ', field_name)
                field_name = re.sub(r'[^\w\s]', '', field_name)
                
                if len(field_name) > 2 and len(field_value) > 0:
                    pairs[field_name] = field_value
        
        return pairs
    
    def _extract_engineering_fields(self, headers: List[str], data_rows: List[List[str]]) -> Dict[str, str]:
        """엔지니어링 필드 추출"""
        fields = {}
        
        for row in data_rows:
            for i, (header, value) in enumerate(zip(headers, row)):
                if header and value and header.strip() and value.strip():
                    field_key = f"{header.strip()}_{i}"
                    fields[field_key] = value.strip()
        
        return fields
    
    def _merge_page_results(self, page_results: List[Dict]) -> Dict[str, Any]:
        """페이지 결과 병합"""
        merged = {
            'text': '',
            'tables': [],
            'fields': {},
            'quality_score': 0.0,
            'metadata': {},
            'structure_analysis': {}
        }
        
        for result in page_results:
            merged['text'] += result['text'] + "\n"
            merged['tables'].extend(result['tables'])
            merged['fields'].update(result['fields'])
        
        # 품질 점수 계산
        merged['quality_score'] = self._calculate_overall_quality(merged)
        
        # 메타데이터 생성
        merged['metadata'] = {
            'total_pages': len(page_results),
            'total_tables': len(merged['tables']),
            'total_fields': len(merged['fields']),
            'processing_method': 'parallel' if self.use_multiprocessing else 'sequential'
        }
        
        return merged
    
    def _calculate_overall_quality(self, results: Dict) -> float:
        """전체 품질 점수 계산"""
        base_score = 0.2
        
        # 텍스트 품질
        if results['text'] and len(results['text']) > 100:
            base_score += 0.3
        
        # 표 품질
        if results['tables']:
            table_scores = [t.get('confidence', 0) for t in results['tables']]
            avg_table_quality = sum(table_scores) / len(table_scores)
            base_score += 0.3 * avg_table_quality
        
        # 필드 추출 품질
        if results['fields']:
            field_count = len(results['fields'])
            field_score = min(field_count * 0.02, 0.2)
            base_score += field_score
        
        return min(base_score, 1.0)

class EnterpriseAnalyzer:
    """기업용 통합 분석기"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # 데이터베이스 관리자 초기화
        self.db_manager = DatabaseManager(config.get('database', {}))
        
        # 향상된 컴포넌트 초기화
        self.ontology_manager = EnhancedOntologyManager(
            config.get('ontology_path', 'ontology.ttl'),
            auto_update=config.get('auto_update_ontology', True)
        )
        
        self.semantic_matcher = AdvancedSemanticMatcher(
            self.ontology_manager, 
            self.db_manager
        )
        
        self.pdf_processor = AdvancedPDFProcessor(
            use_multiprocessing=config.get('use_multiprocessing', True)
        )
        
        self.realtime_processor = RealTimeProcessor(
            max_workers=config.get('max_workers', 4)
        )
        
        # GNN 모델
        self.gnn_model = None
        self.node_embeddings = None
        self.training_history = []
        
        # 지식 그래프
        self.knowledge_graph = nx.DiGraph()
        
        logger.info("🚀 Enterprise Multi-GNN Knowledge Graph Analyzer initialized")
        logger.info(f"✅ GPU available: {torch.cuda.is_available()}")
        logger.info(f"✅ Device: {device}")
    
    async def process_document_enterprise(self, pdf_file, filename: str) -> Dict[str, Any]:
        """기업용 문서 처리"""
        # 실시간 비동기 처리
        extraction_result = await self.realtime_processor.process_document_async(pdf_file, filename)
        
        # 데이터베이스 저장
        doc_data = {
            'filename': filename,
            'fields': extraction_result['fields'],
            'metadata': extraction_result.get('metadata', {}),
            'quality_score': extraction_result['quality_score']
        }
        
        doc_id = self.db_manager.save_document(doc_data)
        
        # 지식 그래프 업데이트
        self._update_knowledge_graph_enterprise(doc_id, extraction_result['fields'])
        
        logger.info(f"📄 Document processed: {filename} (ID: {doc_id})")
        
        return {
            'doc_id': doc_id,
            'extraction_result': extraction_result,
            'graph_stats': {
                'nodes': len(self.knowledge_graph.nodes),
                'edges': len(self.knowledge_graph.edges)
            }
        }
    
    def _update_knowledge_graph_enterprise(self, doc_id: int, fields: Dict[str, str]):
        """기업용 지식 그래프 업데이트"""
        doc_node = f"doc_{doc_id}"
        self.knowledge_graph.add_node(doc_node, type="document", doc_id=doc_id)
        
        for field_name, field_value in fields.items():
            # 표준화된 필드명 사용
            std_field, confidence = self.ontology_manager.find_standard_field(field_name)
            
            field_node = f"field_{std_field}_{doc_id}"
            value_node = f"value_{hashlib.md5(field_value.encode()).hexdigest()}"
            
            # 노드 추가
            self.knowledge_graph.add_node(
                field_node, 
                type="field", 
                name=std_field, 
                original_name=field_name,
                confidence=confidence
            )
            
            self.knowledge_graph.add_node(
                value_node, 
                type="value", 
                value=field_value,
                data_type=self._detect_data_type(field_value)
            )
            
            # 엣지 추가
            self.knowledge_graph.add_edge(doc_node, field_node, relation="has_field")
            self.knowledge_graph.add_edge(field_node, value_node, relation="has_value")
            
            # 필드 간 유사도 엣지 추가
            self._add_similarity_edges(field_node, std_field)
    
    def _detect_data_type(self, value: str) -> str:
        """데이터 타입 감지"""
        value = value.strip()
        
        # 숫자 패턴
        if re.match(r'^-?\d+\.?\d*$', value):
            return "numeric"
        
        # 단위가 있는 숫자
        if re.match(r'^-?\d+\.?\d*\s*[a-zA-Z]+', value):
            return "measurement"
        
        # 날짜 패턴
        if re.match(r'\d{4}-\d{2}-\d{2}', value):
            return "date"
        
        # 카테고리 (대문자로 시작하는 짧은 텍스트)
        if len(value) < 50 and value[0].isupper():
            return "category"
        
        return "text"
    
    def _add_similarity_edges(self, new_field_node: str, std_field: str):
        """유사도 기반 엣지 추가"""
        for node in self.knowledge_graph.nodes():
            if (node.startswith("field_") and 
                node != new_field_node and 
                self.knowledge_graph.nodes[node].get('type') == 'field'):
                
                other_std_field = self.knowledge_graph.nodes[node].get('name', '')
                
                # 같은 표준 필드면 유사도 엣지 추가
                if std_field == other_std_field:
                    self.knowledge_graph.add_edge(
                        new_field_node, 
                        node, 
                        relation="similar_to",
                        similarity=1.0
                    )
    
    def train_adaptive_gnn(self, hyperparameters: Dict = None) -> Dict[str, Any]:
        """적응형 GNN 훈련"""
        if len(self.knowledge_graph.nodes) < 10:
            return {"error": "Need at least 10 nodes for enterprise GNN training"}
        
        # 기본 하이퍼파라미터
        default_hyperparams = {
            'hidden_dim': 128,
            'out_dim': 64,
            'num_heads': 8,
            'learning_rate': 0.001,
            'epochs': 200,
            'batch_size': 32,
            'dropout': 0.3
        }
        
        if hyperparameters:
            default_hyperparams.update(hyperparameters)
        
        logger.info("🤖 Training Adaptive Multi-GNN model...")
        
        # 그래프 데이터 준비
        nodes = list(self.knowledge_graph.nodes())
        edges = list(self.knowledge_graph.edges())
        
        # GPU로 이동
        node_to_idx = {node: idx for idx, node in enumerate(nodes)}
        edge_index = torch.tensor(
            [[node_to_idx[e[0]], node_to_idx[e[1]]] for e in edges], 
            device=device
        ).t()
        
        # 관계 타입 매핑
        relation_types = list(set(nx.get_edge_attributes(self.knowledge_graph, 'relation').values()))
        relation_to_idx = {rel: idx for idx, rel in enumerate(relation_types)}
        
        edge_type = torch.tensor([
            relation_to_idx.get(
                self.knowledge_graph[e[0]][e[1]].get('relation', 'unknown'), 0
            ) for e in edges
        ], device=device)
        
        # 초기 노드 특성
        num_nodes = len(nodes)
        x = torch.eye(num_nodes, device=device)
        
        # 적응형 Multi-GNN 모델 초기화
        self.gnn_model = AdaptiveMultiGNN(
            num_nodes=num_nodes,
            num_relations=len(relation_types),
            hidden_dim=default_hyperparams['hidden_dim'],
            out_dim=default_hyperparams['out_dim'],
            num_heads=default_hyperparams['num_heads']
        ).to(device)
        
        # 옵티마이저 및 스케줄러
        optimizer = torch.optim.AdamW(
            self.gnn_model.parameters(), 
            lr=default_hyperparams['learning_rate'],
            weight_decay=1e-4
        )
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=default_hyperparams['epochs']
        )
        
        # 훈련 루프
        self.gnn_model.train()
        training_losses = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for epoch in range(default_hyperparams['epochs']):
            optimizer.zero_grad()
            
            # Forward pass
            embeddings = self.gnn_model(x, edge_index, edge_type)
            
            # 다중 손실 함수
            # 1. 재구성 손실
            reconstructed = self.gnn_model.document_classifier(embeddings)
            reconstruction_loss = F.mse_loss(reconstructed, x)
            
            # 2. 대조 학습 손실 (유사한 필드는 가깝게, 다른 필드는 멀게)
            contrastive_loss = self._compute_contrastive_loss(embeddings, nodes)
            
            # 3. 정규화 손실
            reg_loss = sum(p.pow(2.0).sum() for p in self.gnn_model.parameters())
            
            # 총 손실
            total_loss = (reconstruction_loss + 
                         0.1 * contrastive_loss + 
                         1e-5 * reg_loss)
            
            total_loss.backward()
            
            # 그래디언트 클리핑
            torch.nn.utils.clip_grad_norm_(self.gnn_model.parameters(), max_norm=1.0)
            
            optimizer.step()
            scheduler.step()
            
            training_losses.append(total_loss.item())
            
            # 진행률 업데이트
            progress_bar.progress((epoch + 1) / default_hyperparams['epochs'])
            if epoch % 20 == 0:
                status_text.text(f"Epoch {epoch}/{default_hyperparams['epochs']}, "
                               f"Loss: {total_loss.item():.4f}, "
                               f"LR: {scheduler.get_last_lr()[0]:.6f}")
        
        # 학습된 임베딩 저장
        self.gnn_model.eval()
        with torch.no_grad():
            self.node_embeddings = self.gnn_model(x, edge_index, edge_type).cpu().numpy()
        
        # 모델 저장
        model_data = {
            'model_state': self.gnn_model.state_dict(),
            'hyperparameters': default_hyperparams,
            'node_mapping': node_to_idx,
            'relation_mapping': relation_to_idx
        }
        
        # 데이터베이스에 모델 저장
        with self.db_manager.pg_engine.connect() as conn:
            conn.execute(text("""
                INSERT INTO gnn_models (model_name, model_data, hyperparameters, performance_metrics)
                VALUES (:name, :data, :hyperparams, :metrics)
            """), {
                'name': f"adaptive_gnn_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                'data': pickle.dumps(model_data),
                'hyperparams': json.dumps(default_hyperparams),
                'metrics': json.dumps({
                    'final_loss': training_losses[-1],
                    'training_losses': training_losses[-10:],  # 마지막 10개만 저장
                    'num_epochs': len(training_losses)
                })
            })
            conn.commit()
        
        progress_bar.empty()
        status_text.empty()
        
        self.training_history = training_losses
        
        logger.info("✅ Adaptive Multi-GNN training completed")
        
        return {
            "status": "success",
            "num_nodes": num_nodes,
            "num_relations": len(relation_types),
            "final_loss": training_losses[-1],
            "training_epochs": len(training_losses),
            "model_saved": True,
            "device": str(device)
        }
    
    def _compute_contrastive_loss(self, embeddings: torch.Tensor, nodes: List[str]) -> torch.Tensor:
        """대조 학습 손실 계산"""
        loss = 0.0
        num_pairs = 0
        
        # 같은 타입의 노드들은 가깝게, 다른 타입은 멀게
        for i, node1 in enumerate(nodes):
            node1_type = self.knowledge_graph.nodes[node1].get('type', 'unknown')
            
            for j, node2 in enumerate(nodes[i+1:], i+1):
                node2_type = self.knowledge_graph.nodes[node2].get('type', 'unknown')
                
                emb1 = embeddings[i]
                emb2 = embeddings[j]
                
                similarity = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0))
                
                if node1_type == node2_type:
                    # 같은 타입: 유사도 높이기
                    loss += (1 - similarity) ** 2
                else:
                    # 다른 타입: 유사도 낮추기
                    loss += similarity ** 2
                
                num_pairs += 1
                
                # 계산량 제한
                if num_pairs > 1000:
                    break
            
            if num_pairs > 1000:
                break
        
        return loss / max(num_pairs, 1)
    
    def compare_documents_enterprise(self) -> Dict[str, Any]:
        """기업용 문서 비교"""
        documents = self.db_manager.get_all_documents()
        
        if len(documents) < 2:
            return {"error": "Need at least 2 documents for comparison"}
        
        logger.info("🔍 Enterprise document comparison with advanced semantic matching...")
        
        comparison_results = {}
        
        for i, doc1 in enumerate(documents):
            for j, doc2 in enumerate(documents[i+1:], i+1):
                doc1_fields = json.loads(doc1['fields']) if isinstance(doc1['fields'], str) else doc1['fields']
                doc2_fields = json.loads(doc2['fields']) if isinstance(doc2['fields'], str) else doc2['fields']
                
                # 고급 비교 수행
                comparison = self._compare_documents_advanced(
                    doc1_fields, doc2_fields, 
                    doc1['filename'], doc2['filename']
                )
                
                comparison_key = f"doc_{doc1['id']}_vs_doc_{doc2['id']}"
                comparison_results[comparison_key] = {
                    'doc1_name': doc1['filename'],
                    'doc2_name': doc2['filename'],
                    'doc1_quality': doc1['quality_score'],
                    'doc2_quality': doc2['quality_score'],
                    'comparison': comparison
                }
                
                # 매핑 결과 저장
                for match in comparison['matched_fields']:
                    self._save_field_mapping(
                        match['field1'], match['field2'], 
                        match['final_score'], match['confidence']
                    )
        
        return comparison_results
    
    def _compare_documents_advanced(self, doc1_fields: Dict, doc2_fields: Dict, 
                                  filename1: str, filename2: str) -> Dict[str, Any]:
        """고급 문서 비교"""
        comparison_result = {
            'matched_fields': [],
            'doc1_unique': [],
            'doc2_unique': [],
            'field_mappings': {},
            'similarity_matrix': {},
            'overall_similarity': 0.0,
            'confidence_scores': {},
            'semantic_clusters': []
        }
        
        # 모든 필드 조합에 대해 고급 유사도 계산
        similarity_matrix = {}
        confidence_matrix = {}
        
        for field1 in doc1_fields:
            for field2 in doc2_fields:
                # 컨텍스트 정보 구성
                context = {
                    'position1': {'document': filename1},
                    'position2': {'document': filename2}
                }
                
                match_result = self.semantic_matcher.match_fields_advanced(
                    field1, field2, self.node_embeddings, context
                )
                
                similarity_matrix[(field1, field2)] = match_result['final_score']
                confidence_matrix[(field1, field2)] = match_result['confidence']
        
        # 동적 임계값 계산
        all_scores = list(similarity_matrix.values())
        dynamic_threshold = np.percentile(all_scores, 75) if all_scores else 0.7
        
        # 최적 매칭 찾기 (헝가리안 알고리즘 근사)
        used_fields2 = set()
        
        # 높은 점수부터 매칭
        sorted_pairs = sorted(similarity_matrix.items(), key=lambda x: x[1], reverse=True)
        
        for (field1, field2), score in sorted_pairs:
            if (field1 not in comparison_result['field_mappings'] and 
                field2 not in used_fields2 and 
                score >= dynamic_threshold):
                
                confidence = confidence_matrix[(field1, field2)]
                
                comparison_result['matched_fields'].append({
                    'field1': field1,
                    'field2': field2,
                    'final_score': score,
                    'confidence': confidence,
                    'value1': doc1_fields[field1],
                    'value2': doc2_fields[field2],
                    'threshold_used': dynamic_threshold
                })
                
                comparison_result['field_mappings'][field1] = field2
                used_fields2.add(field2)
        
        # 매칭되지 않은 필드들
        for field1 in doc1_fields:
            if field1 not in comparison_result['field_mappings']:
                comparison_result['doc1_unique'].append(field1)
        
        for field2 in doc2_fields:
            if field2 not in used_fields2:
                comparison_result['doc2_unique'].append(field2)
        
        # 전체 유사도 계산 (가중평균)
        if comparison_result['matched_fields']:
            weighted_scores = [match['final_score'] * match['confidence'] 
                             for match in comparison_result['matched_fields']]
            total_weight = sum(match['confidence'] for match in comparison_result['matched_fields'])
            
            if total_weight > 0:
                comparison_result['overall_similarity'] = sum(weighted_scores) / total_weight
            
            # Jaccard 유사도로 정규화
            total_fields = len(doc1_fields) + len(doc2_fields) - len(comparison_result['matched_fields'])
            jaccard_factor = len(comparison_result['matched_fields']) / total_fields
            comparison_result['overall_similarity'] *= jaccard_factor
        
        comparison_result['similarity_matrix'] = similarity_matrix
        comparison_result['dynamic_threshold'] = dynamic_threshold
        
        return comparison_result
    
    def _save_field_mapping(self, field1: str, field2: str, score: float, confidence: float):
        """필드 매핑 저장"""
        try:
            with self.db_manager.pg_engine.connect() as conn:
                conn.execute(text("""
                    INSERT INTO field_mappings (source_field, target_field, similarity_score, mapping_type)
                    VALUES (:field1, :field2, :score, :type)
                    ON CONFLICT DO NOTHING
                """), {
                    'field1': field1,
                    'field2': field2,
                    'score': score,
                    'type': 'semantic_advanced'
                })
                conn.commit()
        except Exception as e:
            logger.warning(f"Failed to save field mapping: {e}")

# Streamlit 앱 설정
def get_default_config():
    """기본 설정 반환"""
    return {
        'database': {
            'pg_host': os.getenv('PG_HOST', 'localhost'),
            'pg_port': os.getenv('PG_PORT', '5432'),
            'pg_user': os.getenv('PG_USER', 'postgres'),
            'pg_password': os.getenv('PG_PASSWORD', 'password'),
            'pg_database': os.getenv('PG_DATABASE', 'knowledge_graph'),
            'redis_host': os.getenv('REDIS_HOST', 'localhost'),
            'redis_port': os.getenv('REDIS_PORT', '6379')
        },
        'ontology_path': 'ontology.ttl',
        'auto_update_ontology': True,
        'use_multiprocessing': True,
        'max_workers': 4
    }

# 세션 상태 초기화
if 'enterprise_analyzer' not in st.session_state:
    config = get_default_config()
    st.session_state.enterprise_analyzer = EnterpriseAnalyzer(config)
    st.success("✅ Enterprise Multi-GNN Knowledge Graph Analyzer initialized")

# CSS 스타일
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #2E86AB;
        text-align: center;
        margin-bottom: 2rem;
        background: linear-gradient(90deg, #2E86AB, #A23B72);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .enterprise-badge {
        background: linear-gradient(45deg, #FF6B35, #F7931E);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 25px;
        font-weight: bold;
        display: inline-block;
        margin: 1rem 0;
    }
    .feature-card {
        background: #f8f9fa;
        border-left: 4px solid #2E86AB;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.25rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        margin: 0.5rem;
    }
</style>
""", unsafe_allow_html=True)

def main():
    # 헤더
    st.markdown('<h1 class="main-header">🚀 Enterprise Multi-GNN Knowledge Graph Analyzer</h1>', 
                unsafe_allow_html=True)
    
    st.markdown('<div class="enterprise-badge">🏢 Enterprise Edition</div>', 
                unsafe_allow_html=True)
    
    # 시스템 상태 사이드바
    with st.sidebar:
        st.markdown("## 🎯 Enterprise Features")
        
        st.markdown("""
        <div class="feature-card">
            <strong>✅ GPU Acceleration</strong><br>
            CUDA support for training
        </div>
        <div class="feature-card">
            <strong>🗄️ Database Integration</strong><br>
            PostgreSQL + Redis + FAISS
        </div>
        <div class="feature-card">
            <strong>🤖 Adaptive Multi-GNN</strong><br>
            Dynamic thresholds & multimodal
        </div>
        <div class="feature-card">
            <strong>⚡ Real-time Processing</strong><br>
            Async + multiprocessing
        </div>
        <div class="feature-card">
            <strong>🧠 Auto-learning Ontology</strong><br>
            Self-updating knowledge base
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        analyzer = st.session_state.enterprise_analyzer
        
        # 시스템 메트릭
        col1, col2 = st.columns(2)
        with col1:
            st.metric("GPU Available", "✅" if torch.cuda.is_available() else "❌")
            st.metric("Graph Nodes", len(analyzer.knowledge_graph.nodes))
        
        with col2:
            st.metric("Device", str(device))
            st.metric("Graph Edges", len(analyzer.knowledge_graph.edges))
        
        if analyzer.gnn_model is not None:
            st.success("🤖 Adaptive GNN: Trained")
        else:
            st.info("🤖 Adaptive GNN: Ready to train")
    
    # 메인 탭
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "📄 Document Processing", 
        "🤖 AI Training", 
        "🔍 Intelligent Comparison", 
        "🕸️ Graph Visualization",
        "📊 Analytics Dashboard"
    ])
    
    with tab1:
        st.markdown("## 📄 Enterprise Document Processing")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            uploaded_files = st.file_uploader(
                "Upload PDF documents for enterprise analysis",
                type=['pdf'],
                accept_multiple_files=True,
                help="Supports parallel processing and advanced table extraction"
            )
        
        with col2:
            st.markdown("### Processing Options")
            use_multiprocessing = st.checkbox("Enable Multiprocessing", value=True)
            use_gpu = st.checkbox("GPU Acceleration", value=torch.cuda.is_available())
            quality_threshold = st.slider("Quality Threshold", 0.0, 1.0, 0.7)
        
        if uploaded_files:
            for uploaded_file in uploaded_files:
                if st.button(f"🚀 Process {uploaded_file.name}", key=f"process_{uploaded_file.name}"):
                    with st.spinner(f"Enterprise processing: {uploaded_file.name}..."):
                        try:
                            # 비동기 처리 (Streamlit에서는 동기적으로 처리)
                            result = st.session_state.enterprise_analyzer.pdf_processor.extract_text_and_tables(uploaded_file)
                            
                            # 문서 저장
                            doc_data = {
                                'filename': uploaded_file.name,
                                'fields': result['fields'],
                                'metadata': result.get('metadata', {}),
                                'quality_score': result['quality_score']
                            }
                            
                            doc_id = st.session_state.enterprise_analyzer.db_manager.save_document(doc_data)
                            
                            # 지식 그래프 업데이트
                            st.session_state.enterprise_analyzer._update_knowledge_graph_enterprise(doc_id, result['fields'])
                            
                            # 결과 표시
                            st.markdown(f"""
                            <div class="success-box">
                                <h4>✅ {uploaded_file.name} processed successfully</h4>
                                <div style="display: flex; gap: 1rem; margin: 1rem 0;">
                                    <div class="metric-card">
                                        <h3>{result['quality_score']:.2f}</h3>
                                        <p>Quality Score</p>
                                    </div>
                                    <div class="metric-card">
                                        <h3>{len(result['fields'])}</h3>
                                        <p>Fields Extracted</p>
                                    </div>
                                    <div class="metric-card">
                                        <h3>{len(result['tables'])}</h3>
                                        <p>Tables Found</p>
                                    </div>
                                    <div class="metric-card">
                                        <h3>{result['metadata'].get('total_pages', 1)}</h3>
                                        <p>Pages Processed</p>
                                    </div>
                                </div>
                            </div>
                            """, unsafe_allow_html=True)
                            
                            # 상세 결과 표시
                            if result['fields']:
                                with st.expander("📋 Extracted Fields Details"):
                                    fields_df = pd.DataFrame([
                                        {
                                            "Field Name": k, 
                                            "Value": v[:100] + "..." if len(v) > 100 else v,
                                            "Data Type": st.session_state.enterprise_analyzer._detect_data_type(v),
                                            "Standard Field": st.session_state.enterprise_analyzer.ontology_manager.find_standard_field(k)[0]
                                        } 
                                        for k, v in result['fields'].items()
                                    ])
                                    st.dataframe(fields_df, use_container_width=True)
                            
                            # 표 구조 분석 결과
                            if result['tables']:
                                with st.expander("📊 Table Structure Analysis"):
                                    for i, table in enumerate(result['tables']):
                                        st.write(f"**Table {i+1}:**")
                                        st.write(f"- Structure Type: {table.get('structure_type', 'unknown')}")
                                        st.write(f"- Confidence: {table.get('confidence', 0):.2f}")
                                        st.write(f"- Field-Value Pairs: {len(table.get('field_value_pairs', {}))}")
                                        
                                        if table.get('headers'):
                                            st.write(f"- Headers: {', '.join(table['headers'][:5])}...")
                            
                        except Exception as e:
                            st.error(f"❌ Error processing {uploaded_file.name}: {str(e)}")
    
    with tab2:
        st.markdown("## 🤖 Adaptive Multi-GNN Training")
        
        analyzer = st.session_state.enterprise_analyzer
        
        if len(analyzer.knowledge_graph.nodes) < 10:
            st.warning("⚠️ Need at least 10 nodes for enterprise GNN training. Please upload more documents.")
        else:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.info(f"📊 Current graph: {len(analyzer.knowledge_graph.nodes)} nodes, "
                       f"{len(analyzer.knowledge_graph.edges)} edges")
                
                # 하이퍼파라미터 설정
                st.markdown("### 🎛️ Hyperparameters")
                
                col_a, col_b = st.columns(2)
                with col_a:
                    hidden_dim = st.selectbox("Hidden Dimension", [64, 128, 256], index=1)
                    out_dim = st.selectbox("Output Dimension", [32, 64, 128], index=1)
                    num_heads = st.selectbox("Attention Heads", [4, 8, 16], index=1)
                
                with col_b:
                    learning_rate = st.selectbox("Learning Rate", [0.0001, 0.001, 0.01], index=1)
                    epochs = st.slider("Training Epochs", 50, 500, 200)
                    dropout = st.slider("Dropout Rate", 0.1, 0.5, 0.3)
                
                hyperparams = {
                    'hidden_dim': hidden_dim,
                    'out_dim': out_dim,
                    'num_heads': num_heads,
                    'learning_rate': learning_rate,
                    'epochs': epochs,
                    'dropout': dropout
                }
            
            with col2:
                st.markdown("### 🚀 Training Status")
                
                if analyzer.gnn_model is not None:
                    st.success("✅ Model trained")
                    if analyzer.training_history:
                        st.line_chart(pd.DataFrame({
                            'Loss': analyzer.training_history[-50:]  # 최근 50 에포크
                        }))
                else:
                    st.info("🎯 Ready to train")
            
            if st.button("🚀 Train Adaptive Multi-GNN", type="primary"):
                with st.spinner("Training adaptive multi-GNN model..."):
                    result = analyzer.train_adaptive_gnn(hyperparams)
                    
                    if "error" in result:
                        st.error(f"❌ {result['error']}")
                    else:
                        st.markdown(f"""
                        <div class="success-box">
                            <h4>✅ Adaptive Multi-GNN training completed!</h4>
                            <div style="display: flex; gap: 1rem; margin: 1rem 0;">
                                <div class="metric-card">
                                    <h3>{result['num_nodes']}</h3>
                                    <p>Nodes</p>
                                </div>
                                <div class="metric-card">
                                    <h3>{result['num_relations']}</h3>
                                    <p>Relations</p>
                                </div>
                                <div class="metric-card">
                                    <h3>{result['final_loss']:.6f}</h3>
                                    <p>Final Loss</p>
                                </div>
                                <div class="metric-card">
                                    <h3>{result['training_epochs']}</h3>
                                    <p>Epochs</p>
                                </div>
                            </div>
                            <p><strong>Device:</strong> {result['device']}</p>
                            <p><strong>Model Saved:</strong> {result['model_saved']}</p>
                        </div>
                        """, unsafe_allow_html=True)
        
        # 아키텍처 설명
        with st.expander("🧠 Adaptive Multi-GNN Architecture"):
            st.markdown("""
            ### 🔄 Enhanced Pipeline: R-GCN → GraphSAGE → GAT → Adaptive Threshold
            
            **🎯 Key Improvements:**
            
            1. **Adaptive Threshold Network**
               - Dynamic similarity thresholds based on context
               - Learned from training data patterns
               - Reduces false positives in field matching
            
            2. **Multi-scale Feature Extraction**
               - R-GCN: Relation-aware convolution
               - GraphSAGE: Neighborhood aggregation  
               - GAT: Attention-weighted features
            
            3. **Contrastive Learning**
               - Similar fields pushed together in embedding space
               - Different types pushed apart
               - Improves semantic understanding
            
            4. **Regularization & Optimization**
               - Dropout for generalization
               - Gradient clipping for stability
               - Cosine annealing learning rate schedule
            
            5. **GPU Acceleration**
               - Full CUDA support
               - Optimized tensor operations
               - Batch processing capabilities
            """)
    
    with tab3:
        st.markdown("## 🔍 Intelligent Document Comparison")
        
        analyzer = st.session_state.enterprise_analyzer
        
        # 저장된 문서 목록
        documents = analyzer.db_manager.get_all_documents()
        
        if len(documents) < 2:
            st.warning("⚠️ Need at least 2 documents for intelligent comparison.")
        else:
            col1, col2 = st.columns([3, 1])
            
            with col1:
                st.info(f"📚 {len(documents)} documents available for comparison")
                
                # 문서 목록 표시
                with st.expander("📋 Available Documents"):
                    docs_df = pd.DataFrame([
                        {
                            "ID": doc['id'],
                            "Filename": doc['filename'],
                            "Quality Score": f"{doc['quality_score']:.2f}",
                            "Fields Count": len(json.loads(doc['fields']) if isinstance(doc['fields'], str) else doc['fields']),
                            "Processed": doc['processed_at']
                        }
                        for doc in documents
                    ])
                    st.dataframe(docs_df, use_container_width=True)
            
            with col2:
                st.markdown("### ⚙️ Comparison Settings")
                use_gnn_embeddings = st.checkbox("Use GNN Embeddings", value=analyzer.gnn_model is not None)
                include_context = st.checkbox("Include Context Analysis", value=True)
                min_confidence = st.slider("Minimum Confidence", 0.0, 1.0, 0.5)
            
            if st.button("🔍 Start Intelligent Comparison", type="primary"):
                with st.spinner("Performing intelligent document comparison..."):
                    results = analyzer.compare_documents_enterprise()
                    
                    if "error" in results:
                        st.error(f"❌ {results['error']}")
                    else:
                        st.success(f"✅ Compared {len(results)} document pairs")
                        
                        # 전체 통계
                        total_matches = sum(len(comp['comparison']['matched_fields']) for comp in results.values())
                        avg_similarity = np.mean([comp['comparison']['overall_similarity'] for comp in results.values()])
                        
                        st.markdown(f"""
                        <div class="success-box">
                            <h4>📊 Comparison Summary</h4>
                            <div style="display: flex; gap: 1rem; margin: 1rem 0;">
                                <div class="metric-card">
                                    <h3>{len(results)}</h3>
                                    <p>Document Pairs</p>
                                </div>
                                <div class="metric-card">
                                    <h3>{total_matches}</h3>
                                    <p>Total Matches</p>
                                </div>
                                <div class="metric-card">
                                    <h3>{avg_similarity:.3f}</h3>
                                    <p>Avg Similarity</p>
                                </div>
                            </div>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        # 개별 비교 결과
                        for comp_key, comp_data in results.items():
                            comparison = comp_data['comparison']
                            
                            with st.expander(f"📋 {comp_data['doc1_name']} ↔ {comp_data['doc2_name']}"):
                                
                                # 요약 메트릭
                                col_a, col_b, col_c, col_d = st.columns(4)
                                with col_a:
                                    st.metric("Overall Similarity", f"{comparison['overall_similarity']:.3f}")
                                with col_b:
                                    st.metric("Matched Fields", len(comparison['matched_fields']))
                                with col_c:
                                    st.metric("Dynamic Threshold", f"{comparison.get('dynamic_threshold', 0.7):.3f}")
                                with col_d:
                                    unique_count = len(comparison['doc1_unique']) + len(comparison['doc2_unique'])
                                    st.metric("Unique Fields", unique_count)
                                
                                # 매칭된 필드 상세
                                if comparison['matched_fields']:
                                    st.write("**🎯 Matched Fields:**")
                                    
                                    # 신뢰도별 필터링
                                    high_confidence_matches = [
                                        match for match in comparison['matched_fields'] 
                                        if match['confidence'] >= min_confidence
                                    ]
                                    
                                    if high_confidence_matches:
                                        matched_df = pd.DataFrame([
                                            {
                                                "Doc1 Field": match['field1'],
                                                "Doc2 Field": match['field2'], 
                                                "Score": f"{match['final_score']:.3f}",
                                                "Confidence": f"{match['confidence']:.3f}",
                                                "Doc1 Value": match['value1'][:50] + "..." if len(match['value1']) > 50 else match['value1'],
                                                "Doc2 Value": match['value2'][:50] + "..." if len(match['value2']) > 50 else match['value2']
                                            }
                                            for match in high_confidence_matches
                                        ])
                                        st.dataframe(matched_df, use_container_width=True)
                                    else:
                                        st.info(f"No matches above confidence threshold {min_confidence}")
                                
                                # 고유 필드들
                                if comparison['doc1_unique'] or comparison['doc2_unique']:
                                    col_x, col_y = st.columns(2)
                                    
                                    with col_x:
                                        if comparison['doc1_unique']:
                                            st.write(f"**📄 Unique to {comp_data['doc1_name']}:**")
                                            for field in comparison['doc1_unique'][:10]:  # 최대 10개
                                                st.write(f"• {field}")
                                    
                                    with col_y:
                                        if comparison['doc2_unique']:
                                            st.write(f"**📄 Unique to {comp_data['doc2_name']}:**")
                                            for field in comparison['doc2_unique'][:10]:  # 최대 10개
                                                st.write(f"• {field}")
        
        # 고급 매칭 전략 설명
        with st.expander("🎯 Advanced Semantic Matching Strategy"):
            st.markdown("""
            ### 🧠 5-Level Intelligent Matching
            
            **Level 1: Ontology Direct Matching (40% weight)**
            - Uses ontology.ttl for standard field mappings
            - 100% accuracy for known mappings
            - Auto-learning from new field discoveries
            
            **Level 2: Domain-Specific Embeddings (30% weight)**  
            - Engineering-focused SentenceTransformer model
            - Specialized vocabulary understanding
            - Industry-specific terminology recognition
            
            **Level 3: General Semantic Embeddings (15% weight)**
            - Universal language understanding
            - Handles diverse field naming patterns
            - Robustness for unknown domains
            
            **Level 4: GNN Learned Embeddings (10% weight)**
            - Graph-aware contextual understanding
            - Relationship-informed similarities
            - Document structure consideration
            
            **Level 5: Contextual Analysis (5% weight)**
            - Document position information
            - Surrounding field context
            - Structural pattern recognition
            
            **🎯 Dynamic Features:**
            - Adaptive threshold calculation
            - Confidence-weighted scoring
            - Real-time learning from user feedback
            """)
    
    with tab4:
        st.markdown("## 🕸️ Interactive Knowledge Graph Visualization")
        
        analyzer = st.session_state.enterprise_analyzer
        viz_data = analyzer.create_visualization_data()
        
        if not viz_data['nodes']:
            st.info("📄 No data to visualize. Please upload and process documents first.")
        else:
            col1, col2 = st.columns([3, 1])
            
            with col2:
                st.markdown("### 🎛️ Visualization Controls")
                
                # 필터 옵션
                node_types = list(set(node['type'] for node in viz_data['nodes']))
                selected_types = st.multiselect("Node Types", node_types, default=node_types)
                
                # 레이아웃 옵션
                layout_type = st.selectbox("Layout Algorithm", 
                                         ["spring", "circular", "kamada_kawai", "spectral"])
                
                # 시각화 옵션
                show_labels = st.checkbox("Show Labels", value=True)
                node_size_factor = st.slider("Node Size", 0.5, 2.0, 1.0)
                edge_width_factor = st.slider("Edge Width", 0.5, 2.0, 1.0)
            
            with col1:
                # 필터링된 데이터
                filtered_nodes = [node for node in viz_data['nodes'] if node['type'] in selected_types]
                filtered_node_ids = {node['id'] for node in filtered_nodes}
                filtered_edges = [edge for edge in viz_data['edges'] 
                                if edge['source'] in filtered_node_ids and edge['target'] in filtered_node_ids]
                
                if filtered_nodes:
                    # NetworkX 그래프 생성
                    G = nx.Graph()
                    for node in filtered_nodes:
                        G.add_node(node['id'], **node)
                    for edge in filtered_edges:
                        G.add_edge(edge['source'], edge['target'], **edge)
                    
                    # 레이아웃 계산
                    if layout_type == "spring":
                        pos = nx.spring_layout(G, k=3, iterations=50)
                    elif layout_type == "circular":
                        pos = nx.circular_layout(G)
                    elif layout_type == "kamada_kawai":
                        pos = nx.kamada_kawai_layout(G)
                    else:  # spectral
                        pos = nx.spectral_layout(G)
                    
                    # 색상 매핑
                    color_map = {
                        'document': '#2E86AB',
                        'field': '#A23B72', 
                        'value': '#F18F01'
                    }
                    
                    # Plotly 그래프 생성
                    node_trace = go.Scatter(
                        x=[pos[node['id']][0] for node in filtered_nodes],
                        y=[pos[node['id']][1] for node in filtered_nodes],
                        mode='markers+text' if show_labels else 'markers',
                        text=[node['label'][:15] + '...' if len(node['label']) > 15 else node['label'] 
                              for node in filtered_nodes] if show_labels else [],
                        textposition="middle center",
                        marker=dict(
                            size=[20 * node_size_factor if node['type'] == 'document' 
                                  else 15 * node_size_factor if node['type'] == 'field' 
                                  else 10 * node_size_factor for node in filtered_nodes],
                            color=[color_map.get(node['type'], '#888888') for node in filtered_nodes],
                            line=dict(width=2, color='white'),
                            opacity=0.8
                        ),
                        hoverinfo='text',
                        hovertext=[f"Type: {node['type']}<br>Label: {node['label']}<br>ID: {node['id']}" 
                                  for node in filtered_nodes],
                        name="Nodes"
                    )
                    
                    # 엣지 트레이스
                    edge_traces = []
                    for edge in filtered_edges:
                        x0, y0 = pos[edge['source']]
                        x1, y1 = pos[edge['target']]
                        
                        edge_traces.append(go.Scatter(
                            x=[x0, x1, None],
                            y=[y0, y1, None],
                            mode='lines',
                            line=dict(
                                width=1 * edge_width_factor, 
                                color='rgba(125,125,125,0.5)'
                            ),
                            hoverinfo='text',
                            hovertext=f"Relation: {edge.get('relation', 'unknown')}",
                            showlegend=False
                        ))
                    
                    # 피규어 생성
                    fig = go.Figure(data=[node_trace] + edge_traces)
                    fig.update_layout(
                        title="Enterprise Knowledge Graph - Interactive Visualization",
                        showlegend=True,
                        hovermode='closest',
                        margin=dict(b=20,l=5,r=5,t=40),
                        annotations=[dict(
                            text=f"Nodes: {len(filtered_nodes)} | Edges: {len(filtered_edges)} | Layout: {layout_type}",
                            showarrow=False,
                            xref="paper", yref="paper",
                            x=0.005, y=-0.002,
                            xanchor='left', yanchor='bottom',
                            font=dict(color="gray", size=12)
                        )],
                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        height=600,
                        plot_bgcolor='rgba(0,0,0,0)',
                        paper_bgcolor='rgba(0,0,0,0)'
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                else:
                    st.warning("No nodes match the selected filters.")
            
            # 그래프 통계
            st.markdown("### 📊 Graph Statistics")
            
            col_a, col_b, col_c, col_d = st.columns(4)
            
            with col_a:
                st.metric("Total Nodes", len(viz_data['nodes']))
            with col_b:
                st.metric("Total Edges", len(viz_data['edges']))
            with col_c:
                if viz_data['nodes']:
                    G_full = nx.Graph()
                    for edge in viz_data['edges']:
                        G_full.add_edge(edge['source'], edge['target'])
                    density = nx.density(G_full)
                    st.metric("Graph Density", f"{density:.3f}")
            with col_d:
                node_types_count = {}
                for node in viz_data['nodes']:
                    node_type = node['type']
                    node_types_count[node_type] = node_types_count.get(node_type, 0) + 1
                
                most_common_type = max(node_types_count.items(), key=lambda x: x[1]) if node_types_count else ("None", 0)
                st.metric("Dominant Type", f"{most_common_type[0]} ({most_common_type[1]})")
            
            # 노드 타입 분포
            with st.expander("📈 Node Type Distribution"):
                if node_types_count:
                    type_df = pd.DataFrame([
                        {"Node Type": k, "Count": v, "Percentage": f"{(v/len(viz_data['nodes'])*100):.1f}%"}
                        for k, v in node_types_count.items()
                    ])
                    st.dataframe(type_df, use_container_width=True)
                    
                    # 파이 차트
                    fig_pie = px.pie(
                        values=list(node_types_count.values()),
                        names=list(node_types_count.keys()),
                        title="Node Type Distribution"
                    )
                    st.plotly_chart(fig_pie, use_container_width=True)
    
    with tab5:
        st.markdown("## 📊 Enterprise Analytics Dashboard")
        
        analyzer = st.session_state.enterprise_analyzer
        documents = analyzer.db_manager.get_all_documents()
        
        if not documents:
            st.info("📈 Upload and process documents to see analytics.")
        else:
            # 전체 통계
            st.markdown("### 📈 Overall Performance Metrics")
            
            col1, col2, col3, col4 = st.columns(4)
            
            total_fields = sum(len(json.loads(doc['fields']) if isinstance(doc['fields'], str) else doc['fields']) for doc in documents)
            avg_quality = np.mean([doc['quality_score'] for doc in documents])
            
            with col1:
                st.metric("Total Documents", len(documents))
            with col2:
                st.metric("Total Fields Extracted", total_fields)
            with col3:
                st.metric("Average Quality Score", f"{avg_quality:.3f}")
            with col4:
                processing_efficiency = total_fields / len(documents) if documents else 0
                st.metric("Processing Efficiency", f"{processing_efficiency:.1f} fields/doc")
            
            # 시계열 분석
            st.markdown("### 📅 Processing Timeline")
            
            # 문서 처리 시간별 분포
            docs_df = pd.DataFrame([
                {
                    "Date": pd.to_datetime(doc['processed_at']),
                    "Filename": doc['filename'],
                    "Quality Score": doc['quality_score'],
                    "Fields Count": len(json.loads(doc['fields']) if isinstance(doc['fields'], str) else doc['fields'])
                }
                for doc in documents
            ])
            
            docs_df['Date'] = docs_df['Date'].dt.date
            
            # 일별 처리 통계
            daily_stats = docs_df.groupby('Date').agg({
                'Filename': 'count',
                'Quality Score': 'mean',
                'Fields Count': 'sum'
            }).rename(columns={'Filename': 'Documents Processed'})
            
            col_a, col_b = st.columns(2)
            
            with col_a:
                fig_timeline = px.line(
                    daily_stats.reset_index(),
                    x='Date',
                    y='Documents Processed',
                    title="Documents Processed Over Time",
                    markers=True
                )
                st.plotly_chart(fig_timeline, use_container_width=True)
            
            with col_b:
                fig_quality = px.line(
                    daily_stats.reset_index(),
                    x='Date',
                    y='Quality Score',
                    title="Average Quality Score Over Time",
                    markers=True
                )
                st.plotly_chart(fig_quality, use_container_width=True)
            
            # 품질 점수 분포
            st.markdown("### 📊 Quality Score Distribution")
            
            fig_hist = px.histogram(
                docs_df,
                x='Quality Score',
                nbins=20,
                title="Quality Score Distribution",
                labels={'count': 'Number of Documents'}
            )
            st.plotly_chart(fig_hist, use_container_width=True)
            
            # 필드 추출 효율성
            st.markdown("### 🎯 Field Extraction Analysis")
            
            col_x, col_y = st.columns(2)
            
            with col_x:
                fig_scatter = px.scatter(
                    docs_df,
                    x='Quality Score',
                    y='Fields Count',
                    hover_data=['Filename'],
                    title="Quality Score vs Fields Extracted",
                    trendline="ols"
                )
                st.plotly_chart(fig_scatter, use_container_width=True)
            
            with col_y:
                # 상위/하위 성능 문서
                st.markdown("#### 🏆 Top Performing Documents")
                top_docs = docs_df.nlargest(5, 'Quality Score')[['Filename', 'Quality Score', 'Fields Count']]
                st.dataframe(top_docs, use_container_width=True)
                
                st.markdown("#### ⚠️ Low Quality Documents")
                low_docs = docs_df.nsmallest(3, 'Quality Score')[['Filename', 'Quality Score', 'Fields Count']]
                st.dataframe(low_docs, use_container_width=True)
            
            # GNN 학습 통계 (있는 경우)
            if analyzer.training_history:
                st.markdown("### 🤖 GNN Training Performance")
                
                training_df = pd.DataFrame({
                    'Epoch': range(len(analyzer.training_history)),
                    'Loss': analyzer.training_history
                })
                
                fig_training = px.line(
                    training_df,
                    x='Epoch',
                    y='Loss',
                    title="GNN Training Loss Over Epochs",
                    log_y=True
                )
                st.plotly_chart(fig_training, use_container_width=True)
                
                # 학습 통계
                final_loss = analyzer.training_history[-1]
                initial_loss = analyzer.training_history[0]
                improvement = ((initial_loss - final_loss) / initial_loss) * 100
                
                col_stat1, col_stat2, col_stat3 = st.columns(3)
                with col_stat1:
                    st.metric("Initial Loss", f"{initial_loss:.6f}")
                with col_stat2:
                    st.metric("Final Loss", f"{final_loss:.6f}")
                with col_stat3:
                    st.metric("Improvement", f"{improvement:.1f}%")
            
            # 시스템 리소스 정보
            st.markdown("### 💻 System Resources")
            
            col_res1, col_res2, col_res3 = st.columns(3)
            
            with col_res1:
                st.info(f"**Device:** {device}")
                st.info(f"**CUDA Available:** {'✅' if torch.cuda.is_available() else '❌'}")
            
            with col_res2:
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
                    st.info(f"**GPU Memory:** {gpu_memory:.1f} GB")
                    st.info(f"**GPU Name:** {torch.cuda.get_device_name(0)}")
            
            with col_res3:
                import psutil
                cpu_percent = psutil.cpu_percent()
                memory_percent = psutil.virtual_memory().percent
                st.info(f"**CPU Usage:** {cpu_percent:.1f}%")
                st.info(f"**Memory Usage:** {memory_percent:.1f}%")

if __name__ == "__main__":
    main()