import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
import networkx as nx
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import RGCNConv, SAGEConv, GATConv
from torch_geometric.data import Data, HeteroData
import numpy as np
import pdfplumber
import re
import json
import sqlite3
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import rdflib
from rdflib import Graph, Namespace, URIRef
import logging
from typing import Dict, List, Tuple, Any
import io
from datetime import datetime

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Multi-GNN Knowledge Graph Analyzer",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #2E86AB;
        text-align: center;
        margin-bottom: 2rem;
    }
    .section-header {
        font-size: 1.5rem;
        color: #1f5f7a;
        border-bottom: 2px solid #2E86AB;
        padding-bottom: 0.5rem;
        margin: 1.5rem 0;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 0.25rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .error-box {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        border-radius: 0.25rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .info-box {
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        border-radius: 0.25rem;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

class OntologyManager:
    """Ontology.ttl ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, ontology_path: str = "ontology.ttl"):
        self.graph = Graph()
        self.ontology_path = ontology_path
        self.field_mappings = {}
        self.load_ontology()
    
    def load_ontology(self):
        """Ontology íŒŒì¼ ë¡œë“œ"""
        try:
            self.graph.parse(self.ontology_path, format="ttl")
            self._build_field_mappings()
            logger.info(f"âœ… Ontology loaded: {len(self.graph)} triples")
        except Exception as e:
            logger.warning(f"âš ï¸ Ontology file not found: {e}")
            self._create_default_ontology()
    
    def _build_field_mappings(self):
        """í•„ë“œëª… ë§¤í•‘ ì‚¬ì „ êµ¬ì¶•"""
        # SPARQL ì¿¼ë¦¬ë¡œ í•„ë“œëª…ê³¼ ë™ì˜ì–´ ì¶”ì¶œ
        query = """
        SELECT ?field ?label ?altLabel WHERE {
            ?field a ?type .
            OPTIONAL { ?field rdfs:label ?label }
            OPTIONAL { ?field skos:altLabel ?altLabel }
        }
        """
        try:
            results = self.graph.query(query)
            for row in results:
                field_uri = str(row.field)
                if row.label:
                    self.field_mappings[str(row.label).lower()] = field_uri
                if row.altLabel:
                    self.field_mappings[str(row.altLabel).lower()] = field_uri
        except:
            pass
    
    def _create_default_ontology(self):
        """ê¸°ë³¸ ì˜¨í†¨ë¡œì§€ ìƒì„±"""
        default_mappings = {
            'pressure': 'eng:Pressure',
            'temperature': 'eng:Temperature', 
            'flow': 'eng:FlowRate',
            'diameter': 'eng:Diameter',
            'capacity': 'eng:Capacity',
            'design pressure': 'eng:DesignPressure',
            'operating pressure': 'eng:OperatingPressure',
            'design temperature': 'eng:DesignTemperature',
            'operating temperature': 'eng:OperatingTemperature'
        }
        self.field_mappings = default_mappings
    
    def find_standard_field(self, field_name: str) -> str:
        """í‘œì¤€ í•„ë“œëª… ì°¾ê¸°"""
        normalized = field_name.lower().strip()
        return self.field_mappings.get(normalized, field_name)

class MultiGNN(nn.Module):
    """Multi-GNN ì•„í‚¤í…ì²˜: R-GCN â†’ GraphSAGE â†’ GAT"""
    
    def __init__(self, num_nodes, num_relations, hidden_dim=64, out_dim=32):
        super(MultiGNN, self).__init__()
        self.hidden_dim = hidden_dim
        self.out_dim = out_dim
        
        # R-GCN Layer
        self.rgcn = RGCNConv(num_nodes, hidden_dim, num_relations)
        
        # GraphSAGE Layer
        self.sage = SAGEConv(hidden_dim, hidden_dim)
        
        # GAT Layer
        self.gat = GATConv(hidden_dim, out_dim, heads=4, concat=False)
        
        # Classification head
        self.classifier = nn.Linear(out_dim, num_nodes)
        
    def forward(self, x, edge_index, edge_type):
        # R-GCN: ê´€ê³„ í•™ìŠµ
        x = F.relu(self.rgcn(x, edge_index, edge_type))
        
        # GraphSAGE: ì´ì›ƒ ì •ë³´ ì§‘ê³„
        x = F.relu(self.sage(x, edge_index))
        
        # GAT: ì–´í…ì…˜ ê¸°ë°˜ ì¤‘ìš” ì •ë³´ í¬ì»¤ìŠ¤
        x = self.gat(x, edge_index)
        
        return x

class SemanticMatcher:
    """ì‹œë§¨í‹± ë§¤ì¹­ ì—”ì§„"""
    
    def __init__(self, ontology_manager: OntologyManager):
        self.ontology = ontology_manager
        self.sentence_model = None
        self.field_embeddings = {}
        self._load_sentence_model()
    
    @st.cache_resource
    def _load_sentence_model(_self):
        """SentenceTransformer ëª¨ë¸ ë¡œë“œ (ìºì‹œë¨)"""
        return SentenceTransformer('all-MiniLM-L6-v2')
    
    def match_fields(self, field1: str, field2: str, gnn_embeddings=None) -> float:
        """ë‹¤ë‹¨ê³„ í•„ë“œ ë§¤ì¹­"""
        
        # Level 1: Ontology ì§ì ‘ ë§¤ì¹­
        std_field1 = self.ontology.find_standard_field(field1)
        std_field2 = self.ontology.find_standard_field(field2)
        
        if std_field1 == std_field2 and std_field1 != field1:
            return 1.0
        
        # Level 2: ì„ë² ë”© ê¸°ë°˜ ë§¤ì¹­
        if self.sentence_model is None:
            self.sentence_model = self._load_sentence_model()
        
        emb1 = self.sentence_model.encode([field1])
        emb2 = self.sentence_model.encode([field2])
        semantic_sim = cosine_similarity(emb1, emb2)[0][0]
        
        # Level 3: GNN ì„ë² ë”© ë§¤ì¹­ (ìˆëŠ” ê²½ìš°)
        gnn_sim = 0.0
        if gnn_embeddings is not None:
            # GNN ì„ë² ë”©ì„ ì´ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°
            pass
        
        # ê°€ì¤‘ í‰ê· 
        final_score = 0.5 * semantic_sim + 0.3 * gnn_sim + 0.2
        return min(final_score, 1.0)

class AdvancedPDFProcessor:
    """OCR ì—†ëŠ” ê³ ê¸‰ PDF ì²˜ë¦¬"""
    
    def __init__(self):
        self.text_quality_threshold = 0.7
    
    def extract_text_and_tables(self, pdf_file) -> Dict[str, Any]:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ì™€ í‘œ ì¶”ì¶œ"""
        results = {
            'text': '',
            'tables': [],
            'fields': {},
            'quality_score': 0.0
        }
        
        try:
            with pdfplumber.open(pdf_file) as pdf:
                full_text = ""
                all_tables = []
                
                for page in pdf.pages:
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    page_text = page.extract_text()
                    if page_text:
                        full_text += page_text + "\n"
                    
                    # í‘œ ì¶”ì¶œ
                    tables = page.extract_tables()
                    for table in tables:
                        if table:
                            all_tables.append(self._process_table(table))
                
                results['text'] = full_text
                results['tables'] = all_tables
                results['fields'] = self._extract_fields_from_text(full_text, all_tables)
                results['quality_score'] = self._calculate_quality_score(full_text, all_tables)
                
        except Exception as e:
            logger.error(f"âŒ PDF processing error: {e}")
            st.error(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
        return results
    
    def _process_table(self, table: List[List[str]]) -> Dict[str, Any]:
        """í‘œ êµ¬ì¡° ë¶„ì„ ë° ì²˜ë¦¬"""
        if not table or len(table) < 2:
            return {}
        
        # í—¤ë” ì¶”ì¶œ
        headers = [cell for cell in table[0] if cell]
        
        # ë°ì´í„° í–‰ ì²˜ë¦¬
        data_rows = []
        for row in table[1:]:
            if any(row):  # ë¹ˆ í–‰ì´ ì•„ë‹Œ ê²½ìš°
                data_rows.append(row)
        
        # í•„ë“œ-ê°’ ìŒ ì¶”ì¶œ
        field_value_pairs = {}
        if len(headers) >= 2:
            for row in data_rows:
                if len(row) >= 2 and row[0] and row[1]:
                    field_name = str(row[0]).strip()
                    field_value = str(row[1]).strip()
                    field_value_pairs[field_name] = field_value
        
        return {
            'headers': headers,
            'data': data_rows,
            'field_value_pairs': field_value_pairs,
            'structure_type': self._detect_table_structure(headers, data_rows)
        }
    
    def _detect_table_structure(self, headers: List[str], data: List[List[str]]) -> str:
        """í‘œ êµ¬ì¡° íƒ€ì… ê°ì§€"""
        if not headers:
            return "unstructured"
        
        # ì—”ì§€ë‹ˆì–´ë§ í‘œì¤€ íŒ¨í„´ ê°ì§€
        engineering_keywords = ['pressure', 'temperature', 'flow', 'capacity', 'design', 'operating']
        
        header_text = ' '.join(headers).lower()
        if any(keyword in header_text for keyword in engineering_keywords):
            return "engineering_specification"
        elif len(headers) == 2:
            return "field_value_pairs"
        else:
            return "multi_column_data"
    
    def _extract_fields_from_text(self, text: str, tables: List[Dict]) -> Dict[str, str]:
        """í…ìŠ¤íŠ¸ì™€ í‘œì—ì„œ í•„ë“œ ì¶”ì¶œ"""
        fields = {}
        
        # í‘œì—ì„œ í•„ë“œ-ê°’ ìŒ ì¶”ì¶œ
        for table in tables:
            if 'field_value_pairs' in table:
                fields.update(table['field_value_pairs'])
        
        # í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ ê¸°ë°˜ í•„ë“œ ì¶”ì¶œ
        patterns = [
            r'([A-Z][A-Z\s]+)\s*:\s*([^\n]+)',  # FIELD NAME : value
            r'(\d+\s+[A-Z][A-Z\s]+)\s+([^\n]+)',  # 01 FIELD NAME value
            r'([A-Z][a-z\s]+)\s*=\s*([^\n]+)'  # Field Name = value
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for field_name, field_value in matches:
                field_name = field_name.strip()
                field_value = field_value.strip()
                if len(field_name) > 2 and len(field_value) > 0:
                    fields[field_name] = field_value
        
        return fields
    
    def _calculate_quality_score(self, text: str, tables: List[Dict]) -> float:
        """í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if not text:
            return 0.0
        
        # ê¸°ë³¸ ì ìˆ˜
        base_score = 0.3
        
        # í‘œ êµ¬ì¡° ì ìˆ˜
        table_score = min(len(tables) * 0.2, 0.4)
        
        # í•„ë“œ ì¶”ì¶œ ì ìˆ˜
        total_fields = sum(len(table.get('field_value_pairs', {})) for table in tables)
        field_score = min(total_fields * 0.05, 0.3)
        
        return min(base_score + table_score + field_score, 1.0)

class DocumentComparer:
    """ë¬¸ì„œ ë¹„êµ ì—”ì§„"""
    
    def __init__(self, semantic_matcher: SemanticMatcher):
        self.semantic_matcher = semantic_matcher
        self.comparison_cache = {}
    
    def compare_documents(self, doc1_fields: Dict, doc2_fields: Dict, 
                         gnn_embeddings=None) -> Dict[str, Any]:
        """ë‘ ë¬¸ì„œì˜ í•„ë“œ ë¹„êµ"""
        
        comparison_result = {
            'matched_fields': [],
            'doc1_unique': [],
            'doc2_unique': [],
            'field_mappings': {},
            'similarity_scores': {},
            'overall_similarity': 0.0
        }
        
        # ëª¨ë“  í•„ë“œ ì¡°í•©ì— ëŒ€í•´ ìœ ì‚¬ë„ ê³„ì‚°
        similarity_matrix = {}
        for field1 in doc1_fields:
            for field2 in doc2_fields:
                sim_score = self.semantic_matcher.match_fields(
                    field1, field2, gnn_embeddings
                )
                similarity_matrix[(field1, field2)] = sim_score
        
        # ìµœì  ë§¤ì¹­ ì°¾ê¸° (í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ ê°„ì†Œí™”)
        used_fields2 = set()
        threshold = 0.7
        
        for field1 in doc1_fields:
            best_match = None
            best_score = 0.0
            
            for field2 in doc2_fields:
                if field2 not in used_fields2:
                    score = similarity_matrix[(field1, field2)]
                    if score > best_score and score >= threshold:
                        best_score = score
                        best_match = field2
            
            if best_match:
                comparison_result['matched_fields'].append({
                    'field1': field1,
                    'field2': best_match,
                    'similarity': best_score,
                    'value1': doc1_fields[field1],
                    'value2': doc2_fields[best_match]
                })
                comparison_result['field_mappings'][field1] = best_match
                used_fields2.add(best_match)
            else:
                comparison_result['doc1_unique'].append(field1)
        
        # ë§¤ì¹­ë˜ì§€ ì•Šì€ doc2 í•„ë“œë“¤
        for field2 in doc2_fields:
            if field2 not in used_fields2:
                comparison_result['doc2_unique'].append(field2)
        
        # ì „ì²´ ìœ ì‚¬ë„ ê³„ì‚°
        if len(doc1_fields) > 0 or len(doc2_fields) > 0:
            matched_count = len(comparison_result['matched_fields'])
            total_fields = len(doc1_fields) + len(doc2_fields)
            comparison_result['overall_similarity'] = (2 * matched_count) / total_fields
        
        return comparison_result

class AdvancedKnowledgeGraphAnalyzer:
    """ê³ ê¸‰ ì§€ì‹ ê·¸ë˜í”„ ë¶„ì„ê¸° ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.ontology_manager = OntologyManager()
        self.semantic_matcher = SemanticMatcher(self.ontology_manager)
        self.pdf_processor = AdvancedPDFProcessor()
        self.document_comparer = DocumentComparer(self.semantic_matcher)
        
        # ë°ì´í„° ì €ì¥
        self.documents = {}
        self.knowledge_graph = nx.DiGraph()
        self.gnn_model = None
        self.node_embeddings = None
    
    def process_pdf(self, pdf_file, filename: str) -> Dict[str, Any]:
        """PDF ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        logger.info(f"ğŸ“„ Processing PDF: {filename}")
        
        # PDFì—ì„œ í…ìŠ¤íŠ¸ì™€ í‘œ ì¶”ì¶œ (OCR ì—†ìŒ)
        extraction_result = self.pdf_processor.extract_text_and_tables(pdf_file)
        
        logger.info(f"ğŸ“Š Text quality score: {extraction_result['quality_score']:.2f}")
        logger.info(f"ğŸ·ï¸ Extracted {len(extraction_result['fields'])} field-value pairs")
        logger.info(f"ğŸ“‹ Found {len(extraction_result['tables'])} tables")
        
        # ì§€ì‹ ê·¸ë˜í”„ì— ì¶”ê°€
        doc_id = len(self.documents) + 1
        self.documents[doc_id] = {
            'filename': filename,
            'fields': extraction_result['fields'],
            'tables': extraction_result['tables'],
            'text': extraction_result['text'],
            'quality_score': extraction_result['quality_score'],
            'processed_at': datetime.now().isoformat()
        }
        
        # ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
        self._update_knowledge_graph(doc_id, extraction_result['fields'])
        
        return {
            'doc_id': doc_id,
            'field_count': len(extraction_result['fields']),
            'quality_score': extraction_result['quality_score'],
            'tables_count': len(extraction_result['tables'])
        }
    
    def _update_knowledge_graph(self, doc_id: int, fields: Dict[str, str]):
        """ì§€ì‹ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸"""
        doc_node = f"doc_{doc_id}"
        self.knowledge_graph.add_node(doc_node, type="document")
        
        for field_name, field_value in fields.items():
            # í‘œì¤€í™”ëœ í•„ë“œëª… ì‚¬ìš©
            std_field = self.ontology_manager.find_standard_field(field_name)
            
            field_node = f"field_{std_field}_{doc_id}"
            value_node = f"value_{field_value}_{doc_id}"
            
            self.knowledge_graph.add_node(field_node, type="field", name=std_field)
            self.knowledge_graph.add_node(value_node, type="value", value=field_value)
            
            self.knowledge_graph.add_edge(doc_node, field_node, relation="has_field")
            self.knowledge_graph.add_edge(field_node, value_node, relation="has_value")
    
    def train_multi_gnn(self) -> Dict[str, Any]:
        """Multi-GNN ëª¨ë¸ í›ˆë ¨"""
        if len(self.knowledge_graph.nodes) < 5:
            return {"error": "Not enough nodes for GNN training"}
        
        logger.info("ğŸ¤– Training Multi-GNN model...")
        
        # ê·¸ë˜í”„ ë°ì´í„° ì¤€ë¹„
        nodes = list(self.knowledge_graph.nodes())
        edges = list(self.knowledge_graph.edges())
        
        # ë…¸ë“œì™€ ì—£ì§€ ì¸ë±ìŠ¤ ë§¤í•‘
        node_to_idx = {node: idx for idx, node in enumerate(nodes)}
        
        # ì—£ì§€ ì¸ë±ìŠ¤ì™€ íƒ€ì… ì¤€ë¹„
        edge_index = torch.tensor([[node_to_idx[e[0]], node_to_idx[e[1]]] for e in edges]).t()
        
        # ê´€ê³„ íƒ€ì… ë§¤í•‘
        relation_types = list(set(nx.get_edge_attributes(self.knowledge_graph, 'relation').values()))
        relation_to_idx = {rel: idx for idx, rel in enumerate(relation_types)}
        
        edge_type = torch.tensor([
            relation_to_idx.get(
                self.knowledge_graph[e[0]][e[1]].get('relation', 'unknown'), 0
            ) for e in edges
        ])
        
        # ì´ˆê¸° ë…¸ë“œ íŠ¹ì„± (identity matrix)
        num_nodes = len(nodes)
        x = torch.eye(num_nodes)
        
        # Multi-GNN ëª¨ë¸ ì´ˆê¸°í™”
        self.gnn_model = MultiGNN(
            num_nodes=num_nodes,
            num_relations=len(relation_types),
            hidden_dim=64,
            out_dim=32
        )
        
        # í›ˆë ¨
        optimizer = torch.optim.Adam(self.gnn_model.parameters(), lr=0.01)
        
        self.gnn_model.train()
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for epoch in range(100):
            optimizer.zero_grad()
            
            # Forward pass
            embeddings = self.gnn_model(x, edge_index, edge_type)
            
            # ìì²´ ì§€ë„ í•™ìŠµ: ë…¸ë“œ ì¬êµ¬ì„± ì†ì‹¤
            reconstructed = self.gnn_model.classifier(embeddings)
            loss = F.mse_loss(reconstructed, x)
            
            loss.backward()
            optimizer.step()
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress_bar.progress((epoch + 1) / 100)
            if epoch % 20 == 0:
                status_text.text(f"Epoch {epoch}, Loss: {loss.item():.4f}")
        
        # í•™ìŠµëœ ì„ë² ë”© ì €ì¥
        self.gnn_model.eval()
        with torch.no_grad():
            self.node_embeddings = self.gnn_model(x, edge_index, edge_type).numpy()
        
        progress_bar.empty()
        status_text.empty()
        logger.info("âœ… Multi-GNN training completed")
        
        return {
            "status": "success",
            "num_nodes": num_nodes,
            "num_relations": len(relation_types),
            "final_loss": loss.item()
        }
    
    def compare_all_documents(self) -> Dict[str, Any]:
        """ëª¨ë“  ë¬¸ì„œ ë¹„êµ"""
        if len(self.documents) < 2:
            return {"error": "Need at least 2 documents for comparison"}
        
        logger.info("ğŸ” Comparing all documents with semantic matching...")
        
        doc_ids = list(self.documents.keys())
        comparison_results = {}
        
        for i, doc_id1 in enumerate(doc_ids):
            for doc_id2 in doc_ids[i+1:]:
                doc1_fields = self.documents[doc_id1]['fields']
                doc2_fields = self.documents[doc_id2]['fields']
                
                comparison = self.document_comparer.compare_documents(
                    doc1_fields, doc2_fields, self.node_embeddings
                )
                
                comparison_key = f"doc_{doc_id1}_vs_doc_{doc_id2}"
                comparison_results[comparison_key] = {
                    'doc1_name': self.documents[doc_id1]['filename'],
                    'doc2_name': self.documents[doc_id2]['filename'],
                    'comparison': comparison
                }
        
        return comparison_results
    
    def create_visualization_data(self) -> Dict[str, Any]:
        """ì‹œê°í™”ìš© ë°ì´í„° ìƒì„±"""
        if not self.knowledge_graph.nodes():
            return {"nodes": [], "edges": []}
        
        # ë…¸ë“œ ë°ì´í„°
        nodes = []
        for node in self.knowledge_graph.nodes(data=True):
            node_id, node_data = node
            nodes.append({
                'id': node_id,
                'type': node_data.get('type', 'unknown'),
                'label': node_data.get('name', node_data.get('value', node_id))
            })
        
        # ì—£ì§€ ë°ì´í„°
        edges = []
        for edge in self.knowledge_graph.edges(data=True):
            source, target, edge_data = edge
            edges.append({
                'source': source,
                'target': target,
                'relation': edge_data.get('relation', 'unknown')
            })
        
        return {"nodes": nodes, "edges": edges}

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'analyzer' not in st.session_state:
    st.session_state.analyzer = AdvancedKnowledgeGraphAnalyzer()
    st.success("âœ… Advanced Multi-GNN Knowledge Graph Analyzer initialized")

# ë©”ì¸ ì•±
def main():
    # í—¤ë”
    st.markdown('<h1 class="main-header">ğŸš€ Advanced Multi-GNN Knowledge Graph Analyzer</h1>', 
                unsafe_allow_html=True)
    
    # ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´
    with st.sidebar:
        st.markdown('<h2 class="section-header">ğŸ“Š System Status</h2>', unsafe_allow_html=True)
        
        analyzer = st.session_state.analyzer
        
        st.metric("Documents", len(analyzer.documents))
        st.metric("Graph Nodes", len(analyzer.knowledge_graph.nodes))
        st.metric("Graph Edges", len(analyzer.knowledge_graph.edges))
        
        if analyzer.gnn_model is not None:
            st.success("ğŸ¤– Multi-GNN Model: Trained")
        else:
            st.info("ğŸ¤– Multi-GNN Model: Not trained")
        
        st.markdown("---")
        st.markdown("### ğŸ¯ Features")
        st.markdown("âœ… OCR-free PDF processing")
        st.markdown("âœ… Multi-GNN pipeline")
        st.markdown("âœ… Semantic field matching")
        st.markdown("âœ… Ontology integration")
    
    # ë©”ì¸ íƒ­
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“„ Document Upload", "ğŸ¤– GNN Training", "ğŸ” Document Comparison", "ğŸ•¸ï¸ Visualization"])
    
    with tab1:
        st.markdown('<h2 class="section-header">ğŸ“„ Document Upload & Processing</h2>', 
                   unsafe_allow_html=True)
        
        uploaded_files = st.file_uploader(
            "Upload PDF documents",
            type=['pdf'],
            accept_multiple_files=True,
            help="Upload one or more PDF documents for analysis"
        )
        
        if uploaded_files:
            for uploaded_file in uploaded_files:
                if st.button(f"Process {uploaded_file.name}", key=f"process_{uploaded_file.name}"):
                    with st.spinner(f"Processing {uploaded_file.name}..."):
                        try:
                            result = st.session_state.analyzer.process_pdf(uploaded_file, uploaded_file.name)
                            
                            st.markdown(f"""
                            <div class="success-box">
                                <h4>âœ… {uploaded_file.name} processed successfully</h4>
                                <p><strong>ğŸ“Š Quality Score:</strong> {result['quality_score']:.2f}</p>
                                <p><strong>ğŸ·ï¸ Fields Extracted:</strong> {result['field_count']}</p>
                                <p><strong>ğŸ“‹ Tables Found:</strong> {result['tables_count']}</p>
                            </div>
                            """, unsafe_allow_html=True)
                            
                            # ì¶”ì¶œëœ í•„ë“œ í‘œì‹œ
                            if result['field_count'] > 0:
                                with st.expander("View Extracted Fields"):
                                    doc_fields = st.session_state.analyzer.documents[result['doc_id']]['fields']
                                    fields_df = pd.DataFrame([
                                        {"Field Name": k, "Value": v} 
                                        for k, v in doc_fields.items()
                                    ])
                                    st.dataframe(fields_df, use_container_width=True)
                            
                        except Exception as e:
                            st.markdown(f"""
                            <div class="error-box">
                                <h4>âŒ Error processing {uploaded_file.name}</h4>
                                <p>{str(e)}</p>
                            </div>
                            """, unsafe_allow_html=True)
    
    with tab2:
        st.markdown('<h2 class="section-header">ğŸ¤– Multi-GNN Model Training</h2>', 
                   unsafe_allow_html=True)
        
        if len(st.session_state.analyzer.knowledge_graph.nodes) < 5:
            st.warning("âš ï¸ Need at least 5 nodes for GNN training. Please upload more documents.")
        else:
            st.info(f"ğŸ“Š Current graph: {len(st.session_state.analyzer.knowledge_graph.nodes)} nodes, "
                   f"{len(st.session_state.analyzer.knowledge_graph.edges)} edges")
            
            if st.button("ğŸš€ Train Multi-GNN Model", type="primary"):
                with st.spinner("Training Multi-GNN model..."):
                    result = st.session_state.analyzer.train_multi_gnn()
                    
                    if "error" in result:
                        st.error(f"âŒ {result['error']}")
                    else:
                        st.markdown(f"""
                        <div class="success-box">
                            <h4>âœ… Multi-GNN training completed!</h4>
                            <p><strong>ğŸ“Š Nodes:</strong> {result['num_nodes']}</p>
                            <p><strong>ğŸ”— Relations:</strong> {result['num_relations']}</p>
                            <p><strong>ğŸ“‰ Final Loss:</strong> {result['final_loss']:.6f}</p>
                        </div>
                        """, unsafe_allow_html=True)
        
        # ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ëª…
        with st.expander("ğŸ“š Multi-GNN Architecture Details"):
            st.markdown("""
            **Pipeline: R-GCN â†’ GraphSAGE â†’ GAT**
            
            1. **R-GCN (Relational Graph Convolutional Network)**
               - ê´€ê³„í˜• ê·¸ë˜í”„ì—ì„œ ê¸°ë³¸ì ì¸ ê´€ê³„ í•™ìŠµ
               - ë‹¤ì–‘í•œ ê´€ê³„ íƒ€ì… ì²˜ë¦¬
            
            2. **GraphSAGE (Graph Sample and Aggregate)**
               - ì´ì›ƒ ë…¸ë“œ ì •ë³´ë¥¼ ìƒ˜í”Œë§í•˜ê³  ì§‘ê³„
               - í•„ë“œëª… ì„ë² ë”© í•™ìŠµ
            
            3. **GAT (Graph Attention Network)**
               - ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì¤‘ìš”í•œ ê´€ê³„ì— ì§‘ì¤‘
               - ë¬¸ì„œê°„ êµ¬ì¡°ì  ë¹„êµ
            """)
    
    with tab3:
        st.markdown('<h2 class="section-header">ğŸ” Document Comparison</h2>', 
                   unsafe_allow_html=True)
        
        if len(st.session_state.analyzer.documents) < 2:
            st.warning("âš ï¸ Need at least 2 documents for comparison.")
        else:
            if st.button("ğŸ” Compare All Documents", type="primary"):
                with st.spinner("Comparing documents with semantic matching..."):
                    results = st.session_state.analyzer.compare_all_documents()
                    
                    if "error" in results:
                        st.error(f"âŒ {results['error']}")
                    else:
                        for comp_key, comp_data in results.items():
                            comparison = comp_data['comparison']
                            
                            st.markdown(f"""
                            <div class="info-box">
                                <h4>ğŸ“‹ {comp_data['doc1_name']} vs {comp_data['doc2_name']}</h4>
                                <p><strong>Overall Similarity:</strong> {comparison['overall_similarity']:.2f}</p>
                                <p><strong>Matched Fields:</strong> {len(comparison['matched_fields'])}</p>
                                <p><strong>Unique to Doc1:</strong> {len(comparison['doc1_unique'])}</p>
                                <p><strong>Unique to Doc2:</strong> {len(comparison['doc2_unique'])}</p>
                            </div>
                            """, unsafe_allow_html=True)
                            
                            # ë§¤ì¹­ëœ í•„ë“œ í‘œì‹œ
                            if comparison['matched_fields']:
                                with st.expander(f"View Matched Fields - {comp_data['doc1_name']} vs {comp_data['doc2_name']}"):
                                    matched_df = pd.DataFrame([
                                        {
                                            "Doc1 Field": match['field1'],
                                            "Doc2 Field": match['field2'],
                                            "Similarity": f"{match['similarity']:.3f}",
                                            "Doc1 Value": match['value1'][:50] + "..." if len(match['value1']) > 50 else match['value1'],
                                            "Doc2 Value": match['value2'][:50] + "..." if len(match['value2']) > 50 else match['value2']
                                        }
                                        for match in comparison['matched_fields']
                                    ])
                                    st.dataframe(matched_df, use_container_width=True)
        
        # ì‹œë§¨í‹± ë§¤ì¹­ ì„¤ëª…
        with st.expander("ğŸ¯ Semantic Matching Strategy"):
            st.markdown("""
            **4-Level Semantic Matching:**
            
            1. **Level 1: Ontology Direct Matching**
               - ontology.ttl ê¸°ë°˜ í‘œì¤€ í•„ë“œëª… ë§¤ì¹­
               - ì •í™•ë„: 100% (ì§ì ‘ ì¼ì¹˜ì‹œ)
            
            2. **Level 2: Embedding-based Matching**
               - SentenceTransformer ëª¨ë¸ ì‚¬ìš©
               - ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            
            3. **Level 3: GNN Embedding Matching**
               - í•™ìŠµëœ GNN ì„ë² ë”© í™œìš©
               - êµ¬ì¡°ì  ì»¨í…ìŠ¤íŠ¸ ë°˜ì˜
            
            4. **Level 4: Structural Context**
               - ë¬¸ì„œ ë‚´ ìœ„ì¹˜ ì •ë³´
               - ê³„ì¸µì  ê´€ê³„ ê³ ë ¤
            """)
    
    with tab4:
        st.markdown('<h2 class="section-header">ğŸ•¸ï¸ Knowledge Graph Visualization</h2>', 
                   unsafe_allow_html=True)
        
        viz_data = st.session_state.analyzer.create_visualization_data()
        
        if not viz_data['nodes']:
            st.info("ğŸ“„ No data to visualize. Please upload documents first.")
        else:
            # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
            G = nx.Graph()
            for node in viz_data['nodes']:
                G.add_node(node['id'], **node)
            for edge in viz_data['edges']:
                G.add_edge(edge['source'], edge['target'])
            
            # ë ˆì´ì•„ì›ƒ ê³„ì‚°
            pos = nx.spring_layout(G, k=3, iterations=50)
            
            # Plotly ê·¸ë˜í”„ ìƒì„±
            node_trace = go.Scatter(
                x=[pos[node['id']][0] for node in viz_data['nodes']],
                y=[pos[node['id']][1] for node in viz_data['nodes']],
                mode='markers+text',
                text=[node['label'][:20] + '...' if len(node['label']) > 20 else node['label'] 
                      for node in viz_data['nodes']],
                textposition="middle center",
                marker=dict(
                    size=[20 if node['type'] == 'document' else 15 if node['type'] == 'field' else 10 
                          for node in viz_data['nodes']],
                    color=['blue' if node['type'] == 'document' else 'green' if node['type'] == 'field' else 'orange'
                           for node in viz_data['nodes']],
                    line=dict(width=2, color='white')
                ),
                hoverinfo='text',
                hovertext=[f"Type: {node['type']}<br>Label: {node['label']}" for node in viz_data['nodes']]
            )
            
            # ì—£ì§€ íŠ¸ë ˆì´ìŠ¤
            edge_traces = []
            for edge in viz_data['edges']:
                x0, y0 = pos[edge['source']]
                x1, y1 = pos[edge['target']]
                edge_traces.append(go.Scatter(
                    x=[x0, x1, None],
                    y=[y0, y1, None],
                    mode='lines',
                    line=dict(width=1, color='gray'),
                    hoverinfo='none'
                ))
            
            # í”¼ê·œì–´ ìƒì„±
            fig = go.Figure(data=[node_trace] + edge_traces)
            fig.update_layout(
                title="Multi-GNN Enhanced Knowledge Graph",
                showlegend=False,
                hovermode='closest',
                margin=dict(b=20,l=5,r=5,t=40),
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                height=600
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # í†µê³„ ì •ë³´
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total Nodes", len(viz_data['nodes']))
            with col2:
                st.metric("Total Edges", len(viz_data['edges']))
            with col3:
                node_types = {}
                for node in viz_data['nodes']:
                    node_type = node['type']
                    node_types[node_type] = node_types.get(node_type, 0) + 1
                st.write("**Node Types:**")
                for node_type, count in node_types.items():
                    st.write(f"â€¢ {node_type}: {count}")

if __name__ == "__main__":
    main()