import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
import networkx as nx
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import RGCNConv, SAGEConv, GATConv
from torch_geometric.data import Data, HeteroData
import numpy as np
import pdfplumber
import re
import json
import sqlite3
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import rdflib
from rdflib import Graph, Namespace, URIRef
import logging
from typing import Dict, List, Tuple, Any
import io
from datetime import datetime

# 페이지 설정
st.set_page_config(
    page_title="Multi-GNN Knowledge Graph Analyzer",
    page_icon="🚀",
    layout="wide",
    initial_sidebar_state="expanded"
)

# 로깅 설정
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# CSS 스타일
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #2E86AB;
        text-align: center;
        margin-bottom: 2rem;
    }
    .section-header {
        font-size: 1.5rem;
        color: #1f5f7a;
        border-bottom: 2px solid #2E86AB;
        padding-bottom: 0.5rem;
        margin: 1.5rem 0;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 0.25rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .error-box {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        border-radius: 0.25rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .info-box {
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        border-radius: 0.25rem;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

class OntologyManager:
    """Ontology.ttl 관리 클래스"""
    
    def __init__(self, ontology_path: str = "ontology.ttl"):
        self.graph = Graph()
        self.ontology_path = ontology_path
        self.field_mappings = {}
        self.load_ontology()
    
    def load_ontology(self):
        """Ontology 파일 로드"""
        try:
            self.graph.parse(self.ontology_path, format="ttl")
            self._build_field_mappings()
            logger.info(f"✅ Ontology loaded: {len(self.graph)} triples")
        except Exception as e:
            logger.warning(f"⚠️ Ontology file not found: {e}")
            self._create_default_ontology()
    
    def _build_field_mappings(self):
        """필드명 매핑 사전 구축"""
        # SPARQL 쿼리로 필드명과 동의어 추출
        query = """
        SELECT ?field ?label ?altLabel WHERE {
            ?field a ?type .
            OPTIONAL { ?field rdfs:label ?label }
            OPTIONAL { ?field skos:altLabel ?altLabel }
        }
        """
        try:
            results = self.graph.query(query)
            for row in results:
                field_uri = str(row.field)
                if row.label:
                    self.field_mappings[str(row.label).lower()] = field_uri
                if row.altLabel:
                    self.field_mappings[str(row.altLabel).lower()] = field_uri
        except:
            pass
    
    def _create_default_ontology(self):
        """기본 온톨로지 생성"""
        default_mappings = {
            'pressure': 'eng:Pressure',
            'temperature': 'eng:Temperature', 
            'flow': 'eng:FlowRate',
            'diameter': 'eng:Diameter',
            'capacity': 'eng:Capacity',
            'design pressure': 'eng:DesignPressure',
            'operating pressure': 'eng:OperatingPressure',
            'design temperature': 'eng:DesignTemperature',
            'operating temperature': 'eng:OperatingTemperature'
        }
        self.field_mappings = default_mappings
    
    def find_standard_field(self, field_name: str) -> str:
        """표준 필드명 찾기"""
        normalized = field_name.lower().strip()
        return self.field_mappings.get(normalized, field_name)

class MultiGNN(nn.Module):
    """Multi-GNN 아키텍처: R-GCN → GraphSAGE → GAT"""
    
    def __init__(self, num_nodes, num_relations, hidden_dim=64, out_dim=32):
        super(MultiGNN, self).__init__()
        self.hidden_dim = hidden_dim
        self.out_dim = out_dim
        
        # R-GCN Layer
        self.rgcn = RGCNConv(num_nodes, hidden_dim, num_relations)
        
        # GraphSAGE Layer
        self.sage = SAGEConv(hidden_dim, hidden_dim)
        
        # GAT Layer
        self.gat = GATConv(hidden_dim, out_dim, heads=4, concat=False)
        
        # Classification head
        self.classifier = nn.Linear(out_dim, num_nodes)
        
    def forward(self, x, edge_index, edge_type):
        # R-GCN: 관계 학습
        x = F.relu(self.rgcn(x, edge_index, edge_type))
        
        # GraphSAGE: 이웃 정보 집계
        x = F.relu(self.sage(x, edge_index))
        
        # GAT: 어텐션 기반 중요 정보 포커스
        x = self.gat(x, edge_index)
        
        return x

class SemanticMatcher:
    """시맨틱 매칭 엔진"""
    
    def __init__(self, ontology_manager: OntologyManager):
        self.ontology = ontology_manager
        self.sentence_model = None
        self.field_embeddings = {}
        self._load_sentence_model()
    
    @st.cache_resource
    def _load_sentence_model(_self):
        """SentenceTransformer 모델 로드 (캐시됨)"""
        return SentenceTransformer('all-MiniLM-L6-v2')
    
    def match_fields(self, field1: str, field2: str, gnn_embeddings=None) -> float:
        """다단계 필드 매칭"""
        
        # Level 1: Ontology 직접 매칭
        std_field1 = self.ontology.find_standard_field(field1)
        std_field2 = self.ontology.find_standard_field(field2)
        
        if std_field1 == std_field2 and std_field1 != field1:
            return 1.0
        
        # Level 2: 임베딩 기반 매칭
        if self.sentence_model is None:
            self.sentence_model = self._load_sentence_model()
        
        emb1 = self.sentence_model.encode([field1])
        emb2 = self.sentence_model.encode([field2])
        semantic_sim = cosine_similarity(emb1, emb2)[0][0]
        
        # Level 3: GNN 임베딩 매칭 (있는 경우)
        gnn_sim = 0.0
        if gnn_embeddings is not None:
            # GNN 임베딩을 이용한 유사도 계산
            pass
        
        # 가중 평균
        final_score = 0.5 * semantic_sim + 0.3 * gnn_sim + 0.2
        return min(final_score, 1.0)

class AdvancedPDFProcessor:
    """OCR 없는 고급 PDF 처리"""
    
    def __init__(self):
        self.text_quality_threshold = 0.7
    
    def extract_text_and_tables(self, pdf_file) -> Dict[str, Any]:
        """PDF에서 텍스트와 표 추출"""
        results = {
            'text': '',
            'tables': [],
            'fields': {},
            'quality_score': 0.0
        }
        
        try:
            with pdfplumber.open(pdf_file) as pdf:
                full_text = ""
                all_tables = []
                
                for page in pdf.pages:
                    # 텍스트 추출
                    page_text = page.extract_text()
                    if page_text:
                        full_text += page_text + "\n"
                    
                    # 표 추출
                    tables = page.extract_tables()
                    for table in tables:
                        if table:
                            all_tables.append(self._process_table(table))
                
                results['text'] = full_text
                results['tables'] = all_tables
                results['fields'] = self._extract_fields_from_text(full_text, all_tables)
                results['quality_score'] = self._calculate_quality_score(full_text, all_tables)
                
        except Exception as e:
            logger.error(f"❌ PDF processing error: {e}")
            st.error(f"PDF 처리 오류: {e}")
            
        return results
    
    def _process_table(self, table: List[List[str]]) -> Dict[str, Any]:
        """표 구조 분석 및 처리"""
        if not table or len(table) < 2:
            return {}
        
        # 헤더 추출
        headers = [cell for cell in table[0] if cell]
        
        # 데이터 행 처리
        data_rows = []
        for row in table[1:]:
            if any(row):  # 빈 행이 아닌 경우
                data_rows.append(row)
        
        # 필드-값 쌍 추출
        field_value_pairs = {}
        if len(headers) >= 2:
            for row in data_rows:
                if len(row) >= 2 and row[0] and row[1]:
                    field_name = str(row[0]).strip()
                    field_value = str(row[1]).strip()
                    field_value_pairs[field_name] = field_value
        
        return {
            'headers': headers,
            'data': data_rows,
            'field_value_pairs': field_value_pairs,
            'structure_type': self._detect_table_structure(headers, data_rows)
        }
    
    def _detect_table_structure(self, headers: List[str], data: List[List[str]]) -> str:
        """표 구조 타입 감지"""
        if not headers:
            return "unstructured"
        
        # 엔지니어링 표준 패턴 감지
        engineering_keywords = ['pressure', 'temperature', 'flow', 'capacity', 'design', 'operating']
        
        header_text = ' '.join(headers).lower()
        if any(keyword in header_text for keyword in engineering_keywords):
            return "engineering_specification"
        elif len(headers) == 2:
            return "field_value_pairs"
        else:
            return "multi_column_data"
    
    def _extract_fields_from_text(self, text: str, tables: List[Dict]) -> Dict[str, str]:
        """텍스트와 표에서 필드 추출"""
        fields = {}
        
        # 표에서 필드-값 쌍 추출
        for table in tables:
            if 'field_value_pairs' in table:
                fields.update(table['field_value_pairs'])
        
        # 텍스트에서 패턴 기반 필드 추출
        patterns = [
            r'([A-Z][A-Z\s]+)\s*:\s*([^\n]+)',  # FIELD NAME : value
            r'(\d+\s+[A-Z][A-Z\s]+)\s+([^\n]+)',  # 01 FIELD NAME value
            r'([A-Z][a-z\s]+)\s*=\s*([^\n]+)'  # Field Name = value
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for field_name, field_value in matches:
                field_name = field_name.strip()
                field_value = field_value.strip()
                if len(field_name) > 2 and len(field_value) > 0:
                    fields[field_name] = field_value
        
        return fields
    
    def _calculate_quality_score(self, text: str, tables: List[Dict]) -> float:
        """텍스트 품질 점수 계산"""
        if not text:
            return 0.0
        
        # 기본 점수
        base_score = 0.3
        
        # 표 구조 점수
        table_score = min(len(tables) * 0.2, 0.4)
        
        # 필드 추출 점수
        total_fields = sum(len(table.get('field_value_pairs', {})) for table in tables)
        field_score = min(total_fields * 0.05, 0.3)
        
        return min(base_score + table_score + field_score, 1.0)

class DocumentComparer:
    """문서 비교 엔진"""
    
    def __init__(self, semantic_matcher: SemanticMatcher):
        self.semantic_matcher = semantic_matcher
        self.comparison_cache = {}
    
    def compare_documents(self, doc1_fields: Dict, doc2_fields: Dict, 
                         gnn_embeddings=None) -> Dict[str, Any]:
        """두 문서의 필드 비교"""
        
        comparison_result = {
            'matched_fields': [],
            'doc1_unique': [],
            'doc2_unique': [],
            'field_mappings': {},
            'similarity_scores': {},
            'overall_similarity': 0.0
        }
        
        # 모든 필드 조합에 대해 유사도 계산
        similarity_matrix = {}
        for field1 in doc1_fields:
            for field2 in doc2_fields:
                sim_score = self.semantic_matcher.match_fields(
                    field1, field2, gnn_embeddings
                )
                similarity_matrix[(field1, field2)] = sim_score
        
        # 최적 매칭 찾기 (헝가리안 알고리즘 간소화)
        used_fields2 = set()
        threshold = 0.7
        
        for field1 in doc1_fields:
            best_match = None
            best_score = 0.0
            
            for field2 in doc2_fields:
                if field2 not in used_fields2:
                    score = similarity_matrix[(field1, field2)]
                    if score > best_score and score >= threshold:
                        best_score = score
                        best_match = field2
            
            if best_match:
                comparison_result['matched_fields'].append({
                    'field1': field1,
                    'field2': best_match,
                    'similarity': best_score,
                    'value1': doc1_fields[field1],
                    'value2': doc2_fields[best_match]
                })
                comparison_result['field_mappings'][field1] = best_match
                used_fields2.add(best_match)
            else:
                comparison_result['doc1_unique'].append(field1)
        
        # 매칭되지 않은 doc2 필드들
        for field2 in doc2_fields:
            if field2 not in used_fields2:
                comparison_result['doc2_unique'].append(field2)
        
        # 전체 유사도 계산
        if len(doc1_fields) > 0 or len(doc2_fields) > 0:
            matched_count = len(comparison_result['matched_fields'])
            total_fields = len(doc1_fields) + len(doc2_fields)
            comparison_result['overall_similarity'] = (2 * matched_count) / total_fields
        
        return comparison_result

class AdvancedKnowledgeGraphAnalyzer:
    """고급 지식 그래프 분석기 메인 클래스"""
    
    def __init__(self):
        self.ontology_manager = OntologyManager()
        self.semantic_matcher = SemanticMatcher(self.ontology_manager)
        self.pdf_processor = AdvancedPDFProcessor()
        self.document_comparer = DocumentComparer(self.semantic_matcher)
        
        # 데이터 저장
        self.documents = {}
        self.knowledge_graph = nx.DiGraph()
        self.gnn_model = None
        self.node_embeddings = None
    
    def process_pdf(self, pdf_file, filename: str) -> Dict[str, Any]:
        """PDF 처리 메인 함수"""
        logger.info(f"📄 Processing PDF: {filename}")
        
        # PDF에서 텍스트와 표 추출 (OCR 없음)
        extraction_result = self.pdf_processor.extract_text_and_tables(pdf_file)
        
        logger.info(f"📊 Text quality score: {extraction_result['quality_score']:.2f}")
        logger.info(f"🏷️ Extracted {len(extraction_result['fields'])} field-value pairs")
        logger.info(f"📋 Found {len(extraction_result['tables'])} tables")
        
        # 지식 그래프에 추가
        doc_id = len(self.documents) + 1
        self.documents[doc_id] = {
            'filename': filename,
            'fields': extraction_result['fields'],
            'tables': extraction_result['tables'],
            'text': extraction_result['text'],
            'quality_score': extraction_result['quality_score'],
            'processed_at': datetime.now().isoformat()
        }
        
        # 그래프 업데이트
        self._update_knowledge_graph(doc_id, extraction_result['fields'])
        
        return {
            'doc_id': doc_id,
            'field_count': len(extraction_result['fields']),
            'quality_score': extraction_result['quality_score'],
            'tables_count': len(extraction_result['tables'])
        }
    
    def _update_knowledge_graph(self, doc_id: int, fields: Dict[str, str]):
        """지식 그래프 업데이트"""
        doc_node = f"doc_{doc_id}"
        self.knowledge_graph.add_node(doc_node, type="document")
        
        for field_name, field_value in fields.items():
            # 표준화된 필드명 사용
            std_field = self.ontology_manager.find_standard_field(field_name)
            
            field_node = f"field_{std_field}_{doc_id}"
            value_node = f"value_{field_value}_{doc_id}"
            
            self.knowledge_graph.add_node(field_node, type="field", name=std_field)
            self.knowledge_graph.add_node(value_node, type="value", value=field_value)
            
            self.knowledge_graph.add_edge(doc_node, field_node, relation="has_field")
            self.knowledge_graph.add_edge(field_node, value_node, relation="has_value")
    
    def train_multi_gnn(self) -> Dict[str, Any]:
        """Multi-GNN 모델 훈련"""
        if len(self.knowledge_graph.nodes) < 5:
            return {"error": "Not enough nodes for GNN training"}
        
        logger.info("🤖 Training Multi-GNN model...")
        
        # 그래프 데이터 준비
        nodes = list(self.knowledge_graph.nodes())
        edges = list(self.knowledge_graph.edges())
        
        # 노드와 엣지 인덱스 매핑
        node_to_idx = {node: idx for idx, node in enumerate(nodes)}
        
        # 엣지 인덱스와 타입 준비
        edge_index = torch.tensor([[node_to_idx[e[0]], node_to_idx[e[1]]] for e in edges]).t()
        
        # 관계 타입 매핑
        relation_types = list(set(nx.get_edge_attributes(self.knowledge_graph, 'relation').values()))
        relation_to_idx = {rel: idx for idx, rel in enumerate(relation_types)}
        
        edge_type = torch.tensor([
            relation_to_idx.get(
                self.knowledge_graph[e[0]][e[1]].get('relation', 'unknown'), 0
            ) for e in edges
        ])
        
        # 초기 노드 특성 (identity matrix)
        num_nodes = len(nodes)
        x = torch.eye(num_nodes)
        
        # Multi-GNN 모델 초기화
        self.gnn_model = MultiGNN(
            num_nodes=num_nodes,
            num_relations=len(relation_types),
            hidden_dim=64,
            out_dim=32
        )
        
        # 훈련
        optimizer = torch.optim.Adam(self.gnn_model.parameters(), lr=0.01)
        
        self.gnn_model.train()
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for epoch in range(100):
            optimizer.zero_grad()
            
            # Forward pass
            embeddings = self.gnn_model(x, edge_index, edge_type)
            
            # 자체 지도 학습: 노드 재구성 손실
            reconstructed = self.gnn_model.classifier(embeddings)
            loss = F.mse_loss(reconstructed, x)
            
            loss.backward()
            optimizer.step()
            
            # 진행률 업데이트
            progress_bar.progress((epoch + 1) / 100)
            if epoch % 20 == 0:
                status_text.text(f"Epoch {epoch}, Loss: {loss.item():.4f}")
        
        # 학습된 임베딩 저장
        self.gnn_model.eval()
        with torch.no_grad():
            self.node_embeddings = self.gnn_model(x, edge_index, edge_type).numpy()
        
        progress_bar.empty()
        status_text.empty()
        logger.info("✅ Multi-GNN training completed")
        
        return {
            "status": "success",
            "num_nodes": num_nodes,
            "num_relations": len(relation_types),
            "final_loss": loss.item()
        }
    
    def compare_all_documents(self) -> Dict[str, Any]:
        """모든 문서 비교"""
        if len(self.documents) < 2:
            return {"error": "Need at least 2 documents for comparison"}
        
        logger.info("🔍 Comparing all documents with semantic matching...")
        
        doc_ids = list(self.documents.keys())
        comparison_results = {}
        
        for i, doc_id1 in enumerate(doc_ids):
            for doc_id2 in doc_ids[i+1:]:
                doc1_fields = self.documents[doc_id1]['fields']
                doc2_fields = self.documents[doc_id2]['fields']
                
                comparison = self.document_comparer.compare_documents(
                    doc1_fields, doc2_fields, self.node_embeddings
                )
                
                comparison_key = f"doc_{doc_id1}_vs_doc_{doc_id2}"
                comparison_results[comparison_key] = {
                    'doc1_name': self.documents[doc_id1]['filename'],
                    'doc2_name': self.documents[doc_id2]['filename'],
                    'comparison': comparison
                }
        
        return comparison_results
    
    def create_visualization_data(self) -> Dict[str, Any]:
        """시각화용 데이터 생성"""
        if not self.knowledge_graph.nodes():
            return {"nodes": [], "edges": []}
        
        # 노드 데이터
        nodes = []
        for node in self.knowledge_graph.nodes(data=True):
            node_id, node_data = node
            nodes.append({
                'id': node_id,
                'type': node_data.get('type', 'unknown'),
                'label': node_data.get('name', node_data.get('value', node_id))
            })
        
        # 엣지 데이터
        edges = []
        for edge in self.knowledge_graph.edges(data=True):
            source, target, edge_data = edge
            edges.append({
                'source': source,
                'target': target,
                'relation': edge_data.get('relation', 'unknown')
            })
        
        return {"nodes": nodes, "edges": edges}

# 세션 상태 초기화
if 'analyzer' not in st.session_state:
    st.session_state.analyzer = AdvancedKnowledgeGraphAnalyzer()
    st.success("✅ Advanced Multi-GNN Knowledge Graph Analyzer initialized")

# 메인 앱
def main():
    # 헤더
    st.markdown('<h1 class="main-header">🚀 Advanced Multi-GNN Knowledge Graph Analyzer</h1>', 
                unsafe_allow_html=True)
    
    # 시스템 상태 정보
    with st.sidebar:
        st.markdown('<h2 class="section-header">📊 System Status</h2>', unsafe_allow_html=True)
        
        analyzer = st.session_state.analyzer
        
        st.metric("Documents", len(analyzer.documents))
        st.metric("Graph Nodes", len(analyzer.knowledge_graph.nodes))
        st.metric("Graph Edges", len(analyzer.knowledge_graph.edges))
        
        if analyzer.gnn_model is not None:
            st.success("🤖 Multi-GNN Model: Trained")
        else:
            st.info("🤖 Multi-GNN Model: Not trained")
        
        st.markdown("---")
        st.markdown("### 🎯 Features")
        st.markdown("✅ OCR-free PDF processing")
        st.markdown("✅ Multi-GNN pipeline")
        st.markdown("✅ Semantic field matching")
        st.markdown("✅ Ontology integration")
    
    # 메인 탭
    tab1, tab2, tab3, tab4 = st.tabs(["📄 Document Upload", "🤖 GNN Training", "🔍 Document Comparison", "🕸️ Visualization"])
    
    with tab1:
        st.markdown('<h2 class="section-header">📄 Document Upload & Processing</h2>', 
                   unsafe_allow_html=True)
        
        uploaded_files = st.file_uploader(
            "Upload PDF documents",
            type=['pdf'],
            accept_multiple_files=True,
            help="Upload one or more PDF documents for analysis"
        )
        
        if uploaded_files:
            for uploaded_file in uploaded_files:
                if st.button(f"Process {uploaded_file.name}", key=f"process_{uploaded_file.name}"):
                    with st.spinner(f"Processing {uploaded_file.name}..."):
                        try:
                            result = st.session_state.analyzer.process_pdf(uploaded_file, uploaded_file.name)
                            
                            st.markdown(f"""
                            <div class="success-box">
                                <h4>✅ {uploaded_file.name} processed successfully</h4>
                                <p><strong>📊 Quality Score:</strong> {result['quality_score']:.2f}</p>
                                <p><strong>🏷️ Fields Extracted:</strong> {result['field_count']}</p>
                                <p><strong>📋 Tables Found:</strong> {result['tables_count']}</p>
                            </div>
                            """, unsafe_allow_html=True)
                            
                            # 추출된 필드 표시
                            if result['field_count'] > 0:
                                with st.expander("View Extracted Fields"):
                                    doc_fields = st.session_state.analyzer.documents[result['doc_id']]['fields']
                                    fields_df = pd.DataFrame([
                                        {"Field Name": k, "Value": v} 
                                        for k, v in doc_fields.items()
                                    ])
                                    st.dataframe(fields_df, use_container_width=True)
                            
                        except Exception as e:
                            st.markdown(f"""
                            <div class="error-box">
                                <h4>❌ Error processing {uploaded_file.name}</h4>
                                <p>{str(e)}</p>
                            </div>
                            """, unsafe_allow_html=True)
    
    with tab2:
        st.markdown('<h2 class="section-header">🤖 Multi-GNN Model Training</h2>', 
                   unsafe_allow_html=True)
        
        if len(st.session_state.analyzer.knowledge_graph.nodes) < 5:
            st.warning("⚠️ Need at least 5 nodes for GNN training. Please upload more documents.")
        else:
            st.info(f"📊 Current graph: {len(st.session_state.analyzer.knowledge_graph.nodes)} nodes, "
                   f"{len(st.session_state.analyzer.knowledge_graph.edges)} edges")
            
            if st.button("🚀 Train Multi-GNN Model", type="primary"):
                with st.spinner("Training Multi-GNN model..."):
                    result = st.session_state.analyzer.train_multi_gnn()
                    
                    if "error" in result:
                        st.error(f"❌ {result['error']}")
                    else:
                        st.markdown(f"""
                        <div class="success-box">
                            <h4>✅ Multi-GNN training completed!</h4>
                            <p><strong>📊 Nodes:</strong> {result['num_nodes']}</p>
                            <p><strong>🔗 Relations:</strong> {result['num_relations']}</p>
                            <p><strong>📉 Final Loss:</strong> {result['final_loss']:.6f}</p>
                        </div>
                        """, unsafe_allow_html=True)
        
        # 모델 아키텍처 설명
        with st.expander("📚 Multi-GNN Architecture Details"):
            st.markdown("""
            **Pipeline: R-GCN → GraphSAGE → GAT**
            
            1. **R-GCN (Relational Graph Convolutional Network)**
               - 관계형 그래프에서 기본적인 관계 학습
               - 다양한 관계 타입 처리
            
            2. **GraphSAGE (Graph Sample and Aggregate)**
               - 이웃 노드 정보를 샘플링하고 집계
               - 필드명 임베딩 학습
            
            3. **GAT (Graph Attention Network)**
               - 어텐션 메커니즘으로 중요한 관계에 집중
               - 문서간 구조적 비교
            """)
    
    with tab3:
        st.markdown('<h2 class="section-header">🔍 Document Comparison</h2>', 
                   unsafe_allow_html=True)
        
        if len(st.session_state.analyzer.documents) < 2:
            st.warning("⚠️ Need at least 2 documents for comparison.")
        else:
            if st.button("🔍 Compare All Documents", type="primary"):
                with st.spinner("Comparing documents with semantic matching..."):
                    results = st.session_state.analyzer.compare_all_documents()
                    
                    if "error" in results:
                        st.error(f"❌ {results['error']}")
                    else:
                        for comp_key, comp_data in results.items():
                            comparison = comp_data['comparison']
                            
                            st.markdown(f"""
                            <div class="info-box">
                                <h4>📋 {comp_data['doc1_name']} vs {comp_data['doc2_name']}</h4>
                                <p><strong>Overall Similarity:</strong> {comparison['overall_similarity']:.2f}</p>
                                <p><strong>Matched Fields:</strong> {len(comparison['matched_fields'])}</p>
                                <p><strong>Unique to Doc1:</strong> {len(comparison['doc1_unique'])}</p>
                                <p><strong>Unique to Doc2:</strong> {len(comparison['doc2_unique'])}</p>
                            </div>
                            """, unsafe_allow_html=True)
                            
                            # 매칭된 필드 표시
                            if comparison['matched_fields']:
                                with st.expander(f"View Matched Fields - {comp_data['doc1_name']} vs {comp_data['doc2_name']}"):
                                    matched_df = pd.DataFrame([
                                        {
                                            "Doc1 Field": match['field1'],
                                            "Doc2 Field": match['field2'],
                                            "Similarity": f"{match['similarity']:.3f}",
                                            "Doc1 Value": match['value1'][:50] + "..." if len(match['value1']) > 50 else match['value1'],
                                            "Doc2 Value": match['value2'][:50] + "..." if len(match['value2']) > 50 else match['value2']
                                        }
                                        for match in comparison['matched_fields']
                                    ])
                                    st.dataframe(matched_df, use_container_width=True)
        
        # 시맨틱 매칭 설명
        with st.expander("🎯 Semantic Matching Strategy"):
            st.markdown("""
            **4-Level Semantic Matching:**
            
            1. **Level 1: Ontology Direct Matching**
               - ontology.ttl 기반 표준 필드명 매칭
               - 정확도: 100% (직접 일치시)
            
            2. **Level 2: Embedding-based Matching**
               - SentenceTransformer 모델 사용
               - 코사인 유사도 계산
            
            3. **Level 3: GNN Embedding Matching**
               - 학습된 GNN 임베딩 활용
               - 구조적 컨텍스트 반영
            
            4. **Level 4: Structural Context**
               - 문서 내 위치 정보
               - 계층적 관계 고려
            """)
    
    with tab4:
        st.markdown('<h2 class="section-header">🕸️ Knowledge Graph Visualization</h2>', 
                   unsafe_allow_html=True)
        
        viz_data = st.session_state.analyzer.create_visualization_data()
        
        if not viz_data['nodes']:
            st.info("📄 No data to visualize. Please upload documents first.")
        else:
            # 네트워크 그래프 생성
            G = nx.Graph()
            for node in viz_data['nodes']:
                G.add_node(node['id'], **node)
            for edge in viz_data['edges']:
                G.add_edge(edge['source'], edge['target'])
            
            # 레이아웃 계산
            pos = nx.spring_layout(G, k=3, iterations=50)
            
            # Plotly 그래프 생성
            node_trace = go.Scatter(
                x=[pos[node['id']][0] for node in viz_data['nodes']],
                y=[pos[node['id']][1] for node in viz_data['nodes']],
                mode='markers+text',
                text=[node['label'][:20] + '...' if len(node['label']) > 20 else node['label'] 
                      for node in viz_data['nodes']],
                textposition="middle center",
                marker=dict(
                    size=[20 if node['type'] == 'document' else 15 if node['type'] == 'field' else 10 
                          for node in viz_data['nodes']],
                    color=['blue' if node['type'] == 'document' else 'green' if node['type'] == 'field' else 'orange'
                           for node in viz_data['nodes']],
                    line=dict(width=2, color='white')
                ),
                hoverinfo='text',
                hovertext=[f"Type: {node['type']}<br>Label: {node['label']}" for node in viz_data['nodes']]
            )
            
            # 엣지 트레이스
            edge_traces = []
            for edge in viz_data['edges']:
                x0, y0 = pos[edge['source']]
                x1, y1 = pos[edge['target']]
                edge_traces.append(go.Scatter(
                    x=[x0, x1, None],
                    y=[y0, y1, None],
                    mode='lines',
                    line=dict(width=1, color='gray'),
                    hoverinfo='none'
                ))
            
            # 피규어 생성
            fig = go.Figure(data=[node_trace] + edge_traces)
            fig.update_layout(
                title="Multi-GNN Enhanced Knowledge Graph",
                showlegend=False,
                hovermode='closest',
                margin=dict(b=20,l=5,r=5,t=40),
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                height=600
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # 통계 정보
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total Nodes", len(viz_data['nodes']))
            with col2:
                st.metric("Total Edges", len(viz_data['edges']))
            with col3:
                node_types = {}
                for node in viz_data['nodes']:
                    node_type = node['type']
                    node_types[node_type] = node_types.get(node_type, 0) + 1
                st.write("**Node Types:**")
                for node_type, count in node_types.items():
                    st.write(f"• {node_type}: {count}")

if __name__ == "__main__":
    main()