import os
import base64
import io
import json
import pandas as pd
import networkx as nx
import plotly.graph_objects as go
from dash import Dash, dcc, html, Input, Output, State
import dash_bootstrap_components as dbc
from collections import Counter
import tempfile
import re
import sqlite3
import time  # â† ì´ ì¤„ ì¶”ê°€
from datetime import datetime
from rgcn import RGCNManager
import cv2
import numpy as np
from paddleocr import PaddleOCR
from PIL import Image
import fitz  # PyMuPDF (PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ìš©)
from rdflib import Graph, Namespace, RDF, RDFS, OWL

# R-GCN ëª¨ë“ˆ ì„í¬íŠ¸ (ê°•ì œ í™œì„±í™”)
try:
    from rgcn import RGCNManager
    RGCN_AVAILABLE = True
    print("âœ… R-GCN module loaded successfully")
    
except ImportError as e:
    print(f"âŒ R-GCN import ì‹¤íŒ¨: {e}")
    RGCN_AVAILABLE = False
    RGCNManager = None
except Exception as e:
    print(f"âŒ R-GCN ëª¨ë“ˆ ì˜¤ë¥˜: {e}")
    RGCN_AVAILABLE = False
    RGCNManager = None

# PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì )
try:
    from pdfminer.high_level import extract_text
    PDF_AVAILABLE = True
except ImportError:
    print("PDFMiner not available - using mock text extraction")
    PDF_AVAILABLE = False

# DuckDB ì§€ì› (ì„ íƒì )
try:
    import duckdb
    DUCKDB_AVAILABLE = True
except ImportError:
    print("DuckDB not available - using SQLite instead")
    DUCKDB_AVAILABLE = False

# OCR ê´€ë ¨ ì„¤ì •
OCR_AVAILABLE = False  # ì•ˆì „í•˜ê²Œ ë¹„í™œì„±í™”
ocr_reader = None
PDF_IMAGE_DPI = 300
OCR_CONFIDENCE_THRESHOLD = 0.5


# ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤ (ìˆ˜ì •ëœ ë²„ì „)
class DatabaseManager:
    def __init__(self, db_path="knowledge_graph.db"):
        self.db_path = db_path
        self.use_duckdb = DUCKDB_AVAILABLE
        self.init_database()
    
    def get_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë°˜í™˜"""
        if self.use_duckdb:
            return duckdb.connect(self.db_path.replace('.db', '.duckdb'))
        else:
            return sqlite3.connect(self.db_path)
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì´ˆê¸°í™”"""
        conn = None
        try:
            conn = self.get_connection()
            
            # ë¬¸ì„œ í…Œì´ë¸” (OCR ê´€ë ¨ í•„ë“œ ì¶”ê°€)
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS documents (
                        id INTEGER PRIMARY KEY,
                        filename TEXT NOT NULL,
                        upload_time TEXT NOT NULL,
                        content_length INTEGER,
                        processing_time REAL,
                        text_preview TEXT,
                        pdf_type TEXT DEFAULT 'text',
                        has_images BOOLEAN DEFAULT FALSE,
                        ocr_processed BOOLEAN DEFAULT FALSE,
                        image_count INTEGER DEFAULT 0
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS documents (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        filename TEXT NOT NULL,
                        upload_time TEXT NOT NULL,
                        content_length INTEGER,
                        processing_time REAL,
                        text_preview TEXT,
                        pdf_type TEXT DEFAULT 'text',
                        has_images BOOLEAN DEFAULT FALSE,
                        ocr_processed BOOLEAN DEFAULT FALSE,
                        image_count INTEGER DEFAULT 0
                    );
                """)
            
            # ë…¸ë“œ í…Œì´ë¸” (OCR ê´€ë ¨ í•„ë“œ ì¶”ê°€)
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS nodes (
                        id TEXT PRIMARY KEY,
                        document_id INTEGER,
                        node_type TEXT,
                        ontology_class TEXT,
                        confidence REAL,
                        predicted_class INTEGER,
                        creation_time TEXT,
                        source_type TEXT DEFAULT 'text',
                        bbox_x REAL,
                        bbox_y REAL,
                        bbox_width REAL,
                        bbox_height REAL,
                        page_number INTEGER,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS nodes (
                        id TEXT PRIMARY KEY,
                        document_id INTEGER,
                        node_type TEXT,
                        ontology_class TEXT,
                        confidence REAL,
                        predicted_class INTEGER,
                        creation_time TEXT,
                        source_type TEXT DEFAULT 'text',
                        bbox_x REAL,
                        bbox_y REAL,
                        bbox_width REAL,
                        bbox_height REAL,
                        page_number INTEGER,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            
            # ì—£ì§€ í…Œì´ë¸”
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS edges (
                        id INTEGER PRIMARY KEY,
                        document_id INTEGER,
                        source_node TEXT,
                        target_node TEXT,
                        relation_type TEXT,
                        confidence REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS edges (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        document_id INTEGER,
                        source_node TEXT,
                        target_node TEXT,
                        relation_type TEXT,
                        confidence REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            
            # ì˜¨í†¨ë¡œì§€ íŒ¨í„´ í…Œì´ë¸”
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS ontology_patterns (
                        id INTEGER PRIMARY KEY,
                        pattern_type TEXT,
                        pattern_value TEXT,
                        frequency INTEGER,
                        confidence_avg REAL,
                        last_seen TEXT,
                        document_count INTEGER
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS ontology_patterns (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        pattern_type TEXT,
                        pattern_value TEXT,
                        frequency INTEGER,
                        confidence_avg REAL,
                        last_seen TEXT,
                        document_count INTEGER
                    );
                """)
            
            # ë„ë©”ì¸ ì¸ì‚¬ì´íŠ¸ í…Œì´ë¸”
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS domain_insights (
                        id INTEGER PRIMARY KEY,
                        document_id INTEGER,
                        document_type TEXT,
                        industry_domain TEXT,
                        complexity_score REAL,
                        technical_density REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS domain_insights (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        document_id INTEGER,
                        document_type TEXT,
                        industry_domain TEXT,
                        complexity_score REAL,
                        technical_density REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            
            conn.commit()
            print(f"âœ… Database initialized: {'DuckDB' if self.use_duckdb else 'SQLite'}")
            
        except Exception as e:
            print(f"âŒ Database initialization error: {e}")
        finally:
            if conn:
                try:
                    conn.close()
                except:
                    pass
    
    def save_processing_results(self, filename, text_content, graph_data, learning_results, processing_time=0, pdf_info=None):
        """ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (UPSERT ë¡œì§, OCR ì§€ì›)"""
        conn = None
        try:
            conn = self.get_connection()
            current_time = datetime.now().isoformat()
            
            # PDF ì •ë³´ ê¸°ë³¸ê°’ ì„¤ì •
            if pdf_info is None:
                pdf_info = {
                    'pdf_type': 'text',
                    'has_images': False,
                    'ocr_processed': False,
                    'image_count': 0
                }
            
            # 1. ë¬¸ì„œ ì •ë³´ ì €ì¥/ì—…ë°ì´íŠ¸ (filename ê¸°ì¤€)
            text_preview = text_content[:500] if text_content else ""
            
            if self.use_duckdb:
                # DuckDB ì²˜ë¦¬
                existing_doc = conn.execute("SELECT id FROM documents WHERE filename = ?", [filename]).fetchone()
                
                if existing_doc:
                    # ê¸°ì¡´ ë¬¸ì„œ ì—…ë°ì´íŠ¸
                    doc_id = existing_doc[0]
                    conn.execute("""
                        UPDATE documents 
                        SET upload_time = ?, content_length = ?, processing_time = ?, text_preview = ?,
                            pdf_type = ?, has_images = ?, ocr_processed = ?, image_count = ?
                        WHERE id = ?
                    """, [
                        current_time, len(text_content), processing_time, text_preview,
                        pdf_info['pdf_type'], pdf_info['has_images'], 
                        pdf_info['ocr_processed'], pdf_info['image_count'], doc_id
                    ])
                    print(f"ğŸ“ ê¸°ì¡´ ë¬¸ì„œ ì—…ë°ì´íŠ¸: {filename} (ID: {doc_id})")
                    
                    # ê¸°ì¡´ ê´€ë ¨ ë°ì´í„° ì‚­ì œ (ìƒˆë¡œ ì €ì¥í•˜ê¸° ìœ„í•´)
                    conn.execute("DELETE FROM nodes WHERE document_id = ?", [doc_id])
                    conn.execute("DELETE FROM edges WHERE document_id = ?", [doc_id])
                    conn.execute("DELETE FROM domain_insights WHERE document_id = ?", [doc_id])
                    print(f"ğŸ—‘ï¸ ê¸°ì¡´ ë…¸ë“œ/ì—£ì§€/ì¸ì‚¬ì´íŠ¸ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
                    
                else:
                    # ìƒˆ ë¬¸ì„œ ì‚½ì…
                    doc_count = conn.execute("SELECT COUNT(*) FROM documents").fetchone()[0]
                    doc_id = doc_count + 1
                    
                    conn.execute("""
                        INSERT INTO documents (id, filename, upload_time, content_length, processing_time, text_preview,
                                             pdf_type, has_images, ocr_processed, image_count)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, [
                        doc_id, filename, current_time, len(text_content), processing_time, text_preview,
                        pdf_info['pdf_type'], pdf_info['has_images'], 
                        pdf_info['ocr_processed'], pdf_info['image_count']
                    ])
                    print(f"ğŸ“„ ìƒˆ ë¬¸ì„œ ìƒì„±: {filename} (ID: {doc_id})")
            else:
                # SQLite ì²˜ë¦¬ (UPSERT ë¡œì§ ì¶”ê°€)
                cursor = conn.cursor()
                
                # ê¸°ì¡´ ë¬¸ì„œ í™•ì¸
                cursor.execute("SELECT id FROM documents WHERE filename = ?", (filename,))
                existing_doc = cursor.fetchone()
                
                if existing_doc:
                    # ê¸°ì¡´ ë¬¸ì„œ ì—…ë°ì´íŠ¸
                    doc_id = existing_doc[0]
                    cursor.execute("""
                        UPDATE documents 
                        SET upload_time = ?, content_length = ?, processing_time = ?, text_preview = ?,
                            pdf_type = ?, has_images = ?, ocr_processed = ?, image_count = ?
                        WHERE id = ?
                    """, (
                        current_time, len(text_content), processing_time, text_preview,
                        pdf_info['pdf_type'], pdf_info['has_images'], 
                        pdf_info['ocr_processed'], pdf_info['image_count'], doc_id
                    ))
                    print(f"ğŸ“ ê¸°ì¡´ ë¬¸ì„œ ì—…ë°ì´íŠ¸: {filename} (ID: {doc_id})")
                    
                    # ê¸°ì¡´ ê´€ë ¨ ë°ì´í„° ì‚­ì œ
                    cursor.execute("DELETE FROM nodes WHERE document_id = ?", (doc_id,))
                    cursor.execute("DELETE FROM edges WHERE document_id = ?", (doc_id,))
                    cursor.execute("DELETE FROM domain_insights WHERE document_id = ?", (doc_id,))
                    print(f"ğŸ—‘ï¸ ê¸°ì¡´ ë…¸ë“œ/ì—£ì§€/ì¸ì‚¬ì´íŠ¸ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
                    
                else:
                    # ìƒˆ ë¬¸ì„œ ì‚½ì…
                    cursor.execute("""
                        INSERT INTO documents (filename, upload_time, content_length, processing_time, text_preview,
                                             pdf_type, has_images, ocr_processed, image_count)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        filename, current_time, len(text_content), processing_time, text_preview,
                        pdf_info['pdf_type'], pdf_info['has_images'], 
                        pdf_info['ocr_processed'], pdf_info['image_count']
                    ))
                    doc_id = cursor.lastrowid
                    print(f"ğŸ“„ ìƒˆ ë¬¸ì„œ ìƒì„±: {filename} (ID: {doc_id})")
            
            # 2. ë…¸ë“œ ì €ì¥
            nodes = graph_data.get("nodes", [])
            predictions = graph_data.get("predictions", [])
            
            print(f"ğŸ’¾ ë…¸ë“œ ì €ì¥ ì‹œì‘: {len(nodes)}ê°œ")
            
            for i, node in enumerate(nodes):
                predicted_class = predictions[i] if i < len(predictions) else 0
                node_id = node["id"]
                
                # OCR ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
                bbox = node.get("bbox", {})
                
                if self.use_duckdb:
                    # DuckDB: ìƒˆ ë…¸ë“œ ì‚½ì… (ê¸°ì¡´ ë°ì´í„°ëŠ” ì´ë¯¸ ì‚­ì œë¨)
                    conn.execute("""
                        INSERT INTO nodes (id, document_id, node_type, ontology_class, confidence, predicted_class, 
                                         creation_time, source_type, bbox_x, bbox_y, bbox_width, bbox_height, page_number)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, [
                        node_id, doc_id, node.get("type", "unknown"), 
                        node.get("ontology_class", "Unknown"), node.get("confidence", 0.5),
                        predicted_class, current_time, node.get("source_type", "text"),
                        bbox.get("x"), bbox.get("y"), bbox.get("width"), bbox.get("height"), 
                        node.get("page_number")
                    ])
                else:
                    # SQLite: ìƒˆ ë…¸ë“œ ì‚½ì… (ê¸°ì¡´ ë°ì´í„°ëŠ” ì´ë¯¸ ì‚­ì œë¨)
                    conn.execute("""
                        INSERT INTO nodes (id, document_id, node_type, ontology_class, confidence, predicted_class, 
                                         creation_time, source_type, bbox_x, bbox_y, bbox_width, bbox_height, page_number)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        node_id, doc_id, node.get("type", "unknown"), 
                        node.get("ontology_class", "Unknown"), node.get("confidence", 0.5),
                        predicted_class, current_time, node.get("source_type", "text"),
                        bbox.get("x"), bbox.get("y"), bbox.get("width"), bbox.get("height"), 
                        node.get("page_number")
                    ))
            
            print(f"âœ… ë…¸ë“œ ì €ì¥ ì™„ë£Œ: {len(nodes)}ê°œ")
                
            # 3. ì—£ì§€ ì €ì¥
            edges = graph_data.get("edges", [])
            print(f"ğŸ”— ì—£ì§€ ì €ì¥ ì‹œì‘: {len(edges)}ê°œ")
            
            if self.use_duckdb:
                # DuckDB: ì—£ì§€ ì €ì¥
                max_edge_result = conn.execute("SELECT MAX(id) FROM edges").fetchone()
                edge_id = (max_edge_result[0] if max_edge_result[0] is not None else 0) + 1
                
                for edge in edges:
                    conn.execute("""
                        INSERT INTO edges (id, document_id, source_node, target_node, relation_type, confidence, creation_time)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, [
                        edge_id, doc_id, edge.get("source", ""), edge.get("target", ""),
                        edge.get("relation", "unknown"), 0.8, current_time
                    ])
                    edge_id += 1
            else:
                # SQLite: ì—£ì§€ ì €ì¥
                for edge in edges:
                    conn.execute("""
                        INSERT INTO edges (document_id, source_node, target_node, relation_type, confidence, creation_time)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        doc_id, edge.get("source", ""), edge.get("target", ""),
                        edge.get("relation", "unknown"), 0.8, current_time
                    ))
            
            print(f"âœ… ì—£ì§€ ì €ì¥ ì™„ë£Œ: {len(edges)}ê°œ")
                
            # 4. ì˜¨í†¨ë¡œì§€ íŒ¨í„´ ì €ì¥
            patterns = learning_results.get("patterns", {})
            confidence_scores = learning_results.get("confidence_scores", {})
            
            print(f"ğŸ§  ì˜¨í†¨ë¡œì§€ íŒ¨í„´ ì €ì¥ ì‹œì‘")
            
            for pattern_type, pattern_list in patterns.items():
                for pattern_value in pattern_list:
                    pattern_str = str(pattern_value)[:200]  # ê¸¸ì´ ì œí•œ
                    avg_confidence = sum(confidence_scores.values()) / max(len(confidence_scores), 1)
                    
                    if self.use_duckdb:
                        # DuckDB: íŒ¨í„´ UPSERT
                        existing_pattern = conn.execute("""
                            SELECT id, frequency, document_count FROM ontology_patterns 
                            WHERE pattern_type = ? AND pattern_value = ?
                        """, [pattern_type, pattern_str]).fetchone()
                        
                        if existing_pattern:
                            # ì—…ë°ì´íŠ¸
                            conn.execute("""
                                UPDATE ontology_patterns 
                                SET frequency = ?, confidence_avg = ?, last_seen = ?, document_count = ?
                                WHERE id = ?
                            """, [
                                existing_pattern[1] + 1, avg_confidence, current_time, 
                                existing_pattern[2] + 1, existing_pattern[0]
                            ])
                        else:
                            # ì‚½ì…
                            max_pattern_result = conn.execute("SELECT MAX(id) FROM ontology_patterns").fetchone()
                            pattern_id = (max_pattern_result[0] if max_pattern_result[0] is not None else 0) + 1
                            
                            conn.execute("""
                                INSERT INTO ontology_patterns (id, pattern_type, pattern_value, frequency, confidence_avg, last_seen, document_count)
                                VALUES (?, ?, ?, ?, ?, ?, ?)
                            """, [pattern_id, pattern_type, pattern_str, 1, avg_confidence, current_time, 1])
                    else:
                        # SQLite: íŒ¨í„´ ì‚½ì… (ë‹¨ìˆœí™”)
                        conn.execute("""
                            INSERT INTO ontology_patterns (pattern_type, pattern_value, frequency, confidence_avg, last_seen, document_count)
                            VALUES (?, ?, ?, ?, ?, ?)
                        """, (pattern_type, pattern_str, 1, avg_confidence, current_time, 1))
            
            print(f"âœ… ì˜¨í†¨ë¡œì§€ íŒ¨í„´ ì €ì¥ ì™„ë£Œ")
                
            # 5. ë„ë©”ì¸ ì¸ì‚¬ì´íŠ¸ ì €ì¥
            domain_insights = learning_results.get("domain_insights", {})
            if domain_insights:
                print(f"ğŸ¯ ë„ë©”ì¸ ì¸ì‚¬ì´íŠ¸ ì €ì¥ ì‹œì‘")
                
                if self.use_duckdb:
                    max_insight_result = conn.execute("SELECT MAX(id) FROM domain_insights").fetchone()
                    insight_id = (max_insight_result[0] if max_insight_result[0] is not None else 0) + 1
                    
                    conn.execute("""
                        INSERT INTO domain_insights (id, document_id, document_type, industry_domain, complexity_score, technical_density, creation_time)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, [
                        insight_id, doc_id, domain_insights.get("document_type", "unknown"),
                        domain_insights.get("industry_domain", "unknown"),
                        domain_insights.get("complexity_score", 0),
                        domain_insights.get("technical_density", 0),
                        current_time
                    ])
                else:
                    conn.execute("""
                        INSERT INTO domain_insights (document_id, document_type, industry_domain, complexity_score, technical_density, creation_time)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        doc_id, domain_insights.get("document_type", "unknown"),
                        domain_insights.get("industry_domain", "unknown"),
                        domain_insights.get("complexity_score", 0),
                        domain_insights.get("technical_density", 0),
                        current_time
                    ))
                
                print(f"âœ… ë„ë©”ì¸ ì¸ì‚¬ì´íŠ¸ ì €ì¥ ì™„ë£Œ")
            
            # íŠ¸ëœì­ì…˜ ì»¤ë°‹
            conn.commit()
            
            print(f"ğŸ‰ ì „ì²´ ë°ì´í„° ì €ì¥ ì™„ë£Œ - Document ID: {doc_id}")
            print(f"ğŸ“Š ì €ì¥ëœ ë°ì´í„°: ë…¸ë“œ {len(nodes)}ê°œ, ì—£ì§€ {len(edges)}ê°œ")
            return doc_id
            
        except Exception as e:
            print(f"âŒ Database save error: {e}")
            import traceback
            traceback.print_exc()
            return None
        finally:
            if conn:
                try:
                    conn.close()
                except:
                    pass
    
    def execute_query(self, query):
        """SQL ì¿¼ë¦¬ ì‹¤í–‰"""
        conn = None
        try:
            conn = self.get_connection()
            
            if self.use_duckdb:
                result = conn.execute(query).fetch_df()
            else:
                result = pd.read_sql_query(query, conn)
            
            return result
            
        except Exception as e:
            print(f"âŒ Query execution error: {e}")
            return pd.DataFrame()
        finally:
            if conn:
                try:
                    conn.close()
                except:
                    pass
    
    def get_database_stats(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ë°˜í™˜"""
        try:
            stats = {}
            
            # ê¸°ë³¸ í†µê³„
            stats["total_documents"] = self.execute_query("SELECT COUNT(*) as count FROM documents")["count"].iloc[0]
            stats["total_nodes"] = self.execute_query("SELECT COUNT(*) as count FROM nodes")["count"].iloc[0]
            stats["total_edges"] = self.execute_query("SELECT COUNT(*) as count FROM edges")["count"].iloc[0]
            stats["total_patterns"] = self.execute_query("SELECT COUNT(*) as count FROM ontology_patterns")["count"].iloc[0]
            
            # OCR ê´€ë ¨ í†µê³„ ì¶”ê°€
            ocr_stats = self.execute_query("""
                SELECT 
                    pdf_type,
                    COUNT(*) as count,
                    SUM(CASE WHEN ocr_processed THEN 1 ELSE 0 END) as ocr_processed_count,
                    SUM(image_count) as total_images
                FROM documents 
                GROUP BY pdf_type
            """)
            stats["ocr_statistics"] = ocr_stats.to_dict('records') if not ocr_stats.empty else []
            
            # ìµœê·¼ ë¬¸ì„œ
            recent_docs = self.execute_query("""
                SELECT filename, upload_time, content_length, pdf_type, ocr_processed
                FROM documents 
                ORDER BY upload_time DESC 
                LIMIT 5
            """)
            stats["recent_documents"] = recent_docs.to_dict('records') if not recent_docs.empty else []
            
            # ë…¸ë“œ íƒ€ì…ë³„ ë¶„í¬
            node_distribution = self.execute_query("""
                SELECT node_type, COUNT(*) as count 
                FROM nodes 
                GROUP BY node_type
            """)
            stats["node_distribution"] = node_distribution.to_dict('records') if not node_distribution.empty else []
            
            # ì†ŒìŠ¤ íƒ€ì…ë³„ ë¶„í¬ (í…ìŠ¤íŠ¸ vs OCR)
            source_distribution = self.execute_query("""
                SELECT source_type, COUNT(*) as count 
                FROM nodes 
                GROUP BY source_type
            """)
            stats["source_distribution"] = source_distribution.to_dict('records') if not source_distribution.empty else []
            
            # ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ë³„ ë¶„í¬  
            class_distribution = self.execute_query("""
                SELECT ontology_class, COUNT(*) as count, AVG(confidence) as avg_confidence
                FROM nodes 
                GROUP BY ontology_class 
                ORDER BY count DESC
            """)
            stats["class_distribution"] = class_distribution.to_dict('records') if not class_distribution.empty else []
            
            # ê³ ì‹ ë¢°ë„ ì—”í‹°í‹° ìˆ˜
            high_confidence_count = self.execute_query("""
                SELECT COUNT(*) as count 
                FROM nodes 
                WHERE confidence > 0.8
            """)["count"].iloc[0]
            stats["high_confidence_entities"] = high_confidence_count
            
            return stats
            
        except Exception as e:
            print(f"âŒ Database stats error: {e}")
            return {}

def initialize_ocr():
    """OCR ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global OCR_AVAILABLE, ocr_reader
    
    # OCRì´ ì „ì—­ì ìœ¼ë¡œ ë¹„í™œì„±í™”ëœ ê²½ìš° ê±´ë„ˆë›°ê¸°
    if not OCR_AVAILABLE:
        print("â­ï¸ OCRì´ ì „ì—­ì ìœ¼ë¡œ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return False
    
    try:
        print("ğŸ”„ PaddleOCR ì´ˆê¸°í™” ì¤‘...")
        
        # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import í™•ì¸
        try:
            from paddleocr import PaddleOCR
            import cv2
            import numpy as np
        except ImportError as e:
            print(f"âŒ OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {e}")
            OCR_AVAILABLE = False
            return False
        
        # PaddleOCR ì´ˆê¸°í™” (ì•ˆì „í•œ ì„¤ì •)
        ocr_reader = PaddleOCR(
            use_angle_cls=False,    # íšŒì „ ê°ì§€ ë¹„í™œì„±í™” (ì•ˆì •ì„±)
            lang='en',              # ì˜ì–´ ì¸ì‹
            show_log=False,         # ë¡œê·¸ ìˆ¨ê¸°ê¸°
            use_gpu=False,          # CPU ì‚¬ìš©
            enable_mkldnn=False,    # ë©€í‹°ìŠ¤ë ˆë”© ë¹„í™œì„±í™” (ì•ˆì •ì„±)
            cpu_threads=1           # ë‹¨ì¼ ìŠ¤ë ˆë“œ (ì¶©ëŒ ë°©ì§€)
        )
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        print("ğŸ§ª OCR í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_image = np.ones((100, 100, 3), dtype=np.uint8) * 255  # í°ìƒ‰ ì´ë¯¸ì§€
        test_result = ocr_reader.ocr(test_image, cls=False)
        
        OCR_AVAILABLE = True
        print("âœ… PaddleOCR ì´ˆê¸°í™” ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ PaddleOCR ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        OCR_AVAILABLE = False
        ocr_reader = None
        return False

def get_ocr_status():
    """OCR ìƒíƒœ í™•ì¸ (ê°œì„ ëœ ë²„ì „)"""
    return {
        'available': OCR_AVAILABLE,
        'reader_loaded': ocr_reader is not None,
        'settings': {
            'dpi': PDF_IMAGE_DPI,
            'confidence_threshold': OCR_CONFIDENCE_THRESHOLD,
            'enabled_globally': OCR_AVAILABLE  # ì „ì—­ ì„¤ì • ìƒíƒœ
        },
        'recommendation': 'OCR is disabled for better performance' if not OCR_AVAILABLE else 'OCR is ready'
    }
def enable_ocr():
    """OCRì„ í™œì„±í™”í•˜ê³  ì´ˆê¸°í™”"""
    global OCR_AVAILABLE
    
    print("ğŸ”„ OCR í™œì„±í™” ì‹œë„...")
    OCR_AVAILABLE = True  # ì „ì—­ í”Œë˜ê·¸ í™œì„±í™”
    
    if initialize_ocr():
        print("âœ… OCR í™œì„±í™” ì„±ê³µ")
        return True
    else:
        print("âŒ OCR í™œì„±í™” ì‹¤íŒ¨")
        OCR_AVAILABLE = False
        return False

def disable_ocr():
    """OCRì„ ë¹„í™œì„±í™”"""
    global OCR_AVAILABLE, ocr_reader
    
    print("â¹ï¸ OCR ë¹„í™œì„±í™”...")
    OCR_AVAILABLE = False
    ocr_reader = None
    print("âœ… OCR ë¹„í™œì„±í™” ì™„ë£Œ")

# ì¡°ê±´ë¶€ OCR ì‚¬ìš©ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
def should_use_ocr(text_length, pdf_type):
    """OCR ì‚¬ìš© ì—¬ë¶€ë¥¼ ê²°ì •"""
    MIN_TEXT_THRESHOLD = 100
    
    if not OCR_AVAILABLE or not ocr_reader:
        return False, "OCR not available"
    
    if text_length < MIN_TEXT_THRESHOLD:
        return True, f"Text too short ({text_length} < {MIN_TEXT_THRESHOLD})"
    
    if pdf_type == 'image':
        return True, "Image-based PDF detected"
    
    return False, "Sufficient text available"

def detect_pdf_type(pdf_path):
    """PDF íƒ€ì… íŒë³„ (í…ìŠ¤íŠ¸/ì´ë¯¸ì§€/í•˜ì´ë¸Œë¦¬ë“œ)"""
    try:
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        text_pages = 0
        image_pages = 0
        total_images = 0
        
        print(f"ğŸ“„ PDF ë¶„ì„ ì¤‘: {total_pages}í˜ì´ì§€")
        
        for page_num in range(total_pages):
            page = doc[page_num]
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            text = page.get_text().strip()
            if len(text) > 50:  # 50ì ì´ìƒì´ë©´ í…ìŠ¤íŠ¸ í˜ì´ì§€ë¡œ ê°„ì£¼
                text_pages += 1
            
            # ì´ë¯¸ì§€ ê°œìˆ˜ í™•ì¸
            image_list = page.get_images()
            if image_list:
                image_pages += 1
                total_images += len(image_list)
        
        doc.close()
        
        # íƒ€ì… ê²°ì • ë¡œì§
        text_ratio = text_pages / total_pages if total_pages > 0 else 0
        image_ratio = image_pages / total_pages if total_pages > 0 else 0
        
        if text_ratio > 0.8:
            pdf_type = 'text'
        elif image_ratio > 0.5 or total_images > total_pages:
            pdf_type = 'image'
        else:
            pdf_type = 'hybrid'
        
        result = {
            'pdf_type': pdf_type,
            'total_pages': total_pages,
            'text_pages': text_pages,
            'image_pages': image_pages,
            'total_images': total_images,
            'text_ratio': text_ratio,
            'image_ratio': image_ratio,
            'has_images': total_images > 0
        }
        
        print(f"ğŸ“Š PDF íƒ€ì…: {pdf_type} (í…ìŠ¤íŠ¸: {text_pages}p, ì´ë¯¸ì§€: {image_pages}p)")
        return result
        
    except Exception as e:
        print(f"âŒ PDF íƒ€ì… íŒë³„ ì‹¤íŒ¨: {e}")
        return {
            'pdf_type': 'unknown',
            'total_pages': 0,
            'text_pages': 0,
            'image_pages': 0,
            'total_images': 0,
            'text_ratio': 0,
            'image_ratio': 0,
            'has_images': False
        }
    
def preprocess_image(image):
    """OpenCVë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (OCR ì •í™•ë„ í–¥ìƒ) - ê°œì„ ëœ ë²„ì „"""
    
    # OCRì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ì›ë³¸ ë°˜í™˜
    if not OCR_AVAILABLE:
        return image
    
    try:
        # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
        try:
            import cv2
            import numpy as np
            from PIL import Image
        except ImportError as e:
            print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ: {e}")
            return image
        
        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        if isinstance(image, Image.Image):
            image_np = np.array(image)
        else:
            image_np = image
        
        # ì…ë ¥ ê²€ì¦
        if image_np is None or image_np.size == 0:
            print("âš ï¸ ë¹ˆ ì´ë¯¸ì§€ì…ë‹ˆë‹¤.")
            return image
        
        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        if len(image_np.shape) == 3:
            if image_np.shape[2] == 4:  # RGBA
                gray = cv2.cvtColor(image_np, cv2.COLOR_RGBA2GRAY)
            else:  # RGB
                gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
        else:
            gray = image_np
        
        # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ (ë„ˆë¬´ ì‘ìœ¼ë©´ ì „ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°)
        height, width = gray.shape
        if height < 32 or width < 32:
            print("âš ï¸ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ì•„ ì „ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return Image.fromarray(gray) if not isinstance(image, Image.Image) else image
        
        # ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„  íŒŒì´í”„ë¼ì¸ (ë‹¨ê³„ë³„ ì˜¤ë¥˜ ì²˜ë¦¬)
        processed = gray.copy()
        
        try:
            # 1. ë…¸ì´ì¦ˆ ì œê±° (ë¸”ëŸ¬ í¬ê¸° ì¡°ì •)
            blur_size = 3 if min(height, width) > 100 else 1
            if blur_size > 1:
                processed = cv2.medianBlur(processed, blur_size)
        except Exception as e:
            print(f"âš ï¸ ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
        
        try:
            # 2. ëŒ€ë¹„ ê°œì„  (CLAHE)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            processed = clahe.apply(processed)
        except Exception as e:
            print(f"âš ï¸ ëŒ€ë¹„ ê°œì„  ì‹¤íŒ¨: {e}")
        
        try:
            # 3. ì„ íƒì  ìƒ¤í”„ë‹ (ì‘ì€ ì´ë¯¸ì§€ëŠ” ê±´ë„ˆë›°ê¸°)
            if min(height, width) > 50:
                kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]], dtype=np.float32)
                processed = cv2.filter2D(processed, -1, kernel)
                processed = np.clip(processed, 0, 255).astype(np.uint8)
        except Exception as e:
            print(f"âš ï¸ ìƒ¤í”„ë‹ ì‹¤íŒ¨: {e}")
        
        try:
            # 4. ì ì‘í˜• ì´ì§„í™” (Otsuë³´ë‹¤ ì•ˆì „)
            processed = cv2.adaptiveThreshold(
                processed, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                cv2.THRESH_BINARY, 11, 2
            )
        except Exception as e:
            print(f"âš ï¸ ì´ì§„í™” ì‹¤íŒ¨, Otsu ì‚¬ìš©: {e}")
            try:
                _, processed = cv2.threshold(processed, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            except:
                pass  # ì´ì§„í™”ë„ ì‹¤íŒ¨í•˜ë©´ ê·¸ëƒ¥ ì§„í–‰
        
        try:
            # 5. ëª¨í´ë¡œì§€ ì—°ì‚° (ì„ íƒì )
            if min(height, width) > 50:
                kernel = np.ones((2,2), np.uint8)
                processed = cv2.morphologyEx(processed, cv2.MORPH_CLOSE, kernel)
        except Exception as e:
            print(f"âš ï¸ ëª¨í´ë¡œì§€ ì—°ì‚° ì‹¤íŒ¨: {e}")
        
        # PIL Imageë¡œ ë³€í™˜
        if isinstance(image, Image.Image):
            return Image.fromarray(processed)
        else:
            return processed
        
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ì „ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ ì•ˆì „í•œ ì›ë³¸ ë°˜í™˜
        try:
            if isinstance(image, Image.Image):
                return image
            else:
                return Image.fromarray(image) if hasattr(image, 'shape') else image
        except:
            print("âŒ ì›ë³¸ ì´ë¯¸ì§€ ë°˜í™˜ë„ ì‹¤íŒ¨")
            return image

def pdf_to_images(pdf_path, dpi=None):
    """PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ - ê°œì„ ëœ ë²„ì „"""
    
    # OCRì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if not OCR_AVAILABLE:
        print("â­ï¸ OCRì´ ë¹„í™œì„±í™”ë˜ì–´ PDF ì´ë¯¸ì§€ ë³€í™˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []
    
    if dpi is None:
        dpi = PDF_IMAGE_DPI
    
    doc = None
    try:
        # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
        try:
            import fitz  # PyMuPDF
            from PIL import Image
            import io
        except ImportError as e:
            print(f"âŒ PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ: {e}")
            return []
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(pdf_path):
            print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
            return []
        
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        images = []
        
        # í˜ì´ì§€ ìˆ˜ ì œí•œ (ë©”ëª¨ë¦¬ ë³´í˜¸)
        MAX_PAGES = 10
        if total_pages > MAX_PAGES:
            print(f"âš ï¸ í˜ì´ì§€ ìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤ ({total_pages}). ì²˜ìŒ {MAX_PAGES}í˜ì´ì§€ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            total_pages = MAX_PAGES
        
        print(f"ğŸ–¼ï¸ PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ì¤‘ (DPI: {dpi}, í˜ì´ì§€: {total_pages})")
        
        for page_num in range(total_pages):
            try:
                page = doc[page_num]
                
                # DPI ì¡°ì • (ë„ˆë¬´ ë†’ìœ¼ë©´ ë©”ëª¨ë¦¬ ë¬¸ì œ)
                safe_dpi = min(dpi, 200)  # ìµœëŒ€ 200 DPIë¡œ ì œí•œ
                
                # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                mat = fitz.Matrix(safe_dpi/72, safe_dpi/72)  # 72 DPIê°€ ê¸°ë³¸ê°’
                pix = page.get_pixmap(matrix=mat)
                
                # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ (ë„ˆë¬´ í¬ë©´ ê±´ë„ˆë›°ê¸°)
                if pix.width * pix.height > 4000000:  # 4MP ì´ìƒ
                    print(f"âš ï¸ í˜ì´ì§€ {page_num + 1}: ì´ë¯¸ì§€ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                img_data = pix.tobytes("ppm")
                
                # PIL Imageë¡œ ë³€í™˜
                image = Image.open(io.BytesIO(img_data))
                
                # ì „ì²˜ë¦¬ ì ìš© (ì¡°ê±´ë¶€)
                try:
                    processed_image = preprocess_image(image)
                except Exception as e:
                    print(f"âš ï¸ í˜ì´ì§€ {page_num + 1} ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    processed_image = image  # ì „ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ ì›ë³¸ ì‚¬ìš©
                
                # ê²°ê³¼ ì €ì¥
                page_result = {
                    'page_number': page_num + 1,
                    'image': processed_image,
                    'original_size': image.size,
                    'processed_size': processed_image.size if processed_image != image else image.size,
                    'dpi_used': safe_dpi
                }
                
                images.append(page_result)
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                pix = None
                
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {page_num + 1} ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue  # ì´ í˜ì´ì§€ëŠ” ê±´ë„ˆë›°ê³  ë‹¤ìŒ í˜ì´ì§€ ì²˜ë¦¬
        
        print(f"âœ… PDF ë³€í™˜ ì™„ë£Œ: {len(images)}í˜ì´ì§€ (ì´ {len(doc)}í˜ì´ì§€ ì¤‘)")
        return images
        
    except Exception as e:
        print(f"âŒ PDF ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return []
        
    finally:
        # ì•ˆì „í•œ ë¬¸ì„œ í•´ì œ
        if doc:
            try:
                doc.close()
            except:
                pass
        
def extract_pdf_text(pdf_path, debug_mode=False):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ - ê°œì„ ëœ ë²„ì „"""
    
    if PDF_AVAILABLE:
        try:
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(pdf_path):
                print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
                return ""
            
            # pdfminerë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = extract_text(pdf_path)
            
            # í…ìŠ¤íŠ¸ ê¸°ë³¸ ì •ë¦¬
            if text:
                text = text.strip()
                # ê³¼ë„í•œ ê³µë°± ì •ë¦¬
                text = re.sub(r'\n\s*\n', '\n\n', text)  # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
                text = re.sub(r' +', ' ', text)  # ì—°ì†ëœ ê³µë°± ì •ë¦¬
            
            print(f"ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text)} characters")
            
            # ì„ íƒì  ë””ë²„ê¹… (ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”)
            if debug_mode and text:
                print("ğŸ” PDF TEXT DEBUG:")
                print("="*50)
                print(text[:500])  # ì²« 500ìë§Œ ì¶œë ¥ (ì¶•ì†Œ)
                print("="*50)
                
                # í•µì‹¬ í‚¤ì›Œë“œë§Œ ê²€ìƒ‰
                keywords = ["REVISION", "REV", "PROJECT", "JOB", "EQUIPMENT"]
                found_keywords = [kw for kw in keywords if kw in text.upper()]
                if found_keywords:
                    print(f"ğŸ” Key terms found: {found_keywords}")
                print("="*50)
            
            return text if text else ""
            
        except Exception as e:
            print(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # Fallback: PyMuPDF ì‹œë„
            try:
                import fitz
                doc = fitz.open(pdf_path)
                text = ""
                for page in doc:
                    text += page.get_text()
                doc.close()
                
                if text.strip():
                    print(f"âœ… PyMuPDF fallback ì„±ê³µ: {len(text)} characters")
                    return text.strip()
                else:
                    print("âš ï¸ PyMuPDF fallback: í…ìŠ¤íŠ¸ ì—†ìŒ")
                    
            except Exception as fallback_error:
                print(f"âŒ PyMuPDF fallback ì‹¤íŒ¨: {fallback_error}")
            
            return ""
    else:
        # PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ëŠ” ê²½ìš° Mock ë°ì´í„° ë°˜í™˜
        print("âš ï¸ PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - Mock í…ìŠ¤íŠ¸ ì‚¬ìš©")
        return """Industrial Equipment Specification
Project: 7T04 - Centrifugal Pump Process Data
Equipment ID: P-2105 A/B
Pump Type: CENTRIFUGAL
Driver Type: MOTOR / MOTOR
Capacity: 71.1 m3/h (AM Feed)
Temperature: 384 â„ƒ
Pressure: 17.1 kg/cm2A
Viscosity: 1.0 cP
Revision: 14
Checked by: HJL / SKL
Date: 2012-12-26
JOB NO: 7T04
PROJECT: ERC 12345
DOC. NO.: 7T04-PR-21-DS-505
ITEM NO: P-2105 A/B
SERVICE: OVERFLASH PUMPS
PUMP TYPE: CENTRIFUGAL PUMP
REQUIRED TYPE: HORIZONTAL
DRIVER TYPE: MOTOR
OPERATING: ONE CONTINUOUS
STAND-BY: MOTOR ONE
LIQUID NAME: AM FEED, AH FEED, OVERFLASH
PUMPING TEMPERATURE: 384â„ƒ
VAPOR PRESSURE: 0.071 kg/cm2A
SPECIFIC GRAVITY: 1.0
VISCOSITY: 1.0 cP
CAPACITY: 71.1 m3/hr
SUCTION PRESSURE: 5.9 kg/cm2g
DIFFERENTIAL HEAD: 17.1 kg/cm2
BY/CHECKED: HJL / SKL, WSJ / WGK
REVISION: 14
DATE: 2012-12-26"""

# ë””ë²„ê¹…ìš© í—¬í¼ í•¨ìˆ˜ (ì„ íƒì  ì‚¬ìš©)
def debug_pdf_text(pdf_path, keywords=None):
    """PDF í…ìŠ¤íŠ¸ ë””ë²„ê¹… ì „ìš© í•¨ìˆ˜"""
    if keywords is None:
        keywords = ["REVISION", "PROJECT", "EQUIPMENT", "PUMP", "MOTOR"]
    
    text = extract_pdf_text(pdf_path, debug_mode=False)
    
    if not text:
        print("âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
        return
    
    print("ğŸ” DETAILED PDF TEXT DEBUG:")
    print("="*60)
    print(f"Total length: {len(text)} characters")
    print(f"Lines: {len(text.splitlines())}")
    print("="*60)
    print("First 1000 characters:")
    print(text[:1000])
    print("="*60)
    
    print("ğŸ” KEYWORD ANALYSIS:")
    for keyword in keywords:
        if keyword.upper() in text.upper():
            print(f"   âœ… Found: {keyword}")
            # ì»¨í…ìŠ¤íŠ¸ í‘œì‹œ
            idx = text.upper().find(keyword.upper())
            context = text[max(0, idx-30):idx+50]
            print(f"      Context: {repr(context)}")
        else:
            print(f"   âŒ Missing: {keyword}")
    print("="*60)

def extract_pdf_content(pdf_path):
    """PDFì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ (ì¡°ê±´ë¶€ OCR ì§€ì›) - ê°œì„ ëœ ë²„ì „"""
    try:
        print(f"ğŸ“„ PDF ì½˜í…ì¸  ì¶”ì¶œ ì‹œì‘: {pdf_path}")
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(pdf_path):
            print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
            return {
                'text': "",
                'ocr_results': [],
                'pdf_info': {'pdf_type': 'unknown', 'has_images': False},
                'processing_method': 'error'
            }
        
        # 1. PDF íƒ€ì… ë¶„ì„
        pdf_info = detect_pdf_type(pdf_path)
        pdf_type = pdf_info['pdf_type']
        
        print(f"ğŸ“Š PDF íƒ€ì… ë¶„ì„: {pdf_type}")
        print(f"   - ì´ í˜ì´ì§€: {pdf_info['total_pages']}")
        print(f"   - í…ìŠ¤íŠ¸ í˜ì´ì§€: {pdf_info['text_pages']}")
        print(f"   - ì´ë¯¸ì§€ í˜ì´ì§€: {pdf_info['image_pages']}")
        
        text_content = ""
        ocr_results = []
        processing_method = "text"
        
        # 2. ë¨¼ì € ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
        text_content = extract_pdf_text(pdf_path)
        text_length = len(text_content.strip())
        
        print(f"ğŸ“ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ: {text_length} characters")
        
        # 3. OCR ì‚¬ìš© ì¡°ê±´ íŒë‹¨
        MIN_TEXT_THRESHOLD = 100  # ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´
        use_ocr, ocr_reason = should_use_ocr(text_length, pdf_type)
        
        print(f"ğŸ” OCR ì‚¬ìš© íŒë‹¨: {'ì‚¬ìš©' if use_ocr else 'ê±´ë„ˆë›°ê¸°'} - {ocr_reason}")
        
        # 4. ì¡°ê±´ë¶€ OCR ì‹¤í–‰
        if use_ocr:
            try:
                print("ğŸ” OCR ì²˜ë¦¬ ì‹œì‘...")
                images = pdf_to_images(pdf_path)
                
                if not images:
                    print("âš ï¸ PDF ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨ - OCR ê±´ë„ˆë›°ê¸°")
                else:
                    ocr_text_added = ""
                    
                    for image_info in images:
                        try:
                            page_num = image_info['page_number']
                            image = image_info['image']
                            
                            # OCR ìˆ˜í–‰ (ì•ˆì „í•œ í˜¸ì¶œ)
                            if ocr_reader:
                                ocr_result = ocr_reader.ocr(np.array(image), cls=True)
                                
                                # OCR ê²°ê³¼ ì²˜ë¦¬
                                if ocr_result and len(ocr_result) > 0:
                                    for line in ocr_result:
                                        if line:  # ë¹ˆ ë¼ì¸ í™•ì¸
                                            for word_info in line:
                                                if len(word_info) >= 2:  # ì˜¬ë°”ë¥¸ êµ¬ì¡° í™•ì¸
                                                    bbox, (text, confidence) = word_info
                                                    
                                                    if confidence > OCR_CONFIDENCE_THRESHOLD and len(text.strip()) > 0:
                                                        # ì¤‘ë³µ ë°©ì§€: ì´ë¯¸ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì— ì—†ëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                                                        if text.strip() not in text_content:
                                                            ocr_results.append({
                                                                'text': text.strip(),
                                                                'confidence': confidence,
                                                                'bbox': {
                                                                    'x': float(bbox[0][0]),
                                                                    'y': float(bbox[0][1]),
                                                                    'width': float(bbox[2][0] - bbox[0][0]),
                                                                    'height': float(bbox[2][1] - bbox[0][1])
                                                                },
                                                                'page_number': page_num
                                                            })
                                                            ocr_text_added += text.strip() + " "
                            else:
                                print(f"âš ï¸ í˜ì´ì§€ {page_num}: OCR readerê°€ Noneì…ë‹ˆë‹¤.")
                                
                        except Exception as page_error:
                            print(f"âŒ í˜ì´ì§€ {image_info.get('page_number', '?')} OCR ì‹¤íŒ¨: {page_error}")
                            continue
                    
                    # OCRë¡œ ì¶”ê°€ëœ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê¸°ì¡´ í…ìŠ¤íŠ¸ì— í•©ì¹˜ê¸°
                    if ocr_text_added.strip():
                        text_content += " " + ocr_text_added.strip()
                        processing_method = "hybrid" if text_length >= MIN_TEXT_THRESHOLD else "image"
                        print(f"ğŸ” OCR ì™„ë£Œ: {len(ocr_results)}ê°œ ìš”ì†Œ ì¶”ê°€, ì´ í…ìŠ¤íŠ¸: {len(text_content)} characters")
                    else:
                        print("ğŸ” OCR ì™„ë£Œ: ì¶”ê°€ëœ í…ìŠ¤íŠ¸ ì—†ìŒ")
                        processing_method = "text"
                    
            except Exception as e:
                print(f"âŒ OCR ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                processing_method = "text"
        else:
            print("âœ… ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤. OCRì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            processing_method = "text"
        
        # 5. ìµœì¢… ê²°ê³¼ í™•ì¸
        final_text_length = len(text_content.strip())
        if final_text_length < MIN_TEXT_THRESHOLD:
            print(f"âš ï¸ ìµœì¢… í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {final_text_length} characters")
        
        # 6. ê²°ê³¼ ë°˜í™˜
        result = {
            'text': text_content.strip(),
            'ocr_results': ocr_results,
            'pdf_info': pdf_info,
            'processing_method': processing_method
        }
        
        print(f"âœ… PDF ì½˜í…ì¸  ì¶”ì¶œ ì™„ë£Œ:")
        print(f"   - ì²˜ë¦¬ ë°©ë²•: {processing_method}")
        print(f"   - ìµœì¢… í…ìŠ¤íŠ¸: {len(text_content)} characters")
        print(f"   - OCR ìš”ì†Œ: {len(ocr_results)}ê°œ")
        print(f"   - OCR ì‚¬ìš©ë¨: {'Yes' if use_ocr and ocr_results else 'No'}")
        
        return result
        
    except Exception as e:
        print(f"âŒ PDF ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        return {
            'text': "",
            'ocr_results': [],
            'pdf_info': {'pdf_type': 'unknown', 'has_images': False},
            'processing_method': 'error'
        }


# ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ (ì™„ì „ ìˆ˜ì •ëœ ë²„ì „)
class AdvancedOntologyLearner:
    def __init__(self):
        # ê¸°ë³¸ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ
        self.classes = {"Project", "Equipment", "ProcessRequirement", "Person", "Document", "Revision", "Note", "Field", "Date"}
        self.object_properties = {"hasEquipment", "hasProcessReq", "reviewedBy", "hasProject", "hasValue", "hasRevision", "hasNote", "hasField"}
        self.datatype_properties = {"jobNo", "itemNo", "value", "noteText", "revisionNumber", "revisionDate", "fieldName", "fieldValue"}
        
        # í•™ìŠµëœ íŒ¨í„´ ì €ì¥ì†Œ
        self.learned_patterns = {}
        self.entity_instances = {}
        self.confidence_scores = {}
        
        # í™•ì¥ëœ ë„ë©”ì¸ íŠ¹í™” í‚¤ì›Œë“œ
        self.domain_keywords = {
            "Project": ["job", "project", "doc", "document", "specification", "drawing", "client", "location"],
            "Equipment": ["pump", "motor", "driver", "centrifugal", "equipment", "vessel", "tank", "compressor", "exchanger"],
            "ProcessRequirement": ["temperature", "pressure", "capacity", "flow", "viscosity", "density", "npsh", "head", "suction", "discharge"],
            "Person": ["checked", "reviewed", "approved", "by", "engineer", "manager"],
            "Revision": ["revision", "rev", "version", "updated", "modified"],
            "Note": ["note", "remark", "comment", "description", "notes"],
            "Field": ["type", "required", "operating", "duty", "liquid", "vapor", "specific", "differential", "material", "method"],
            "Date": ["date", "time", "year", "month", "day"]
        }
        
        # í™•ì¥ëœ ë‹¨ìœ„ ë° ì¸¡ì •ê°’ íŒ¨í„´
        self.unit_patterns = {
            "temperature": ["â„ƒ", "Â°C", "Â°F", "K", "DEG C"],
            "pressure": ["kg/cm2", "kg/cm2g", "kg/cm2A", "bar", "psi", "Pa", "kPa", "MPa"],
            "flow": ["m3/h", "m3/hr", "M3/HR", "l/min", "gpm", "bph"],
            "viscosity": ["cP", "PaÂ·s", "cSt"],
            "length": ["mm", "cm", "m", "in", "ft", "METER"],
            "percentage": ["%", "wt%", "WT%"],
            "density": ["KG/M3", "kg/m3"],
            "time": ["hr", "yr", "hour", "year"]
        }
        
        # í™”í•™ê³µì • ì „ë¬¸ìš©ì–´
        self.process_terms = {
            "fluids": ["OVERFLASH", "GAS OIL", "AH FEED", "AM FEED", "SULFUR"],
            "materials": ["API CLASS", "FLAMMABLE", "TOXIC", "H2S", "CHLORIDE"],
            "operations": ["CONTINUOUS", "INTERMITTENT", "MANUAL", "AUTOMATIC", "INDOOR", "OUTDOOR"],
            "equipment_types": ["HORIZONTAL", "CENTRIFUGAL", "STEAM TRACING", "STEAM JACKET", "INSULATION"]
        }
        
    def learn_from_text(self, text):
        print("ğŸ§  Advanced ontology learning from text...")
        
        # 1. ê¸°ë³¸ íŒ¨í„´ ì¶”ì¶œ (ëŒ€í­ ê°•í™”)
        basic_patterns = self._extract_basic_patterns(text)
        
        # 2. í•„ë“œ-ê°’ ìŒ ì¶”ì¶œ
        field_value_pairs = self._extract_field_value_pairs(text)
        
        # 3. NOTE ì„¹ì…˜ ì¶”ì¶œ  
        notes = self._extract_notes_section(text)
        
        # 4. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì—”í‹°í‹° ì¸ì‹ + ê¸°ë³¸ íŒ¨í„´ì„ ì—”í‹°í‹°ë¡œ ë³€í™˜
        contextual_entities = self._extract_contextual_entities(text, basic_patterns)
        
        # 5. ê´€ê³„ ì¶”ì¶œ
        relations = self._extract_relations(text, contextual_entities)
        
        # 6. ì‹ ë¢°ë„ ê³„ì‚°
        confidence_scores = self._calculate_confidence(basic_patterns, contextual_entities)
        
        # 7. ì˜¨í†¨ë¡œì§€ ë§¤ì¹­
        ontology_matches = self._match_to_ontology(contextual_entities)
        
        # 8. í•™ìŠµ ê²°ê³¼ ì €ì¥
        self._update_learned_knowledge(basic_patterns, contextual_entities, relations)
        
        # í†µí•© ê²°ê³¼
        results = {
            "patterns": basic_patterns,
            "field_value_pairs": field_value_pairs,
            "notes": notes,
            "new_entities": contextual_entities,  # ì´ì œ ê¸°ë³¸ íŒ¨í„´ì´ í¬í•¨ë¨
            "new_relations": relations,
            "matched_entities": ontology_matches,
            "confidence_scores": confidence_scores,
            "domain_insights": self._generate_domain_insights(text)
        }
        
        # ì´ ì—”í‹°í‹° ìˆ˜ ê³„ì‚°
        total_entities = (
            sum(len(v) if isinstance(v, list) else 1 for v in basic_patterns.values()) +
            len(field_value_pairs) +
            len(notes) +
            sum(len(v) for v in contextual_entities.values())
        )
        
        print(f"ğŸ“Š Enhanced learning completed:")
        print(f"   - Basic Patterns: {len(basic_patterns)} types")
        print(f"   - Field-Value Pairs: {len(field_value_pairs)}")
        print(f"   - Notes: {len(notes)}")
        print(f"   - Contextual Entities: {sum(len(v) for v in contextual_entities.values())}")
        print(f"   - Total Entities: {total_entities}")
        print(f"   - Relations: {len(relations)} identified")
        print(f"   - Avg Confidence: {sum(confidence_scores.values())/len(confidence_scores) if confidence_scores else 0:.2f}")
        
        return results
    
    def _extract_basic_patterns(self, text):
        """ê¸°ë³¸ íŒ¨í„´ ì¶”ì¶œ - ì™„ì „ ê°œì„ ëœ ë²„ì „"""
        patterns = {}
        
        try:
            # 1. í”„ë¡œì íŠ¸/ë¬¸ì„œ ë²ˆí˜¸ íŒ¨í„´ (í™•ì¥)
            project_patterns = [
                r'(?:JOB\s*NO\.?\s*:?\s*)([A-Z0-9]{2,}(?:-[A-Z0-9]+)*)',
                r'(?:PROJECT\s*:?\s*)([A-Z0-9]{2,}(?:\s+[A-Z0-9]+)*)',
                r'(?:DOC\.?\s*NO\.?\s*:?\s*)([A-Z0-9]{2,}(?:-[A-Z0-9]+)*)',
                r'(?:ITEM\s*NO\.?\s*:?\s*)([A-Z0-9]{2,}(?:-[A-Z0-9]+)*)',
                r'\b([A-Z]\d+[A-Z]?\d*(?:-[A-Z0-9]+)*)\b'
            ]
            
            project_ids = []
            for pattern in project_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                project_ids.extend(matches)
            
            if project_ids:
                patterns["project_ids"] = list(set(project_ids))
            
            # 2. ëª¨ë“  ë¦¬ë¹„ì „ ë²ˆí˜¸ ì¶”ì¶œ
            revision_patterns = [
                r'(?:REVISION|REV\.?|REV)\s*:?\s*(\d+[A-Z]?)',
                r'(?:revision|rev)\s+(\d+[A-Z]?)',
                r'\bR(\d+[A-Z]?)\b',
                r'\bRev\s*(\d+[A-Z]?)\b',
                r'\b(1[0-9][A-Z]?)\b',  # 10-19
                r'\b([2-9]\d[A-Z]?)\b'   # 20+
            ]
            
            revision_numbers = []
            for pattern in revision_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                revision_numbers.extend(matches)
            
            if revision_numbers:
                patterns["revision_numbers"] = list(set(revision_numbers))
            
            # 3. ëª¨ë“  ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ
            date_patterns = [
                r'(\d{4}-\d{1,2}-\d{1,2})',  # YYYY-MM-DD
                r'(\d{1,2}/\d{1,2}/\d{4})',  # MM/DD/YYYY
                r'(\d{1,2}-\d{1,2}-\d{4})',  # MM-DD-YYYY
                r'(20\d{2}-\d{1,2}-\d{1,2})', # 20XX-XX-XX
            ]
            
            dates = []
            for pattern in date_patterns:
                matches = re.findall(pattern, text)
                dates.extend(matches)
            
            if dates:
                patterns["dates"] = list(set(dates))
            
            # 4. ëª¨ë“  ì‚¬ëŒ ì´ë¦„/ì´ë‹ˆì…œ ì¶”ì¶œ
            person_patterns = [
                r'(?:BY/CHECKED|BY CHECKED|Checked\s+by|Reviewed\s+by|Approved\s+by)\s*:?\s*([A-Z]{2,4}(?:\s*/\s*[A-Z]{2,4})*)',
                r'\b([A-Z]{2,4})\s*/\s*([A-Z]{2,4})\b',
                r'\b([A-Z]{3})\s+/\s+([A-Z]{3})\b',
            ]
            
            person_names = []
            for pattern in person_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    if isinstance(match, tuple):
                        person_names.extend([m for m in match if m and len(m) >= 2])
                    else:
                        names = re.split(r'\s*/\s*', match)
                        person_names.extend([n.strip() for n in names if n.strip() and len(n.strip()) >= 2])
            
            if person_names:
                patterns["person_names"] = list(set(person_names))
            
            # 5. ì¥ë¹„ ì‹ë³„ì íŒ¨í„´
            equipment_patterns = [
                r'([A-Z]-\d+(?:\s*[A-Z](?:/[A-Z])?)?)',
                r'([A-Z]{2,3}-\d+)',
                r'([A-Z]\d{3,}[A-Z]?)',
                r'(P-\d+\s*[A-Z]?/?[A-Z]?)',
            ]
            
            equipment_ids = []
            for pattern in equipment_patterns:
                matches = re.findall(pattern, text)
                equipment_ids.extend(matches)
            
            if equipment_ids:
                patterns["equipment_ids"] = list(set(equipment_ids))
            
            # 6. ìˆ˜ì¹˜ ê°’ íŒ¨í„´
            numerical_patterns = []
            for unit_type, units in self.unit_patterns.items():
                for unit in units:
                    pattern = rf'(\d+\.?\d*)\s*{re.escape(unit)}'
                    matches = re.findall(pattern, text, re.IGNORECASE)
                    for match in matches:
                        numerical_patterns.append((match, unit, unit_type))
            
            if numerical_patterns:
                patterns["numerical_values"] = numerical_patterns[:50]  # ì œí•œ
            
            # 7. í™”í•™ê³µì • ì „ë¬¸ìš©ì–´
            process_entities = []
            for category, terms in self.process_terms.items():
                for term in terms:
                    if term in text:
                        process_entities.append((term, category))
            
            if process_entities:
                patterns["process_terms"] = process_entities
            
            # 8. API ë¶„ë¥˜ ë° ì¬ë£Œ ë“±ê¸‰
            material_patterns = [
                r'(API\s+CLASS\s+[A-Z]-\d+)',
                r'(API\s+[A-Z]-\d+)',
                r'([A-Z]\s*-\s*\d+\s*\([^)]+\))',
            ]
            
            materials = []
            for pattern in material_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                materials.extend(matches)
            
            if materials:
                patterns["materials"] = list(set(materials))
            
            print(f"ğŸ” Extracted basic patterns: {list(patterns.keys())}")
            for pattern_type, values in patterns.items():
                if isinstance(values, list):
                    print(f"   {pattern_type}: {values[:5]}{'...' if len(values) > 5 else ''}")
                else:
                    print(f"   {pattern_type}: {values}")
            
        except Exception as e:
            print(f"âŒ Pattern extraction error: {e}")
            patterns = {}
        
        return patterns
    
    def _extract_field_value_pairs(self, text):
        """í•„ë“œ-ê°’ ìŒ ì¶”ì¶œ"""
        field_value_pairs = []
        
        try:
            # ë²ˆí˜¸ê°€ ìˆëŠ” í•„ë“œ íŒ¨í„´
            numbered_field_pattern = r'(\d{2})\s+([A-Z\s/()]+?)\s+([A-Z0-9\s:@.â„ƒ]+?)(?=\d{2}|\n\n|$)'
            matches = re.findall(numbered_field_pattern, text, re.MULTILINE)
            
            for match in matches:
                field_num, field_name, field_value = match
                field_value_pairs.append({
                    "field_number": field_num.strip(),
                    "field_name": field_name.strip(),
                    "field_value": field_value.strip(),
                    "type": "numbered_field"
                })
            
            print(f"ğŸ·ï¸ Extracted {len(field_value_pairs)} field-value pairs")
            
        except Exception as e:
            print(f"âŒ Field-value extraction error: {e}")
        
        return field_value_pairs
    
    def _extract_notes_section(self, text):
        """NOTES ì„¹ì…˜ ì¶”ì¶œ"""
        notes = []
        
        try:
            notes_pattern = r'NOTES?\s*:?\s*(.*?)(?=\n\n|\nREVISION|\nDATE|$)'
            notes_match = re.search(notes_pattern, text, re.DOTALL | re.IGNORECASE)
            
            if notes_match:
                notes_text = notes_match.group(1).strip()
                if notes_text:
                    notes.append({
                        "note_text": notes_text,
                        "type": "general_note"
                    })
            
            print(f"ğŸ“ Extracted {len(notes)} notes")
            
        except Exception as e:
            print(f"âŒ Notes extraction error: {e}")
        
        return notes
    
    def _extract_contextual_entities(self, text, basic_patterns):
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œ + ê¸°ë³¸ íŒ¨í„´ ë³€í™˜"""
        entities = {}
        
        try:
            # 1. ê¸°ë³¸ íŒ¨í„´ì„ ì—”í‹°í‹°ë¡œ ë³€í™˜ (í•µì‹¬ ìˆ˜ì •!)
            if "project_ids" in basic_patterns:
                entities["Project"] = basic_patterns["project_ids"][:10]
            
            if "equipment_ids" in basic_patterns:
                entities["Equipment"] = basic_patterns["equipment_ids"][:10]
            
            if "person_names" in basic_patterns:
                entities["Person"] = basic_patterns["person_names"][:10]
            
            if "dates" in basic_patterns:
                entities["Date"] = basic_patterns["dates"][:10]
            
            if "revision_numbers" in basic_patterns:
                entities["Revision"] = basic_patterns["revision_numbers"][:10]
            
            # ProcessRequirement ì—”í‹°í‹° ì¶”ê°€
            process_entities = []
            
            # ìˆ˜ì¹˜ ê°’ë“¤ì„ ProcessRequirementë¡œ ë¶„ë¥˜
            if "numerical_values" in basic_patterns:
                for num_tuple in basic_patterns["numerical_values"][:20]:
                    if isinstance(num_tuple, tuple) and len(num_tuple) >= 3:
                        value, unit, unit_type = num_tuple
                        if unit:
                            process_entities.append(f"{value} {unit}")
                        else:
                            process_entities.append(value)
            
            # í”„ë¡œì„¸ìŠ¤ ìš©ì–´ë“¤
            if "process_terms" in basic_patterns:
                for term_tuple in basic_patterns["process_terms"]:
                    if isinstance(term_tuple, tuple):
                        term, category = term_tuple
                        process_entities.append(term)
            
            if process_entities:
                entities["ProcessRequirement"] = process_entities[:15]
            
            # 2. ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì—”í‹°í‹° (ê¸°ì¡´ ë¡œì§)
            sentences = re.split(r'[.!?\n]+', text)
            
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) < 5:
                    continue
                
                for class_name, keywords in self.domain_keywords.items():
                    if any(keyword.lower() in sentence.lower() for keyword in keywords):
                        if class_name not in entities:
                            entities[class_name] = []
                        
                        specific_entities = self._extract_specific_entities(sentence, class_name)
                        entities[class_name].extend(specific_entities)
            
            # 3. ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
            for class_name in entities:
                entities[class_name] = list(set(entities[class_name]))[:15]  # ìµœëŒ€ 15ê°œë¡œ ì œí•œ
                
        except Exception as e:
            print(f"âŒ Contextual entity extraction error: {e}")
            entities = {}
        
        return entities
    
    def _extract_specific_entities(self, sentence, class_name):
        """íŠ¹ì • í´ë˜ìŠ¤ ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = []
        
        try:
            if class_name == "Equipment":
                equipment_types = re.findall(r'\b(CENTRIFUGAL|PUMP|MOTOR|DRIVER|COMPRESSOR|VESSEL|TANK|HORIZONTAL)\b', sentence.upper())
                entities.extend(equipment_types)
                
            elif class_name == "ProcessRequirement":
                process_vars = re.findall(r'\b(temperature|pressure|flow|capacity|viscosity|density|npsh|head|suction|discharge)\b', sentence.lower())
                entities.extend(process_vars)
                
                # ë‹¨ìœ„ê°€ ìˆëŠ” ìˆ˜ì¹˜ê°’
                for unit_type, units in self.unit_patterns.items():
                    for unit in units:
                        pattern = rf'(\d+\.?\d*\s*{re.escape(unit)})'
                        matches = re.findall(pattern, sentence, re.IGNORECASE)
                        entities.extend(matches)
            
            elif class_name == "Person":
                if any(word in sentence.lower() for word in ['by', 'checked', 'reviewed', 'approved']):
                    names = re.findall(r'\b([A-Z]{2,4})\b', sentence)
                    entities.extend(names)
                    
        except Exception as e:
            print(f"âŒ Specific entity extraction error for {class_name}: {e}")
        
        return entities
    
    def _extract_relations(self, text, entities):
        """ê´€ê³„ ì¶”ì¶œ"""
        relations = []
        
        try:
            # ê°„ë‹¨í•œ ê´€ê³„ íŒ¨í„´
            relation_patterns = {
                "hasEquipment": (r'(project|job).*?(equipment|pump|motor)', "Project", "Equipment"),
                "hasProcessReq": (r'(equipment|pump).*?(temperature|pressure|flow)', "Equipment", "ProcessRequirement"),
                "reviewedBy": (r'(revision|document).*?(?:by|checked|reviewed).*?([A-Z]{2,4})', "Revision", "Person"),
            }
            
            for relation_name, (pattern, domain_class, range_class) in relation_patterns.items():
                matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    if isinstance(match, tuple) and len(match) >= 2:
                        relations.append({
                            "source": match[0],
                            "target": match[1],
                            "relation": relation_name
                        })
                        
        except Exception as e:
            print(f"âŒ Relation extraction error: {e}")
        
        return relations
    
    def _calculate_confidence(self, patterns, entities):
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence_scores = {}
        
        try:
            for class_name, entity_list in entities.items():
                for entity in entity_list:
                    confidence = 0.8  # ê¸°ë³¸ ì‹ ë¢°ë„
                    confidence_scores[entity] = confidence
                    
        except Exception as e:
            print(f"âŒ Confidence calculation error: {e}")
        
        return confidence_scores
    
    def _match_to_ontology(self, entities):
        """ì˜¨í†¨ë¡œì§€ ë§¤ì¹­"""
        matches = {}
        return matches
    
    def _generate_domain_insights(self, text):
        """ë„ë©”ì¸ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = {
            "document_type": "technical_specification",
            "industry_domain": "process_engineering",
            "complexity_score": min(len(text) / 1000, 5.0),
            "technical_density": len(re.findall(r'\d+\.?\d*\s*[a-zA-Z/%]+', text)) / max(len(text.split()), 1)
        }
        return insights
    
    def _update_learned_knowledge(self, patterns, entities, relations):
        """í•™ìŠµ ì§€ì‹ ì—…ë°ì´íŠ¸"""
        try:
            print(f"ğŸ“š Knowledge updated: {len(patterns)} pattern types, "
                  f"{sum(len(entity_list) for entity_list in entities.values())} entity instances")
        except Exception as e:
            print(f"âŒ Knowledge update error: {e}")

def classify_ocr_text(text):
    """OCR í…ìŠ¤íŠ¸ë¥¼ ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜"""
    try:
        text = text.strip().upper()
        
        # ìˆ«ì íŒ¨í„´
        if re.match(r'^\d+\.?\d*$', text):
            return "Dimension"
        
        # ì¢Œí‘œ/ì¸¡ì •ê°’ íŒ¨í„´
        if re.match(r'^\d+(\.\d+)?[A-Z]*$', text):
            return "Value"
        
        # ë¼ë²¨ íŒ¨í„´ (LT18, MH2 ë“±)
        if re.match(r'^[A-Z]{1,3}\d+$', text):
            return "Label"
        
        # ë¶€í’ˆëª… íŒ¨í„´
        if any(word in text for word in ['BREAKER', 'PUMP', 'VALVE', 'TANK', 'PIPE']):
            return "Component"
        
        # ê¸°ë³¸ê°’
        return "Text"
    except:
        return "Unknown"

def integrate_ocr_results(learning_results, ocr_results):
    """OCR ê²°ê³¼ë¥¼ í•™ìŠµ ê²°ê³¼ì— í†µí•©"""
    try:
        # OCR ì—”í‹°í‹°ë¥¼ ê¸°ì¡´ ì—”í‹°í‹°ì— ì¶”ê°€
        ocr_entities = {}
        for ocr_item in ocr_results:
            ocr_class = classify_ocr_text(ocr_item['text'])
            if ocr_class not in ocr_entities:
                ocr_entities[ocr_class] = []
            ocr_entities[ocr_class].append(ocr_item['text'])
        
        # ê¸°ì¡´ ì—”í‹°í‹°ì™€ ë³‘í•©
        new_entities = learning_results.get("new_entities", {})
        for class_name, entity_list in ocr_entities.items():
            if class_name in new_entities:
                new_entities[class_name].extend(entity_list)
            else:
                new_entities[class_name] = entity_list
        
        learning_results["new_entities"] = new_entities
        
        # OCR ì‹ ë¢°ë„ë¥¼ ì „ì²´ ì‹ ë¢°ë„ì— ì¶”ê°€
        confidence_scores = learning_results.get("confidence_scores", {})
        for ocr_item in ocr_results:
            confidence_scores[f"ocr_{ocr_item['text']}"] = ocr_item['confidence']
        
        learning_results["confidence_scores"] = confidence_scores
        
        print(f"âœ… OCR í†µí•© ì™„ë£Œ: {sum(len(v) for v in ocr_entities.values())}ê°œ OCR ì—”í‹°í‹° ì¶”ê°€")
        return learning_results
        
    except Exception as e:
        print(f"âŒ OCR í†µí•© ì‹¤íŒ¨: {e}")
        return learning_results

def detect_spatial_relations(ocr_results):
    """OCR ê²°ê³¼ì—ì„œ ê³µê°„ì  ê´€ê³„ ê°ì§€"""
    relations = []
    
    try:
        # ìœ„ì¹˜ ê¸°ë°˜ ê´€ê³„ ê°ì§€
        for i, item1 in enumerate(ocr_results):
            for j, item2 in enumerate(ocr_results[i+1:], i+1):
                if item1.get('page_number') == item2.get('page_number'):
                    bbox1 = item1.get('bbox', {})
                    bbox2 = item2.get('bbox', {})
                    
                    if bbox1 and bbox2:
                        # ìˆ˜ì§ ê´€ê³„ (ìœ„/ì•„ë˜)
                        if abs(bbox1.get('x', 0) - bbox2.get('x', 0)) < 50:  # ë¹„ìŠ·í•œ x ì¢Œí‘œ
                            if bbox1.get('y', 0) < bbox2.get('y', 0):
                                relations.append({
                                    'source_id': i,
                                    'target_id': j,
                                    'source_text': item1['text'],
                                    'target_text': item2['text'],
                                    'relation_type': 'above'
                                })
                        
                        # ìˆ˜í‰ ê´€ê³„ (ì¢Œ/ìš°)
                        if abs(bbox1.get('y', 0) - bbox2.get('y', 0)) < 20:  # ë¹„ìŠ·í•œ y ì¢Œí‘œ
                            if bbox1.get('x', 0) < bbox2.get('x', 0):
                                relations.append({
                                    'source_id': i,
                                    'target_id': j,
                                    'source_text': item1['text'],
                                    'target_text': item2['text'],
                                    'relation_type': 'leftOf'
                                })
        
        print(f"ğŸ”— ê³µê°„ ê´€ê³„ ê°ì§€: {len(relations)}ê°œ")
        return relations
        
    except Exception as e:
        print(f"âŒ ê³µê°„ ê´€ê³„ ê°ì§€ ì‹¤íŒ¨: {e}")
        return []

def process_pdf_to_graph_with_ttl(pdf_content, filename):
    """TTL ì˜¨í†¨ë¡œì§€ ì§€ì‹ì„ í™œìš©í•œ PDF ì²˜ë¦¬"""
    global ontology_learner  # ê¸€ë¡œë²Œ ë³€ìˆ˜ ì ‘ê·¼
    
    text = pdf_content['text'] if isinstance(pdf_content, dict) else pdf_content
    
    print(f"ğŸ§  Processing with TTL-enhanced ontology learning...")
    
    # TTL ì§€ì‹ í™œìš© ì—”í‹°í‹° ì¶”ì¶œ
    entities, learning_results = ontology_learner.extract_entities_and_learn(text, filename)
    
    # â† ì—¬ê¸°ì— ì¶”ê°€
    print(f"âš¡ ê¸°ì¡´ ì¶”ì¶œ: {len(entities)}ê°œ â†’ ëŒ€í­ í™•ì¥ ì¤‘...")

    # ê·¹ê°• ë¶„í• : ëª¨ë“  ê°€ëŠ¥í•œ ë°©ë²•ìœ¼ë¡œ ë¶„í• 
    all_words = []

    # 1. ê¸°ë³¸ ë¶„í• 
    all_words.extend(text.split())

    # 2. ëª¨ë“  ë¹„ì•ŒíŒŒë²³ ë¬¸ìë¡œ ë¶„í• 
    import re
    all_words.extend(re.findall(r'[A-Za-z]+', text))
    all_words.extend(re.findall(r'\d+\.?\d*', text))

    # 3. ì¤„ë³„ ë¶„í• 
    for line in text.split('\n'):
        all_words.extend(line.split())
        # íƒ­ìœ¼ë¡œë„ ë¶„í• 
        all_words.extend(line.split('\t'))

    # 4. íŠ¹ìˆ˜ë¬¸ìë¡œ ë¶„í• 
    for char in '.,()[]{}:;"\'!?/-_|':
        temp_words = []
        for word in all_words:
            temp_words.extend(word.split(char))
        all_words.extend(temp_words)

    # ì¤‘ë³µ ì œê±°
    all_words = list(set([w for w in all_words if w.strip()]))

    print(f"ğŸ’¥ ê·¹ê°• ë¶„í• : {len(all_words)}ê°œ ë‹¨ì–´ ì¶”ì¶œ")


    for word in all_words:
        clean_word = word.strip('.,()[]{}:;"\'!?').strip()
        if len(clean_word) > 0:
            # ì¤‘ë³µ ì²´í¬
            existing = any(e['text'].lower() == clean_word.lower() for e in entities)
            if not existing:
                entities.append({
                    'text': clean_word,
                    'context': "",
                    'ontology_class': 'Other',
                    'confidence': 0.4,
                    'classification_method': 'word-split'
                })

    print(f"ğŸš€ í™•ì¥ ì™„ë£Œ: {len(entities)}ê°œ ì—”í‹°í‹°")

    # ê¸°ì¡´ ë‹¨ì–´ ë¶„í• ì— ì¶”ê°€
    all_words = text.split()  # ê¸°ì¡´

    # ì¶”ê°€ ë¶„í•  ë°©ë²•ë“¤
    import re
    # 1. ëª¨ë“  ë¬¸ìì™€ ìˆ«ì ì¡°í•©
    alpha_num = re.findall(r'[A-Za-z0-9]+', text)
    # 2. íŠ¹ìˆ˜ë¬¸ìë¡œ ë¶„í• 
    special_split = re.split(r'[^\w]', text)
    # 3. ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë“¤
    capital_words = re.findall(r'[A-Z][a-z]*', text)
    # 4. ìˆ«ìì™€ ì†Œìˆ˜ì 
    numbers = re.findall(r'\d+\.?\d*', text)

    # ëª¨ë“  ì¶”ì¶œ ê²°ê³¼ í•©ì¹˜ê¸°
    all_extractions = all_words + alpha_num + special_split + capital_words + numbers

    # í•„í„°ë§ í›„ ì—”í‹°í‹° ì¶”ê°€
    for word in all_extractions:
        clean_word = word.strip('.,()[]{}:;"\'!?').strip()
        if len(clean_word) > 0:
            # ì¤‘ë³µ ì²´í¬ í›„ ì¶”ê°€
            existing = any(e['text'].lower() == clean_word.lower() for e in entities)
            if not existing:
                entities.append({
                    'text': clean_word,
                    'context': "",
                    'ontology_class': 'Other',
                    'confidence': 0.4,
                    'classification_method': 'enhanced-split'
                })

    # TTL ê¸°ë°˜ ë¶„ë¥˜ ì ìš©
    enhanced_entities = []
    ttl_classifications = 0
    
    for entity in entities:
        entity_text = entity['text']
        context = entity.get('context', '')
        
        # TTL ì§€ì‹ í™œìš© ë¶„ë¥˜
        if hasattr(ontology_learner, 'classify_entity_with_ttl'):
            ttl_class, ttl_confidence, ttl_reason = ontology_learner.classify_entity_with_ttl(entity_text, context)
            
            if ttl_confidence > entity.get('confidence', 0):
                entity['ontology_class'] = ttl_class
                entity['confidence'] = ttl_confidence
                entity['classification_method'] = 'TTL-enhanced'
                entity['classification_reason'] = ttl_reason
                ttl_classifications += 1
            else:
                entity['classification_method'] = 'rule-based'
        
        enhanced_entities.append(entity)
    
    print(f"âœ… TTL-enhanced classification: {ttl_classifications}/{len(entities)} entities")
    
    # ê·¸ë˜í”„ ìƒì„±
    G = nx.DiGraph()
    
    # ë…¸ë“œ ì¶”ê°€ (TTL ì •ë³´ í¬í•¨)
    node_features = {}
    predictions = []
    
    for i, entity in enumerate(enhanced_entities):
        node_id = entity['text']
        
        # TTL í´ë˜ìŠ¤ ë§¤í•‘
        ttl_class = entity.get('ontology_class', 'Other')
        confidence = entity.get('confidence', 0.5)
        
        G.add_node(node_id)
        node_features[node_id] = {
            'id': node_id,
            'type': 'entity',
            'ontology_class': ttl_class,
            'confidence': confidence,
            'classification_method': entity.get('classification_method', 'unknown'),
            'ttl_enhanced': entity.get('classification_method') == 'TTL-enhanced'
        }
        
        # ì˜ˆì¸¡ê°’ (TTL í´ë˜ìŠ¤ë¥¼ ìˆ«ìë¡œ ë§¤í•‘)
        class_mapping = {
            'Equipment': 1, 'ProcessRequirement': 1, 'Project': 1,
            'Note': 2, 'Other': 2, 'Revision': 2,
            'Person': 0, 'FeedType': 0, 'ConditionType': 0
        }
        predictions.append(class_mapping.get(ttl_class, 2))
    
    # TTL ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ
    if hasattr(ontology_learner, 'ttl_object_properties'):
        ttl_relations = ontology_learner.ttl_object_properties
        print(f"ğŸ”— Using TTL relations: {ttl_relations}")
        
        # ê°„ë‹¨í•œ ê´€ê³„ ì¶”ì¶œ (í‚¤ì›Œë“œ ê¸°ë°˜)
        relation_patterns = {
            'hasEquipment': ['pump', 'motor', 'equipment'],
            'hasProcessReq': ['temperature', 'pressure', 'capacity'],
            'hasNote': ['note', 'remark'],
            'hasRevision': ['revision', 'rev'],
            'reviewedBy': ['checked', 'reviewed', 'by']
        }
        
        nodes = list(G.nodes())
        for i, node1 in enumerate(nodes):
            for j, node2 in enumerate(nodes[i+1:], i+1):
                # TTL ê´€ê³„ íŒ¨í„´ ë§¤ì¹­
                for relation, keywords in relation_patterns.items():
                    if any(keyword in node1.lower() or keyword in node2.lower() for keyword in keywords):
                        if relation in ttl_relations:  # TTLì— ì •ì˜ëœ ê´€ê³„ë§Œ ì‚¬ìš©
                            G.add_edge(node1, node2, relation=relation, source='TTL-pattern')
                            break
    
    # í•™ìŠµ ê²°ê³¼ì— TTL ì •ë³´ ì¶”ê°€
    learning_results['ttl_enhanced'] = True
    learning_results['ttl_classifications'] = ttl_classifications
    learning_results['ttl_relations_used'] = len([1 for _, _, data in G.edges(data=True) if data.get('source') == 'TTL-pattern'])
    
    if hasattr(ontology_learner, 'get_enhanced_statistics'):
        learning_results['ttl_statistics'] = ontology_learner.get_enhanced_statistics()
    
    print(f"ğŸ•¸ï¸ TTL-enhanced graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
    
    return G, node_features, predictions, learning_results


def process_pdf_to_graph(content, filename):
    """PDF ì²˜ë¦¬ - TTL ì§€ì› í™•ì¸ í›„ ì ì ˆí•œ ë°©ë²• ì„ íƒ"""
    global ontology_learner  # ê¸€ë¡œë²Œ ë³€ìˆ˜ ì ‘ê·¼
    
    if hasattr(ontology_learner, 'classify_entity_with_ttl'):
        return process_pdf_to_graph_with_ttl(content, filename)
    else:
        # ê¸°ì¡´ ë¡œì§ ì‹¤í–‰
        try:
            # ì…ë ¥ ë°ì´í„° íƒ€ì… í™•ì¸ ë° ì²˜ë¦¬
            if isinstance(content, str):
                # ê¸°ì¡´ ë°©ì‹: í…ìŠ¤íŠ¸ë§Œ ì „ë‹¬ëœ ê²½ìš°
                text_content = content
                ocr_results = []
                pdf_info = {'pdf_type': 'text', 'has_images': False}
                source_type = 'text'
            elif isinstance(content, dict):
                # ìƒˆë¡œìš´ ë°©ì‹: OCR ê²°ê³¼ í¬í•¨ëœ ê²½ìš°
                text_content = content.get('text', '')
                ocr_results = content.get('ocr_results', [])
                pdf_info = content.get('pdf_info', {})
                source_type = pdf_info.get('pdf_type', 'text')
            else:
                raise ValueError("Invalid content type")
            
            # ì˜¨í†¨ë¡œì§€ í•™ìŠµ ë° ê·¸ë˜í”„ ìƒì„± (ê¸€ë¡œë²Œ ontology_learner ì‚¬ìš©)
            learning_results = ontology_learner.learn_from_text(text_content)
            
            # OCR ê²°ê³¼ë¥¼ ì—”í‹°í‹°ë¡œ ì¶”ê°€ ì²˜ë¦¬
            if ocr_results:
                print(f"ğŸ” OCR ë°ì´í„°ë¥¼ ê·¸ë˜í”„ì— í†µí•© ì¤‘...")
                learning_results = integrate_ocr_results(learning_results, ocr_results)
            
            # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
            G = nx.Graph()
            node_features = {}
            predictions = []
            entity_counter = 0  # í•µì‹¬ ìˆ˜ì •: entity_counter ì´ˆê¸°í™”
            
            # ì—”í‹°í‹° ìˆ˜ì§‘ ë° í†µí•©
            entities = {}

            # ê¸°ë³¸ íŒ¨í„´ì—ì„œ ì—”í‹°í‹° ì¶”ê°€
            patterns = learning_results.get("patterns", {})
            for pattern_type, pattern_list in patterns.items():
                if pattern_type == "project_ids":
                    entities["Project"] = pattern_list[:10]  # ì œí•œ ì¶”ê°€
                elif pattern_type == "equipment_ids": 
                    entities["Equipment"] = pattern_list[:10]
                elif pattern_type == "person_names":
                    entities["Person"] = pattern_list[:10]
                elif pattern_type == "dates":
                    entities["Date"] = pattern_list[:10]
                elif pattern_type == "revision_numbers":
                    entities["Revision"] = pattern_list[:10]
                elif pattern_type == "numerical_values":
                    # ìˆ˜ì¹˜ê°’ë“¤ì„ ProcessRequirementë¡œ ë¶„ë¥˜
                    numerical_entities = []
                    for num_tuple in pattern_list[:20]:
                        if isinstance(num_tuple, tuple) and len(num_tuple) >= 3:
                            value, unit, unit_type = num_tuple
                            if unit:
                                numerical_entities.append(f"{value} {unit}")
                            else:
                                numerical_entities.append(str(value))
                    if numerical_entities:
                        entities["ProcessRequirement"] = numerical_entities
                elif pattern_type == "process_terms":
                    # í”„ë¡œì„¸ìŠ¤ ìš©ì–´ë“¤
                    process_entities = []
                    for term_tuple in pattern_list:
                        if isinstance(term_tuple, tuple):
                            term, category = term_tuple
                            process_entities.append(term)
                    if process_entities:
                        if "ProcessRequirement" in entities:
                            entities["ProcessRequirement"].extend(process_entities[:10])
                        else:
                            entities["ProcessRequirement"] = process_entities[:10]

            # í•„ë“œ-ê°’ ìŒ ì¶”ê°€
            field_value_pairs = learning_results.get("field_value_pairs", [])
            field_entities = [pair.get("field_name", "") for pair in field_value_pairs if pair.get("field_name", "").strip()]
            if field_entities:
                entities["Field"] = field_entities[:10]

            # ì»¨í…ìŠ¤íŠ¸ ì—”í‹°í‹°ë„ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
            contextual_entities = learning_results.get("new_entities", {})
            for class_name, entity_list in contextual_entities.items():
                if class_name in entities:
                    # ê¸°ì¡´ ì—”í‹°í‹°ì™€ ì¤‘ë³µ ì œê±°
                    existing = set(entities[class_name])
                    new_entities = [e for e in entity_list if e not in existing]
                    entities[class_name].extend(new_entities[:5])  # ì¶”ê°€ ì œí•œ
                else:
                    entities[class_name] = entity_list[:10]
            
            # ì—”í‹°í‹°ë¥¼ ë…¸ë“œë¡œ ë³€í™˜
            print(f"ğŸ”— ì—”í‹°í‹° â†’ ë…¸ë“œ ë³€í™˜: {sum(len(v) for v in entities.values())}ê°œ ì—”í‹°í‹°")
            
            for class_name, entity_list in entities.items():
                for entity in entity_list:
                    if not entity or not str(entity).strip():  # ë¹ˆ ì—”í‹°í‹° ê±´ë„ˆë›°ê¸°
                        continue
                        
                    # ì•ˆì „í•œ ë…¸ë“œ ID ìƒì„±
                    safe_entity = str(entity).replace(' ', '_').replace('/', '_').replace('\\', '_')[:50]
                    node_id = f"{class_name}_{safe_entity}_{entity_counter}"
                    entity_counter += 1
                    
                    G.add_node(node_id)
                    node_features[node_id] = {
                        "type": "entity",
                        "ontology_class": class_name,
                        "label": str(entity),
                        "confidence": 0.8,
                        "source_type": "text"
                    }
                    
                    # ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡
                    if class_name == "Document":
                        predictions.append(0)
                    elif class_name in ["Entity", "Person", "Location", "Equipment"]:
                        predictions.append(1)
                    else:
                        predictions.append(2)
            
            # OCR ì—”í‹°í‹° ì¶”ê°€
            for ocr_item in ocr_results:
                if not ocr_item.get('text', '').strip():  # ë¹ˆ í…ìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°
                    continue
                    
                safe_text = ocr_item['text'].replace(' ', '_').replace('/', '_').replace('\\', '_')[:50]
                node_id = f"OCR_{safe_text}_{entity_counter}"
                entity_counter += 1
                
                # OCR í…ìŠ¤íŠ¸ ë¶„ë¥˜
                ocr_class = classify_ocr_text(ocr_item['text'])
                
                G.add_node(node_id)
                node_features[node_id] = {
                    "type": "entity",
                    "ontology_class": ocr_class,
                    "label": ocr_item['text'],
                    "confidence": ocr_item.get('confidence', 0.5),
                    "source_type": "ocr",
                    "bbox": ocr_item.get('bbox', {}),
                    "page_number": ocr_item.get('page_number', 1)
                }
                
                # OCR ê¸°ë°˜ ì˜ˆì¸¡
                if ocr_class in ["Dimension", "Value"]:
                    predictions.append(0)
                elif ocr_class in ["Label", "Component"]:
                    predictions.append(1)
                else:
                    predictions.append(2)
            
            # ê´€ê³„ ì¶”ê°€
            relations = learning_results.get("new_relations", [])
            edge_count = 0
            for relation in relations:
                source = relation.get("source", "")
                target = relation.get("target", "")
                relation_type = relation.get("relation", "unknown")
                
                if not source or not target:  # ë¹ˆ ê´€ê³„ ê±´ë„ˆë›°ê¸°
                    continue
                
                # ë…¸ë“œ ë§¤ì¹­
                source_nodes = [n for n in G.nodes() if source.lower() in node_features[n]["label"].lower()]
                target_nodes = [n for n in G.nodes() if target.lower() in node_features[n]["label"].lower()]
                
                for s_node in source_nodes[:1]:  # ì²« ë²ˆì§¸ ë§¤ì¹˜ë§Œ ì‚¬ìš©
                    for t_node in target_nodes[:1]:
                        if s_node != t_node:
                            G.add_edge(s_node, t_node, relation=relation_type)
                            edge_count += 1
            
            # OCR ê¸°ë°˜ ê³µê°„ ê´€ê³„ ì¶”ê°€
            if ocr_results:
                spatial_relations = detect_spatial_relations(ocr_results)
                for relation in spatial_relations:
                    source_text = relation.get('source_text', '')
                    target_text = relation.get('target_text', '')
                    relation_type = relation.get('relation_type', 'spatial')
                    
                    if not source_text or not target_text:
                        continue
                    
                    # í•´ë‹¹í•˜ëŠ” ë…¸ë“œ ì°¾ê¸°
                    source_nodes = [n for n in G.nodes() if source_text in node_features[n]["label"]]
                    target_nodes = [n for n in G.nodes() if target_text in node_features[n]["label"]]
                    
                    for s_node in source_nodes[:1]:
                        for t_node in target_nodes[:1]:
                            if s_node != t_node:
                                G.add_edge(s_node, t_node, relation=relation_type)
                                edge_count += 1
            
            print(f"âœ… ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ: {G.number_of_nodes()}ê°œ ë…¸ë“œ, {G.number_of_edges()}ê°œ ì—£ì§€")
            print(f"ğŸ“Š ì†ŒìŠ¤ë³„ ë…¸ë“œ: í…ìŠ¤íŠ¸ {len([n for n in G.nodes() if node_features[n]['source_type'] == 'text'])}ê°œ, OCR {len([n for n in G.nodes() if node_features[n]['source_type'] == 'ocr'])}ê°œ")
            
            return G, node_features, predictions, learning_results
            
        except Exception as e:
            print(f"âŒ ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
            # ë¹ˆ ê·¸ë˜í”„ ë°˜í™˜
            G = nx.Graph()
            empty_node_features = {}
            empty_predictions = []
            empty_learning_results = {
                "patterns": {},
                "confidence_scores": {},
                "new_entities": {},
                "new_relations": [],
                "field_value_pairs": [],
                "notes": [],
                "domain_insights": {}
            }
            return G, empty_node_features, empty_predictions, empty_learning_results

        
def classify_ocr_text(text):
    """OCR í…ìŠ¤íŠ¸ë¥¼ ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜"""
    text = text.strip().upper()
    
    # ìˆ«ì íŒ¨í„´
    if re.match(r'^\d+\.?\d*$', text):
        return "Dimension"
    
    # ì¢Œí‘œ/ì¸¡ì •ê°’ íŒ¨í„´
    if re.match(r'^\d+(\.\d+)?[A-Z]*$', text):
        return "Value"
    
    # ë¼ë²¨ íŒ¨í„´ (LT18, MH2 ë“±)
    if re.match(r'^[A-Z]{1,3}\d+$', text):
        return "Label"
    
    # ë¶€í’ˆëª… íŒ¨í„´
    if any(word in text for word in ['BREAKER', 'PUMP', 'VALVE', 'TANK', 'PIPE']):
        return "Component"
    
    # ê¸°ë³¸ê°’
    return "Text"

def integrate_ocr_results(learning_results, ocr_results):
    """OCR ê²°ê³¼ë¥¼ í•™ìŠµ ê²°ê³¼ì— í†µí•©"""
    try:
        # OCR ì—”í‹°í‹°ë¥¼ ê¸°ì¡´ ì—”í‹°í‹°ì— ì¶”ê°€
        ocr_entities = {}
        for ocr_item in ocr_results:
            ocr_class = classify_ocr_text(ocr_item['text'])
            if ocr_class not in ocr_entities:
                ocr_entities[ocr_class] = []
            ocr_entities[ocr_class].append(ocr_item['text'])
        
        # ê¸°ì¡´ ì—”í‹°í‹°ì™€ ë³‘í•©
        new_entities = learning_results.get("new_entities", {})
        for class_name, entity_list in ocr_entities.items():
            if class_name in new_entities:
                new_entities[class_name].extend(entity_list)
            else:
                new_entities[class_name] = entity_list
        
        learning_results["new_entities"] = new_entities
        
        # OCR ì‹ ë¢°ë„ë¥¼ ì „ì²´ ì‹ ë¢°ë„ì— ì¶”ê°€
        confidence_scores = learning_results.get("confidence_scores", {})
        for ocr_item in ocr_results:
            confidence_scores[f"ocr_{ocr_item['text']}"] = ocr_item['confidence']
        
        learning_results["confidence_scores"] = confidence_scores
        
        print(f"âœ… OCR í†µí•© ì™„ë£Œ: {sum(len(v) for v in ocr_entities.values())}ê°œ OCR ì—”í‹°í‹° ì¶”ê°€")
        return learning_results
        
    except Exception as e:
        print(f"âŒ OCR í†µí•© ì‹¤íŒ¨: {e}")
        return learning_results

def detect_spatial_relations(ocr_results):
    """OCR ê²°ê³¼ì—ì„œ ê³µê°„ì  ê´€ê³„ ê°ì§€"""
    relations = []
    
    try:
        # ìœ„ì¹˜ ê¸°ë°˜ ê´€ê³„ ê°ì§€ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
        for i, item1 in enumerate(ocr_results):
            for j, item2 in enumerate(ocr_results[i+1:], i+1):
                if item1.get('page_number') == item2.get('page_number'):
                    bbox1 = item1.get('bbox', {})
                    bbox2 = item2.get('bbox', {})
                    
                    if bbox1 and bbox2:
                        # ìˆ˜ì§ ê´€ê³„ (ìœ„/ì•„ë˜)
                        if abs(bbox1.get('x', 0) - bbox2.get('x', 0)) < 50:  # ë¹„ìŠ·í•œ x ì¢Œí‘œ
                            if bbox1.get('y', 0) < bbox2.get('y', 0):
                                relations.append({
                                    'source_id': i,
                                    'target_id': j,
                                    'source_text': item1['text'],
                                    'target_text': item2['text'],
                                    'relation_type': 'above'
                                })
                        
                        # ìˆ˜í‰ ê´€ê³„ (ì¢Œ/ìš°)
                        if abs(bbox1.get('y', 0) - bbox2.get('y', 0)) < 20:  # ë¹„ìŠ·í•œ y ì¢Œí‘œ
                            if bbox1.get('x', 0) < bbox2.get('x', 0):
                                relations.append({
                                    'source_id': i,
                                    'target_id': j,
                                    'source_text': item1['text'],
                                    'target_text': item2['text'],
                                    'relation_type': 'leftOf'
                                })
        
        print(f"ğŸ”— ê³µê°„ ê´€ê³„ ê°ì§€: {len(relations)}ê°œ")
        return relations
        
    except Exception as e:
        print(f"âŒ ê³µê°„ ê´€ê³„ ê°ì§€ ì‹¤íŒ¨: {e}")
        return []
    
    
# ìƒ˜í”Œ ë°ì´í„° ìƒì„± í•¨ìˆ˜
def create_sample_graph():
    G = nx.DiGraph()
    
    nodes = [
        ("Project_7T04", {"node_type": "entity", "ontology_class": "Project"}),
        ("Equipment_P2105", {"node_type": "entity", "ontology_class": "Equipment"}), 
        ("ProcessReq_Temp", {"node_type": "entity", "ontology_class": "ProcessRequirement"}),
        ("7T04", {"node_type": "literal", "ontology_class": "string"}),
        ("P-2105 A/B", {"node_type": "literal", "ontology_class": "string"}),
        ("384 â„ƒ", {"node_type": "literal", "ontology_class": "decimal"})
    ]
    
    for node_id, attrs in nodes:
        G.add_node(node_id, **attrs)
    
    edges = [
        ("Project_7T04", "7T04", "hasValue"),
        ("Equipment_P2105", "P-2105 A/B", "hasValue"),
        ("ProcessReq_Temp", "384 â„ƒ", "hasValue"),
        ("Project_7T04", "Equipment_P2105", "hasEquipment"),
        ("Equipment_P2105", "ProcessReq_Temp", "hasProcessReq")
    ]
    
    for src, dst, rel in edges:
        G.add_edge(src, dst, relation=rel)
    
    node_features = {}
    for node in G.nodes():
        node_data = G.nodes[node]
        node_features[node] = {
            "type": node_data.get("node_type", "entity"),
            "ontology_class": node_data.get("ontology_class", "Unknown"),
            "confidence": 0.8 if node_data.get("node_type") == "entity" else 0.9
        }
    
    predictions = []
    for node in G.nodes():
        node_type = node_features[node]["type"]
        if node_type == "entity":
            predictions.append(1)
        elif node_type == "literal":
            predictions.append(2)
        else:
            predictions.append(0)
    
    return G, node_features, predictions

# ontology.ttl í•™ìŠµ ê¸°ëŠ¥ ì¶”ê°€
from rdflib import Graph, Namespace, RDF, RDFS, OWL
import os

class EnhancedOntologyLearner:
    def __init__(self, ttl_file_path="ontology.ttl"):
        """ì˜¨í†¨ë¡œì§€ TTL íŒŒì¼ì„ í•™ìŠµí•˜ëŠ” ê³ ê¸‰ ì˜¨í†¨ë¡œì§€ í•™ìŠµê¸°"""
        self.ttl_file_path = ttl_file_path
        self.ontology_graph = Graph()
        self.ex_namespace = None
        
        # TTLì—ì„œ í•™ìŠµí•œ êµ¬ì¡°
        self.ttl_classes = []
        self.ttl_object_properties = []
        self.ttl_datatype_properties = []
        self.ttl_instances = {}
        self.ttl_class_hierarchy = {}
        
        # ê¸°ì¡´ í•˜ë“œì½”ë”©ëœ êµ¬ì¡° (fallbackìš©)
        self.default_ontology_classes = [
            "Equipment", "ProcessRequirement", "Field", "Note", 
            "Project", "Person", "Literal", "Other", "Revision", 
            "FeedType", "ConditionType"
        ]
        
        self.default_object_properties = [
            "hasEquipment", "hasProcessReq", "hasOther", "hasRevision",
            "hasNote", "hasFeedType", "hasCondition", "reviewedBy"
        ]
        
        self.default_datatype_properties = [
            "jobNo", "projectName", "docNo", "itemNo", "client", "service",
            "pumpType", "driverType", "value", "conditionType", "noteText"
        ]
        
        # TTL íŒŒì¼ ë¡œë“œ ë° í•™ìŠµ
        self.load_ontology()
        
        # ìµœì¢… ì˜¨í†¨ë¡œì§€ êµ¬ì¡° (TTL + ê¸°ë³¸)
        self.ontology_classes = self.merge_ontology_structures()
        self.object_properties = self.merge_object_properties()
        self.datatype_properties = self.merge_datatype_properties()
        
        print(f"âœ… Enhanced ontology loaded: {len(self.ontology_classes)} classes, {len(self.object_properties)} object properties")
    
    def load_ontology(self):
        """ontology.ttl íŒŒì¼ì„ ë¡œë“œí•˜ê³  êµ¬ì¡°ë¥¼ ì¶”ì¶œ"""
        if not os.path.exists(self.ttl_file_path):
            print(f"âš ï¸ TTL file not found: {self.ttl_file_path}. Using default ontology.")
            return
        
        try:
            # TTL íŒŒì¼ íŒŒì‹±
            self.ontology_graph.parse(self.ttl_file_path, format="turtle")
            print(f"âœ… TTL file loaded: {len(self.ontology_graph)} triples")
            
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì •
            self.ex_namespace = Namespace("http://example.org/plant#")
            
            # ì˜¨í†¨ë¡œì§€ êµ¬ì¡° ì¶”ì¶œ
            self._extract_classes()
            self._extract_object_properties()
            self._extract_datatype_properties()
            self._extract_instances()
            self._extract_class_hierarchy()
            
            print(f"ğŸ“š TTL Analysis Complete:")
            print(f"   ğŸ·ï¸ Classes: {len(self.ttl_classes)}")
            print(f"   ğŸ”— Object Properties: {len(self.ttl_object_properties)}")
            print(f"   ğŸ“ Datatype Properties: {len(self.ttl_datatype_properties)}")
            print(f"   ğŸ“¦ Instances: {sum(len(v) for v in self.ttl_instances.values())}")
            
        except Exception as e:
            print(f"âŒ Error loading TTL file: {e}")
            print("ğŸ”„ Falling back to default ontology")
    
    def _extract_classes(self):
        """TTLì—ì„œ í´ë˜ìŠ¤ë“¤ì„ ì¶”ì¶œ"""
        classes = set()
        
        # owl:Classë¡œ ì •ì˜ëœ í´ë˜ìŠ¤ë“¤
        for subj, pred, obj in self.ontology_graph.triples((None, RDF.type, OWL.Class)):
            class_name = self._get_local_name(subj)
            if class_name:
                classes.add(class_name)
        
        self.ttl_classes = sorted(list(classes))
        print(f"   ğŸ“‹ Classes found: {self.ttl_classes}")
    
    def _extract_object_properties(self):
        """TTLì—ì„œ ê°ì²´ ì†ì„±ë“¤ì„ ì¶”ì¶œ"""
        properties = set()
        
        # owl:ObjectPropertyë¡œ ì •ì˜ëœ ì†ì„±ë“¤
        for subj, pred, obj in self.ontology_graph.triples((None, RDF.type, OWL.ObjectProperty)):
            prop_name = self._get_local_name(subj)
            if prop_name:
                properties.add(prop_name)
        
        self.ttl_object_properties = sorted(list(properties))
        print(f"   ğŸ”— Object Properties: {self.ttl_object_properties}")
    
    def _extract_datatype_properties(self):
        """TTLì—ì„œ ë°ì´í„°íƒ€ì… ì†ì„±ë“¤ì„ ì¶”ì¶œ"""
        properties = set()
        
        # owl:DatatypePropertyë¡œ ì •ì˜ëœ ì†ì„±ë“¤
        for subj, pred, obj in self.ontology_graph.triples((None, RDF.type, OWL.DatatypeProperty)):
            prop_name = self._get_local_name(subj)
            if prop_name:
                properties.add(prop_name)
        
        self.ttl_datatype_properties = sorted(list(properties))
        print(f"   ğŸ“ Datatype Properties: {self.ttl_datatype_properties}")
    
    def _extract_instances(self):
        """TTLì—ì„œ ì¸ìŠ¤í„´ìŠ¤ë“¤ì„ ì¶”ì¶œ"""
        instances = {}
        
        for class_name in self.ttl_classes:
            class_uri = self.ex_namespace[class_name]
            class_instances = []
            
            # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë“¤ ì°¾ê¸°
            for subj, pred, obj in self.ontology_graph.triples((None, RDF.type, class_uri)):
                instance_name = self._get_local_name(subj)
                if instance_name:
                    class_instances.append(instance_name)
            
            if class_instances:
                instances[class_name] = sorted(class_instances)
        
        self.ttl_instances = instances
        print(f"   ğŸ“¦ Instances by class: {[(k, len(v)) for k, v in instances.items()]}")
    
    def _extract_class_hierarchy(self):
        """TTLì—ì„œ í´ë˜ìŠ¤ ê³„ì¸µêµ¬ì¡°ë¥¼ ì¶”ì¶œ"""
        hierarchy = {}
        
        # rdfs:domainê³¼ rdfs:range ê´€ê³„ ë¶„ì„
        for subj, pred, obj in self.ontology_graph.triples((None, RDFS.domain, None)):
            prop_name = self._get_local_name(subj)
            domain_class = self._get_local_name(obj)
            
            if prop_name and domain_class:
                if domain_class not in hierarchy:
                    hierarchy[domain_class] = {"properties": [], "ranges": []}
                hierarchy[domain_class]["properties"].append(prop_name)
        
        for subj, pred, obj in self.ontology_graph.triples((None, RDFS.range, None)):
            prop_name = self._get_local_name(subj)
            range_class = self._get_local_name(obj)
            
            if prop_name and range_class:
                # domainì„ ì°¾ì•„ì„œ ì—°ê²°
                for domain_subj, domain_pred, domain_obj in self.ontology_graph.triples((subj, RDFS.domain, None)):
                    domain_class = self._get_local_name(domain_obj)
                    if domain_class and domain_class in hierarchy:
                        hierarchy[domain_class]["ranges"].append(range_class)
        
        self.ttl_class_hierarchy = hierarchy
        print(f"   ğŸŒ³ Class hierarchy: {len(hierarchy)} classes with relationships")
    
    def _get_local_name(self, uri):
        """URIì—ì„œ ë¡œì»¬ ì´ë¦„ ì¶”ì¶œ"""
        try:
            if hasattr(uri, 'split'):
                return str(uri).split('#')[-1].split('/')[-1]
            return str(uri).split('#')[-1].split('/')[-1]
        except:
            return None
    
    def merge_ontology_structures(self):
        """TTLê³¼ ê¸°ë³¸ ì˜¨í†¨ë¡œì§€ êµ¬ì¡°ë¥¼ ë³‘í•©"""
        merged_classes = set(self.default_ontology_classes)
        merged_classes.update(self.ttl_classes)
        return sorted(list(merged_classes))
    
    def merge_object_properties(self):
        """TTLê³¼ ê¸°ë³¸ ê°ì²´ ì†ì„±ì„ ë³‘í•©"""
        merged_props = set(self.default_object_properties)
        merged_props.update(self.ttl_object_properties)
        return sorted(list(merged_props))
    
    def merge_datatype_properties(self):
        """TTLê³¼ ê¸°ë³¸ ë°ì´í„°íƒ€ì… ì†ì„±ì„ ë³‘í•©"""
        merged_props = set(self.default_datatype_properties)
        merged_props.update(self.ttl_datatype_properties)
        return sorted(list(merged_props))
    
    def get_ttl_knowledge(self):
        """TTLì—ì„œ í•™ìŠµí•œ ì§€ì‹ ë°˜í™˜"""
        return {
            "classes": self.ttl_classes,
            "object_properties": self.ttl_object_properties,
            "datatype_properties": self.ttl_datatype_properties,
            "instances": self.ttl_instances,
            "class_hierarchy": self.ttl_class_hierarchy,
            "total_triples": len(self.ontology_graph)
        }
    
    def classify_entity_with_ttl(self, entity_text, context=""):
        """TTL ì§€ì‹ì„ í™œìš©í•œ ì—”í‹°í‹° ë¶„ë¥˜"""
        entity_lower = entity_text.lower()
        
        # 1. TTL ì¸ìŠ¤í„´ìŠ¤ì™€ ì§ì ‘ ë§¤ì¹­
        for class_name, instances in self.ttl_instances.items():
            for instance in instances:
                if instance.lower() in entity_lower or entity_lower in instance.lower():
                    return class_name, 0.95, f"TTL instance match: {instance}"
        
        # 2. TTL ë°ì´í„°íƒ€ì… ì†ì„±ê³¼ ë§¤ì¹­
        for prop in self.ttl_datatype_properties:
            if prop.lower() in entity_lower:
                # ì†ì„±ì˜ ë„ë©”ì¸ í´ë˜ìŠ¤ ì°¾ê¸°
                for class_name, hierarchy in self.ttl_class_hierarchy.items():
                    if prop in hierarchy.get("properties", []):
                        return class_name, 0.85, f"TTL property domain: {prop}"
        
        # 3. TTL í´ë˜ìŠ¤ ì´ë¦„ê³¼ ì§ì ‘ ë§¤ì¹­
        for class_name in self.ttl_classes:
            if class_name.lower() in entity_lower or entity_lower in class_name.lower():
                return class_name, 0.8, f"TTL class match: {class_name}"
        
        # 4. ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ (fallback)
        return self._classify_with_rules(entity_text, context)
    
    def _classify_with_rules(self, entity_text, context=""):
        """ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ (fallback)"""
        entity_lower = entity_text.lower()
        
        # ê¸°ì¡´ ë¶„ë¥˜ ë¡œì§...
        if any(keyword in entity_lower for keyword in ["pump", "motor", "compressor", "vessel"]):
            return "Equipment", 0.7, "Rule-based: equipment keyword"
        elif any(keyword in entity_lower for keyword in ["temperature", "pressure", "capacity", "flow"]):
            return "ProcessRequirement", 0.7, "Rule-based: process keyword"
        elif entity_lower.startswith("note"):
            return "Note", 0.8, "Rule-based: note prefix"
        elif any(keyword in entity_lower for keyword in ["project", "job", "doc"]):
            return "Project", 0.7, "Rule-based: project keyword"
        else:
            return "Other", 0.3, "Rule-based: default"
    
    def get_enhanced_statistics(self):
        """TTL ì§€ì‹ì„ í¬í•¨í•œ í™•ì¥ëœ í†µê³„"""
        ttl_knowledge = self.get_ttl_knowledge()
        
        return {
            "total_classes": len(self.ontology_classes),
            "ttl_classes": len(self.ttl_classes),
            "default_classes": len(self.default_ontology_classes),
            "total_properties": len(self.object_properties) + len(self.datatype_properties),
            "ttl_triples": ttl_knowledge["total_triples"],
            "ttl_instances": sum(len(v) for v in self.ttl_instances.values()),
            "class_hierarchy_depth": len(self.ttl_class_hierarchy),
            "ontology_source": "TTL + Default" if self.ttl_classes else "Default only"
        }

    def extract_entities_and_learn(self, text, filename):
        """TTL ì§€ì‹ì„ í™œìš©í•œ ì—”í‹°í‹° ì¶”ì¶œ ë° í•™ìŠµ"""
        print(f"ğŸ§  Processing with TTL-enhanced ontology learning...")
        
        # ê¸°ë³¸ ì—”í‹°í‹° ì¶”ì¶œ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
        entities = []
        confidence_scores = {}
        patterns = {}
        new_entities = {"Equipment": [], "ProcessRequirement": [], "Note": []}
        new_relations = []
        
        # ê°„ë‹¨í•œ ì—”í‹°í‹° ì¶”ì¶œ
        lines = text.split('\n')
        for i, line in enumerate(lines):
            words = line.split()
            for j, word in enumerate(words):
                if len(word) > 3:  # ìµœì†Œ ê¸¸ì´ ì¡°ê±´
                    # TTL ì§€ì‹ í™œìš© ë¶„ë¥˜
                    ttl_class, ttl_confidence, ttl_reason = self.classify_entity_with_ttl(word, line)
                    
                    entity = {
                        'text': word,
                        'context': line,
                        'ontology_class': ttl_class,
                        'confidence': ttl_confidence,
                        'classification_method': 'TTL-enhanced' if ttl_confidence > 0.3 else 'rule-based',
                        'classification_reason': ttl_reason
                    }
                    entities.append(entity)
                    confidence_scores[word] = ttl_confidence
        
        # í•™ìŠµ ê²°ê³¼ êµ¬ì„±
        learning_results = {
            "confidence_scores": confidence_scores,
            "patterns": patterns,
            "new_entities": new_entities,
            "new_relations": new_relations,
            "domain_insights": {
                "industry_domain": "Chemical Process",
                "complexity_score": 3.5,
                "technical_density": 0.8,
                "document_type": "Technical Specification"
            },
            "ttl_enhanced": True,
            "ttl_statistics": self.get_enhanced_statistics()
        }
        
        print(f"âœ… TTL-enhanced entity extraction complete: {len(entities)} entities")
        return entities, learning_results

class RGCNManager:
    def __init__(self):
        """TTL í†µí•© R-GCN ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        self.initialized = True
        self.device = "cpu"
        self.num_relations = 0
        print("ğŸ¤– TTL-enhanced RGCNManager initialized")
    
    def get_status(self):
        """R-GCN ìƒíƒœ ë°˜í™˜"""
        return {
            "initialized": self.initialized,
            "device": self.device,
            "num_relations": self.num_relations
        }

    def predict(self, graph_data, ttl_ontology=None):
        """TTL ì˜¨í†¨ë¡œì§€ë¥¼ í™œìš©í•œ í–¥ìƒëœ R-GCN ì˜ˆì¸¡"""
        try:
            nodes = graph_data.get("nodes", [])
            edges = graph_data.get("edges", [])
            
            if not nodes:
                return None
            
            print(f"ğŸ”® R-GCN TTL-enhanced prediction starting...")
            print(f"ğŸ“Š Input: {len(nodes)} nodes, {len(edges)} edges")
            
            # TTL ì˜¨í†¨ë¡œì§€ ì •ë³´ í†µí•©
            if ttl_ontology:
                print(f"ğŸ§  TTL integration: {len(ttl_ontology.get('classes', []))} classes, {len(ttl_ontology.get('instances', {}))} instance groups")
            
            # ë…¸ë“œ íŠ¹ì„± ë²¡í„° ìƒì„± (TTL ì •ë³´ í¬í•¨)
            node_features = self._create_ttl_enhanced_features(nodes, ttl_ontology)
            
            # ì—£ì§€ ì •ë³´ ì²˜ë¦¬ (TTL ê´€ê³„ í¬í•¨)
            edge_features = self._process_ttl_relations(edges, ttl_ontology)
            
            # R-GCN í´ëŸ¬ìŠ¤í„°ë§ (TTL ì œì•½ ì¡°ê±´ ì ìš©)
            clusters = self._rgcn_clustering_with_ttl(node_features, edge_features, ttl_ontology)
            
            # TTL ì§€ì‹ ê¸°ë°˜ í›„ì²˜ë¦¬
            refined_predictions = self._refine_with_ttl_knowledge(clusters, nodes, ttl_ontology)
            
            print(f"âœ… TTL-enhanced R-GCN prediction complete!")
            
            return {
                "predictions": refined_predictions,
                "confidence_scores": [pred.get("confidence", 0.5) for pred in refined_predictions],
                "class_distribution": self._calculate_ttl_distribution(refined_predictions),
                "average_confidence": sum(pred.get("confidence", 0.5) for pred in refined_predictions) / len(refined_predictions),
                "ttl_enhanced": True,
                "ttl_classes_used": len(ttl_ontology.get('classes', [])) if ttl_ontology else 0
            }
            
        except Exception as e:
            print(f"âŒ TTL-enhanced R-GCN prediction failed: {e}")
            return None

    def _create_ttl_enhanced_features(self, nodes, ttl_ontology):
        """TTL ì •ë³´ë¥¼ í¬í•¨í•œ ë…¸ë“œ íŠ¹ì„± ë²¡í„° ìƒì„±"""
        enhanced_features = []
        
        for node in nodes:
            node_id = node.get("id", "")
            ttl_class = node.get("ontology_class", "Other")
            ttl_confidence = node.get("confidence", 0.5)
            
            # ê¸°ë³¸ íŠ¹ì„±
            features = {
                "text_length": len(node_id),
                "is_numeric": node_id.replace(".", "").isdigit(),
                "is_uppercase": node_id.isupper(),
                "has_special_chars": any(c in node_id for c in "-/_()"),
            }
            
            # TTL ì˜¨í†¨ë¡œì§€ íŠ¹ì„± ì¶”ê°€
            if ttl_ontology:
                # TTL í´ë˜ìŠ¤ ì›í•« ì¸ì½”ë”©
                ttl_classes = ttl_ontology.get("classes", [])
                for cls in ttl_classes:
                    features[f"ttl_class_{cls}"] = 1.0 if ttl_class == cls else 0.0
                
                # TTL ì¸ìŠ¤í„´ìŠ¤ ë§¤ì¹­
                ttl_instances = ttl_ontology.get("instances", {})
                features["ttl_instance_match"] = 0.0
                for cls, instances in ttl_instances.items():
                    if any(inst.lower() in node_id.lower() for inst in instances):
                        features["ttl_instance_match"] = 1.0
                        features[f"ttl_matched_class_{cls}"] = 1.0
                        break
                
                # TTL ì‹ ë¢°ë„
                features["ttl_confidence"] = ttl_confidence
            
            enhanced_features.append(features)
        
        return enhanced_features

    def _process_ttl_relations(self, edges, ttl_ontology):
        """TTL ê´€ê³„ë¥¼ R-GCN ì—£ì§€ë¡œ ì²˜ë¦¬"""
        if not ttl_ontology:
            return edges
        
        ttl_relations = ttl_ontology.get("object_properties", [])
        processed_edges = []
        
        for edge in edges:
            relation = edge.get("relation", "unknown")
            
            # TTL ê´€ê³„ íƒ€ì… ë§¤í•‘
            if relation in ttl_relations:
                edge["ttl_relation"] = True
                edge["relation_weight"] = 1.0
            else:
                edge["ttl_relation"] = False
                edge["relation_weight"] = 0.5
            
            processed_edges.append(edge)
        
        return processed_edges

    def _rgcn_clustering_with_ttl(self, node_features, edge_features, ttl_ontology):
        """TTL ì œì•½ ì¡°ê±´ì„ ì ìš©í•œ R-GCN í´ëŸ¬ìŠ¤í„°ë§"""
        # ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ë§ ë¡œì§ + TTL ì œì•½
        
        # 1. TTL í´ë˜ìŠ¤ ê¸°ë°˜ ì´ˆê¸° í´ëŸ¬ìŠ¤í„°ë§
        ttl_clusters = {}
        if ttl_ontology:
            ttl_classes = ttl_ontology.get("classes", [])
            for i, features in enumerate(node_features):
                # TTL í´ë˜ìŠ¤ ì¤‘ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ê²ƒ ì„ íƒ
                best_class = "Other"
                best_score = 0.0
                
                for cls in ttl_classes:
                    score = features.get(f"ttl_class_{cls}", 0.0)
                    if score > best_score:
                        best_score = score
                        best_class = cls
                
                if best_class not in ttl_clusters:
                    ttl_clusters[best_class] = []
                ttl_clusters[best_class].append(i)
        
        # 2. ê¸°ì¡´ R-GCN í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ì™€ ê²°í•©
        base_clusters = self._original_clustering(node_features, edge_features)
        
        # 3. TTL ì§€ì‹ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ì •ì œ
        refined_clusters = self._merge_clusters_with_ttl(base_clusters, ttl_clusters, ttl_ontology)
        
        return refined_clusters

    def _refine_with_ttl_knowledge(self, clusters, nodes, ttl_ontology):
        """TTL ì§€ì‹ìœ¼ë¡œ ì˜ˆì¸¡ ê²°ê³¼ ì •ì œ"""
        refined_predictions = []
        
        for i, node in enumerate(nodes):
            node_id = node.get("id", "")
            cluster = clusters.get(i, "unknown")
            
            # TTL ê¸°ë°˜ ì‹ ë¢°ë„ ì¡°ì •
            confidence = 0.5
            
            if ttl_ontology:
                # TTL ì¸ìŠ¤í„´ìŠ¤ì™€ ë§¤ì¹­ë˜ë©´ ì‹ ë¢°ë„ ì¦ê°€
                ttl_instances = ttl_ontology.get("instances", {})
                for cls, instances in ttl_instances.items():
                    if any(inst.lower() in node_id.lower() for inst in instances):
                        confidence = min(0.95, confidence + 0.3)
                        cluster = self._map_ttl_class_to_importance(cls)
                        break
                
                # TTL í´ë˜ìŠ¤ ì •ë³´ í™œìš©
                ttl_class = node.get("ontology_class", "Other")
                if ttl_class != "Other":
                    confidence = min(0.9, confidence + 0.2)
                    cluster = self._map_ttl_class_to_importance(ttl_class)
            
            refined_predictions.append({
                "node_id": node_id,
                "predicted_cluster": cluster,
                "confidence": confidence,
                "ttl_enhanced": True,
                "original_ttl_class": node.get("ontology_class", "Other")
            })
        
        return refined_predictions

    def _map_ttl_class_to_importance(self, ttl_class):
        """TTL í´ë˜ìŠ¤ë¥¼ ì¤‘ìš”ë„ë¡œ ë§¤í•‘"""
        importance_mapping = {
            "Equipment": "high_importance",
            "ProcessRequirement": "high_importance", 
            "Project": "high_importance",
            "Note": "medium_importance",
            "Revision": "medium_importance",
            "Person": "medium_importance",
            "Other": "low_importance",
            "FeedType": "low_importance",
            "ConditionType": "low_importance"
        }
        return importance_mapping.get(ttl_class, "low_importance")

    def _calculate_ttl_distribution(self, predictions):
        """TTL ê¸°ë°˜ í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚°"""
        distribution = {}
        importance_to_class = {
            "high_importance": 0,
            "medium_importance": 1, 
            "low_importance": 2
        }
        
        for pred in predictions:
            cluster = pred.get("predicted_cluster", "low_importance")
            class_id = importance_to_class.get(cluster, 2)
            distribution[class_id] = distribution.get(class_id, 0) + 1
        
        return distribution

    def _original_clustering(self, node_features, edge_features):
        """ê¸°ì¡´ R-GCN í´ëŸ¬ìŠ¤í„°ë§ ë¡œì§ (ë°±ì—…ìš©)"""
        # ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ë§ ë¡œì§ì„ ì—¬ê¸°ì— ë³´ì¡´
        clusters = {}
        for i in range(len(node_features)):
            clusters[i] = "medium_importance"  # ê¸°ë³¸ê°’
        return clusters

    def _merge_clusters_with_ttl(self, base_clusters, ttl_clusters, ttl_ontology):
        """ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ì™€ TTL í´ëŸ¬ìŠ¤í„° ë³‘í•©"""
        merged = base_clusters.copy()
        
        # TTL í´ëŸ¬ìŠ¤í„°ê°€ ë” ì‹ ë¢°í•  ë§Œí•˜ë©´ ìš°ì„  ì ìš©
        for ttl_class, node_indices in ttl_clusters.items():
            importance = self._map_ttl_class_to_importance(ttl_class)
            for idx in node_indices:
                merged[idx] = importance
        
        return merged

class NaturalLanguageQueryProcessor:
    def __init__(self, ontology_learner, db_manager, rgcn_manager=None):
        self.ontology_learner = ontology_learner
        self.db_manager = db_manager
        self.rgcn_manager = rgcn_manager
        self._last_graph_data = None  # ë©”ëª¨ë¦¬ ë°ì´í„° ì €ì¥ìš©
        
        # í™•ì¥ëœ ì¿¼ë¦¬ íŒ¨í„´ ë§¤í•‘ (ìƒˆë¡œìš´ ì—”í‹°í‹° íƒ€ì… ì§€ì›)
        self.query_patterns = {
            "show_all": [
                r"show\s+all\s+(\w+)",
                r"list\s+all\s+(\w+)",
                r"find\s+all\s+(\w+)",
                r"get\s+all\s+(\w+)"
            ],
            "filter_by_confidence": [
                r"(?:show|find|list)\s+(\w+)\s+with\s+(?:high|good)\s+confidence",
                r"(?:show|find|list)\s+high\s+confidence\s+(\w+)",
                r"(\w+)\s+with\s+confidence\s+>\s*(\d+\.?\d*)"
            ],
            "filter_by_class": [
                r"(?:show|find|list)\s+(\w+)\s+(?:of\s+type|class)\s+(\w+)",
                r"(\w+)\s+that\s+are\s+(\w+)",
                r"all\s+(\w+)\s+(\w+)"
            ],
            "count": [
                r"(?:how\s+many|count)\s+(\w+)",
                r"number\s+of\s+(\w+)",
                r"total\s+(\w+)"
            ],
            "stats": [
                r"(?:statistics|stats)\s+(?:for|of|about)?\s*(\w+)?",
                r"(?:summary|overview)\s+(?:of|about)?\s*(\w+)?",
                r"(?:analyze|analysis)\s+(\w+)?",
                r"database\s+(?:stats|statistics)"
            ],
            "relationships": [
                r"(?:show|find|list)\s+(?:relationships?|relations?|connections?)",
                r"what\s+is\s+connected\s+to\s+(\w+)",
                r"(\w+)\s+(?:connected|related|linked)\s+to\s+(\w+)"
            ],
            "recent": [
                r"(?:recent|latest|newest)\s+(\w+)",
                r"last\s+(\d+)\s+(\w+)",
                r"show\s+recent\s+(?:documents|files)"
            ],
            "search": [
                r"search\s+(?:for\s+)?(.+)",
                r"find\s+(.+)\s+in\s+database",
                r"lookup\s+(.+)"
            ],
            "rgcn": [
                r"r-?gcn\s+(?:predict|prediction|status)",
                r"(?:run|execute)\s+r-?gcn",
                r"neural\s+(?:network|model)\s+predict",
                r"r-?gcn\s+(?:compare|comparison)",
                r"(?:show|display)\s+r-?gcn\s+(?:result|results)"
            ],
            # ìƒˆë¡œìš´ íŠ¹í™” íŒ¨í„´ë“¤
            "field_search": [
                r"(?:show|find|list)\s+field\s+(.+)",
                r"field\s+(.+)",
                r"what\s+is\s+(.+)\s+field"
            ],
            "note_search": [
                r"(?:show|find|list)\s+note\s*(\d*)",
                r"note\s+(\d+)",
                r"notes?\s+about\s+(.+)"
            ]
        }
        
        # ëŒ€í­ í™•ì¥ëœ ë™ì˜ì–´ ë§¤í•‘ (í™”í•™ê³µì • íŠ¹í™”)
        self.synonyms = {
            # ê¸°ë³¸ ì¥ë¹„ ê´€ë ¨
            "equipment": ["equipment", "machine", "device", "pump", "motor", "driver", "vessel", "tank", "compressor"],
            "project": ["project", "job", "document", "doc", "specification", "drawing"],
            "requirement": ["requirement", "spec", "specification", "parameter", "condition"],
            "person": ["person", "people", "engineer", "reviewer", "checker", "by", "checked", "reviewed"],
            "entity": ["entity", "entities", "item", "object", "node"],
            "literal": ["literal", "value", "data", "text"],
            "confidence": ["confidence", "certainty", "reliability", "accuracy"],
            "documents": ["documents", "files", "pdfs", "papers"],
            
            # í”„ë¡œì„¸ìŠ¤ ê´€ë ¨ (ëŒ€í­ í™•ì¥)
            "temperature": ["temperature", "temp", "pumping temperature", "operating temperature", "minimum temperature", "maximum temperature"],
            "pressure": ["pressure", "suction pressure", "discharge pressure", "differential pressure", "vapor pressure"],
            "capacity": ["capacity", "flow", "flowrate", "flow rate", "rated capacity", "normal capacity"],
            "viscosity": ["viscosity", "fluid viscosity", "liquid viscosity"],
            "density": ["density", "specific gravity", "fluid density"],
            "head": ["head", "differential head", "pump head"],
            "npsh": ["npsh", "npsha", "net positive suction head", "available npsh"],
            
            # ì¬ë£Œ/ë¶„ë¥˜
            "material": ["material", "materials", "api class", "classification", "casing", "impeller", "shaft"],
            "api": ["api", "api class", "api classification", "american petroleum institute"],
            
            # ìš´ì „ ì¡°ê±´
            "duty": ["duty", "operation", "operating", "continuous", "intermittent"],
            "startup": ["startup", "start-up", "start up", "starting", "initial condition"],
            "minimum": ["minimum", "min", "lowest", "bottom"],
            "maximum": ["maximum", "max", "highest", "top"],
            "normal": ["normal", "standard", "typical", "operating"],
            "rated": ["rated", "design", "nominal"],
            
            # í™”í•™ê³µì • ìœ ì²´
            "fluid": ["fluid", "liquid", "gas oil", "overflash", "ah feed", "am feed"],
            "overflash": ["overflash", "over flash", "overhead"],
            "feed": ["feed", "ah feed", "am feed", "feedstock"],
            
            # ì•ˆì „/ìœ„í—˜
            "flammable": ["flammable", "combustible", "fire hazard"],
            "toxic": ["toxic", "poisonous", "hazardous"],
            "sulfur": ["sulfur", "sulphur", "h2s", "hydrogen sulfide"],
            
            # ì„¤ì¹˜/ìœ„ì¹˜
            "location": ["location", "indoor", "outdoor", "under roof"],
            "insulation": ["insulation", "steam tracing", "steam jacket", "heating"],
            
            # ì œì–´/ì¡°ì‘
            "manual": ["manual", "hand operated", "manually operated"],
            "automatic": ["automatic", "auto", "automatically operated"],
            
            # NOTE ê´€ë ¨ í‚¤ì›Œë“œ
            "note": ["note", "notes", "remark", "comment", "description"],
            "foundation": ["foundation", "base", "mounting", "support"],
            "turndown": ["turndown", "turn down", "reduced operation", "minimum operation"],
            "overdesign": ["overdesign", "over design", "safety margin", "design margin"],
            "mdmt": ["mdmt", "minimum design metal temperature", "minimum temperature"],
            
            # í•„ë“œ ê´€ë ¨
            "field": ["field", "parameter", "specification", "data", "information"],
            "type": ["type", "classification", "category", "kind"],
            "required": ["required", "specification", "requirement", "needed"]
        }
        
        # ì•½ì–´/ì „ì²´ëª… ë§¤í•‘
        self.abbreviation_mapping = {
            "npsh": "net positive suction head",
            "npsha": "npsh available",
            "mdmt": "minimum design metal temperature",
            "api": "american petroleum institute",
            "h2s": "hydrogen sulfide",
            "cp": "centipoise",
            "gpm": "gallons per minute",
            "bph": "barrels per hour",
            "psi": "pounds per square inch",
            "deg": "degree",
            "wt": "weight",
            "pt": "point",
            "max": "maximum",
            "min": "minimum",
            "nor": "normal",
            "oper": "operating",
            "temp": "temperature"
        }
        
        # ì»¨í…ìŠ¤íŠ¸ë³„ í‚¤ì›Œë“œ ë§¤í•‘
        self.context_keywords = {
            "temperature_related": ["pumping", "operating", "minimum", "maximum", "design", "metal"],
            "pressure_related": ["suction", "discharge", "differential", "vapor", "rated"],
            "flow_related": ["capacity", "rate", "continuous", "minimum", "maximum", "rated", "normal"],
            "material_related": ["casing", "impeller", "shaft", "api", "class", "classification"],
            "note_related": ["npsha", "foundation", "turndown", "overdesign", "mdmt", "startup", "slop"],
            "safety_related": ["flammable", "toxic", "h2s", "sulfur", "leakage", "hazard"]
        }
    
    def process_query(self, query, graph_data=None):
        """ìì—°ì–´ ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•˜ì—¬ ê²°ê³¼ ë°˜í™˜ (ëŒ€í­ ê°œì„ ëœ ë²„ì „)"""
        query = query.lower().strip()
        print(f"ğŸ” Processing enhanced query: '{query}'")
        
        # ë©”ëª¨ë¦¬ ë°ì´í„° ì €ì¥
        if graph_data:
            self._last_graph_data = graph_data
        
        # 1. ì¿¼ë¦¬ ì „ì²˜ë¦¬ (ì•½ì–´ í™•ì¥, ë™ì˜ì–´ ì •ê·œí™”)
        processed_query = self._preprocess_query(query)
        print(f"ğŸ”„ Processed query: '{processed_query}'")
        
        # 2. ì¿¼ë¦¬ íƒ€ì… ì‹ë³„
        query_type, matches = self._identify_query_type(processed_query)
        print(f"ğŸ¯ Query type: {query_type}, matches: {matches}")
        
        # 3. ë°ì´í„°ë² ì´ìŠ¤ ìš°ì„  ì¿¼ë¦¬ë“¤
        if query_type in ["stats", "recent", "search", "rgcn"]:
            return self._handle_database_query(query_type, matches, graph_data, processed_query)
        
        # 4. ìƒˆë¡œìš´ íŠ¹í™” ì¿¼ë¦¬ë“¤
        if query_type == "field_search":
            return self._handle_field_search(matches, graph_data)
        elif query_type == "note_search":
            return self._handle_note_search(matches, graph_data)
        
        # 5. ë©”ëª¨ë¦¬ ë°ì´í„° ì¿¼ë¦¬ë“¤ (ê¸°ì¡´ + ê°œì„ )
        if graph_data:
            nodes = graph_data.get("nodes", [])
            edges = graph_data.get("edges", [])
            
            if query_type == "show_all":
                return self._handle_show_all(matches, nodes, edges)
            elif query_type == "filter_by_confidence":
                return self._handle_confidence_filter(matches, nodes)
            elif query_type == "filter_by_class":
                return self._handle_class_filter(matches, nodes)
            elif query_type == "count":
                return self._handle_count(matches, nodes)
            elif query_type == "relationships":
                return self._handle_relationships(matches, nodes, edges)
        
        # 6. Fallback: í–¥ìƒëœ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰
        return self._handle_enhanced_database_fallback(processed_query, query)
    
    def _preprocess_query(self, query):
        """ì¿¼ë¦¬ ì „ì²˜ë¦¬: ì•½ì–´ í™•ì¥ ë° ë™ì˜ì–´ ì •ê·œí™”"""
        processed = query
        
        # 1. ì•½ì–´ í™•ì¥
        for abbr, full_form in self.abbreviation_mapping.items():
            processed = re.sub(rf'\b{re.escape(abbr)}\b', full_form, processed, flags=re.IGNORECASE)
        
        # 2. íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        processed = re.sub(r'[^\w\s-]', ' ', processed)
        processed = re.sub(r'\s+', ' ', processed).strip()
        
        return processed
    
    def _search_memory_data(self, query, graph_data):
        """ë©”ëª¨ë¦¬ ë°ì´í„°ì—ì„œ ì§ì ‘ ê²€ìƒ‰"""
        if not graph_data or 'nodes' not in graph_data:
            return None
        
        nodes = graph_data['nodes']
        query_lower = query.lower()
        
        # ëª¨ë“  ë…¸ë“œì—ì„œ ê²€ìƒ‰ (Entity + Literal)
        matches = [node for node in nodes 
                  if query_lower in node.get('id', '').lower()]
        
        return {
            "type": "table",
            "data": matches,
            "message": f"Found {len(matches)} items in memory matching '{query}'",
            "columns": ["id", "type", "ontology_class", "confidence"]
        }

    def _identify_query_type(self, query):
        """ì¿¼ë¦¬ íƒ€ì…ê³¼ ë§¤ì¹­ëœ ê·¸ë£¹ ì‹ë³„"""
        for query_type, patterns in self.query_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, query, re.IGNORECASE)
                if match:
                    return query_type, match.groups()
        return "unknown", ()
    
    def _normalize_term(self, term):
        """ìš©ì–´ ì •ê·œí™” (ë™ì˜ì–´ ì²˜ë¦¬)"""
        if not term:
            return term
            
        term = term.lower()
        for canonical, synonyms in self.synonyms.items():
            if term in synonyms:
                return canonical
        return term
    
    def _handle_database_query(self, query_type, matches, graph_data, processed_query):
        """ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ì¿¼ë¦¬ ì²˜ë¦¬ (ê²€ìƒ‰ ê°•í™”)"""
        try:
            if query_type == "stats":
                db_stats = self.db_manager.get_database_stats()
                stats_data = {
                    "Total Documents": db_stats.get("total_documents", 0),
                    "Total Nodes": db_stats.get("total_nodes", 0),
                    "Total Edges": db_stats.get("total_edges", 0),
                    "Total Patterns": db_stats.get("total_patterns", 0),
                    "High Confidence Entities": db_stats.get("high_confidence_entities", 0)
                }
                return {
                    "type": "stat",
                    "data": stats_data,
                    "message": "Database statistics across all processed documents"
                }
            
            elif query_type == "recent":
                recent_query = """
                    SELECT filename, upload_time, content_length, processing_time
                    FROM documents 
                    ORDER BY upload_time DESC 
                    LIMIT 10
                """
                result_df = self.db_manager.execute_query(recent_query)
                
                if not result_df.empty:
                    result_df['upload_time'] = result_df['upload_time'].apply(
                        lambda x: x.split('T')[0] if 'T' in str(x) else str(x)
                    )
                    return {
                        "type": "table",
                        "data": result_df.to_dict('records'),
                        "message": f"Found {len(result_df)} recent documents",
                        "columns": ["filename", "upload_time", "content_length", "processing_time"]
                    }
                else:
                    return {"type": "table", "data": [], "message": "No documents found in database"}
            
            elif query_type == "search":
                search_term = matches[0] if matches else ""
                return self._enhanced_database_search(search_term, processed_query)

            elif query_type == "rgcn":
                return self._handle_rgcn_query(matches, graph_data)
            
            # Fallback: í–¥ìƒëœ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰
            return self._handle_enhanced_database_fallback(processed_query, processed_query)

        except Exception as e:
            print(f"âŒ Database query error: {e}")
            return {"type": "error", "message": f"Database query failed: {str(e)}"}
    
    def _enhanced_database_search(self, search_term, processed_query):
        """í–¥ìƒëœ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ (ë‹¤ì¤‘ í‚¤ì›Œë“œ, ì»¨í…ìŠ¤íŠ¸ ê³ ë ¤)"""
        try:
            # ê²€ìƒ‰ì–´ì—ì„œ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë“¤ ì¶”ì¶œ
            keywords = self._extract_meaningful_keywords(search_term, processed_query)
            print(f"ğŸ” Extracted keywords: {keywords}")
            
            if not keywords:
                return {"type": "suggestions", "data": ["temperature", "pressure", "capacity", "npsha", "startup"], 
                       "message": "Try searching for specific terms like:"}
            
            # ë‹¤ì¤‘ í‚¤ì›Œë“œ ê²€ìƒ‰ ì¡°ê±´ ìƒì„±
            search_conditions = []
            for keyword in keywords[:5]:  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œ
                search_conditions.extend([
                    f"LOWER(n.id) LIKE '%{keyword}%'",
                    f"LOWER(n.ontology_class) LIKE '%{keyword}%'",
                    f"LOWER(d.filename) LIKE '%{keyword}%'"
                ])
            
            search_query = f"""
                SELECT DISTINCT n.id, n.node_type, n.ontology_class, n.confidence, d.filename
                FROM nodes n
                JOIN documents d ON n.document_id = d.id
                WHERE {' OR '.join(search_conditions)}
                ORDER BY n.confidence DESC, n.ontology_class
                LIMIT 25
            """
            
            result_df = self.db_manager.execute_query(search_query)
            
            return {
                "type": "table",
                "data": result_df.to_dict('records') if not result_df.empty else [],
                "message": f"Found {len(result_df)} items matching '{search_term}' (keywords: {', '.join(keywords)})",
                "columns": ["id", "node_type", "ontology_class", "confidence", "filename"]
            }
            
        except Exception as e:
            print(f"âŒ Enhanced search error: {e}")
            return {"type": "error", "message": f"Enhanced search failed: {str(e)}"}
    
    def _extract_meaningful_keywords(self, search_term, processed_query):
        """ê²€ìƒ‰ì–´ì—ì„œ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë“¤ ì¶”ì¶œ"""
        keywords = set()
        
        # 1. ì›ë³¸ ê²€ìƒ‰ì–´ ë¶„í• 
        original_words = search_term.split()
        keywords.update([w for w in original_words if len(w) > 2])
        
        # 2. ë™ì˜ì–´ ë§¤í•‘ì—ì„œ ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
        for canonical, synonyms in self.synonyms.items():
            if any(word in search_term for word in synonyms):
                keywords.add(canonical)
                keywords.update([s for s in synonyms if len(s) > 2])
        
        # 3. ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ì¶”ê°€
        for context, context_keywords in self.context_keywords.items():
            if any(keyword in search_term for keyword in context_keywords):
                keywords.update(context_keywords)
        
        # 4. ë¶ˆìš©ì–´ ì œê±°
        stop_words = {"for", "the", "and", "or", "in", "on", "at", "to", "from", "with", "by"}
        keywords = keywords - stop_words
        
        return list(keywords)
    
    def _handle_field_search(self, matches, graph_data):
        """í•„ë“œ ê²€ìƒ‰ ì²˜ë¦¬"""
        if not graph_data or not matches:
            return {"type": "error", "message": "No field search term provided"}
        
        search_term = matches[0].lower()
        nodes = graph_data.get("nodes", [])
        
        # í•„ë“œ ê´€ë ¨ ë…¸ë“œë“¤ ì°¾ê¸°
        field_nodes = [n for n in nodes if n.get("ontology_class") == "Field" and 
                      search_term in n.get("id", "").lower()]
        
        return {
            "type": "table",
            "data": field_nodes,
            "message": f"Found {len(field_nodes)} fields matching '{search_term}'",
            "columns": ["id", "ontology_class", "confidence"]
        }
    
    def _handle_note_search(self, matches, graph_data):
        """NOTE ê²€ìƒ‰ ì²˜ë¦¬"""
        if not graph_data:
            return {"type": "error", "message": "No graph data available"}
        
        nodes = graph_data.get("nodes", [])
        
        if matches and matches[0].isdigit():
            # íŠ¹ì • NOTE ë²ˆí˜¸ ê²€ìƒ‰
            note_num = matches[0]
            note_nodes = [n for n in nodes if n.get("ontology_class") == "Note" and 
                         note_num in n.get("id", "")]
            message = f"Found NOTE {note_num}"
        else:
            # ëª¨ë“  NOTE ê²€ìƒ‰
            note_nodes = [n for n in nodes if n.get("ontology_class") == "Note"]
            message = f"Found {len(note_nodes)} notes"
        
        return {
            "type": "table",
            "data": note_nodes,
            "message": message,
            "columns": ["id", "ontology_class", "confidence"]
        }
    
    def _handle_show_all(self, matches, nodes, edges):
        """'show all X' íƒ€ì… ì¿¼ë¦¬ ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)"""
        if not matches:
            return {"type": "table", "data": nodes[:10], "message": "Showing all nodes (limited to 10)"}
        
        target = self._normalize_term(matches[0])
        
        if target in ["entity", "entities"]:
            entities = [n for n in nodes if n.get("type") == "entity"]
            return {
                "type": "table", 
                "data": entities,
                "message": f"Found {len(entities)} entities",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in ["literal", "literals", "value", "values"]:
            literals = [n for n in nodes if n.get("type") == "literal"]
            return {
                "type": "table", 
                "data": literals,
                "message": f"Found {len(literals)} literals",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in ["field", "fields"]:
            fields = [n for n in nodes if n.get("ontology_class") == "Field"]
            return {
                "type": "table", 
                "data": fields,
                "message": f"Found {len(fields)} fields",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in ["note", "notes"]:
            notes = [n for n in nodes if n.get("ontology_class") == "Note"]
            return {
                "type": "table", 
                "data": notes,
                "message": f"Found {len(notes)} notes",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in self.synonyms.get("equipment", []):
            equipment = [n for n in nodes if n.get("ontology_class", "").lower() == "equipment"]
            return {
                "type": "table", 
                "data": equipment,
                "message": f"Found {len(equipment)} equipment entities",
                "columns": ["id", "confidence"]
            }
        elif target in self.synonyms.get("project", []):
            projects = [n for n in nodes if n.get("ontology_class", "").lower() == "project"]
            return {
                "type": "table", 
                "data": projects,
                "message": f"Found {len(projects)} project entities",
                "columns": ["id", "confidence"]
            }
        else:
            # ì¼ë°˜ì ì¸ ê²€ìƒ‰
            filtered = [n for n in nodes if target in n.get("id", "").lower() or 
                       target in n.get("ontology_class", "").lower()]
            return {
                "type": "table", 
                "data": filtered,
                "message": f"Found {len(filtered)} items matching '{target}'",
                "columns": ["id", "type", "ontology_class", "confidence"]
            }
    
    def _handle_confidence_filter(self, matches, nodes):
        """ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§ (ê°œì„ ëœ ë²„ì „)"""
        if len(matches) >= 2 and matches[1].replace('.', '').isdigit():
            # íŠ¹ì • ì„ê³„ê°’ ì§€ì •ëœ ê²½ìš°
            threshold = float(matches[1])
            target = self._normalize_term(matches[0])
        else:
            # 'high confidence' ë“±ì˜ ê²½ìš°
            threshold = 0.8
            target = self._normalize_term(matches[0]) if matches else "entity"
        
        filtered_nodes = []
        for node in nodes:
            confidence = node.get("confidence", 0)
            if confidence > threshold:
                if target == "entity" and node.get("type") == "entity":
                    filtered_nodes.append(node)
                elif target in node.get("ontology_class", "").lower():
                    filtered_nodes.append(node)
                elif target in ["all", "any", ""]:
                    filtered_nodes.append(node)
        
        return {
            "type": "table",
            "data": filtered_nodes,
            "message": f"Found {len(filtered_nodes)} items with confidence > {threshold}",
            "columns": ["id", "type", "ontology_class", "confidence"]
        }
    
    def _handle_class_filter(self, matches, nodes):
        """í´ë˜ìŠ¤/íƒ€ì… ê¸°ë°˜ í•„í„°ë§ (ê°œì„ ëœ ë²„ì „)"""
        if len(matches) >= 2:
            item_type = self._normalize_term(matches[0])
            class_name = self._normalize_term(matches[1])
            
            filtered = [n for n in nodes if 
                       class_name in n.get("ontology_class", "").lower() or
                       class_name in n.get("type", "").lower()]
            
            return {
                "type": "table",
                "data": filtered,
                "message": f"Found {len(filtered)} {item_type} of type {class_name}",
                "columns": ["id", "type", "ontology_class", "confidence"]
            }
        
        return {"type": "error", "message": "Could not parse class filter query"}
    
    def _handle_count(self, matches, nodes):
        """ê°œìˆ˜ ì„¸ê¸° (í™•ì¥ëœ ë²„ì „)"""
        if not matches:
            total = len(nodes)
            return {
                "type": "stat",
                "data": {"Total Nodes": total},
                "message": f"Total count: {total}"
            }
        
        target = self._normalize_term(matches[0])
        
        if target in ["entity", "entities"]:
            count = sum(1 for n in nodes if n.get("type") == "entity")
        elif target in ["literal", "literals"]:
            count = sum(1 for n in nodes if n.get("type") == "literal")
        elif target in ["field", "fields"]:
            count = sum(1 for n in nodes if n.get("ontology_class") == "Field")
        elif target in ["note", "notes"]:
            count = sum(1 for n in nodes if n.get("ontology_class") == "Note")
        elif target in self.synonyms.get("equipment", []):
            count = sum(1 for n in nodes if n.get("ontology_class", "").lower() == "equipment")
        elif target in self.synonyms.get("project", []):
            count = sum(1 for n in nodes if n.get("ontology_class", "").lower() == "project")
        else:
            count = sum(1 for n in nodes if target in n.get("ontology_class", "").lower())
        
        return {
            "type": "stat",
            "data": {f"{target.title()} Count": count},
            "message": f"Count of {target}: {count}"
        }
    
    def _handle_relationships(self, matches, nodes, edges):
        """ê´€ê³„ ì •ë³´ ì²˜ë¦¬ (í™•ì¥ëœ ë²„ì „)"""
        if not edges:
            return {
                "type": "table",
                "data": [],
                "message": "No relationships found in the current graph"
            }
        
        # ê´€ê³„ í†µê³„
        relation_counts = {}
        for edge in edges:
            rel = edge.get("relation", "unknown")
            relation_counts[rel] = relation_counts.get(rel, 0) + 1
        
        relationship_data = [
            {"Relationship Type": rel, "Count": count}
            for rel, count in relation_counts.items()
        ]
        
        return {
            "type": "table",
            "data": relationship_data,
            "message": f"Found {len(edges)} relationships of {len(relation_counts)} types",
            "columns": ["Relationship Type", "Count"]
        }
    
    def _handle_enhanced_database_fallback(self, processed_query, original_query):
        """í–¥ìƒëœ ë°ì´í„°ë² ì´ìŠ¤ fallback ê²€ìƒ‰"""
        try:
            # ë©”ëª¨ë¦¬ ë°ì´í„° ë¨¼ì € ê²€ìƒ‰
            if self._last_graph_data:
                memory_result = self._search_memory_data(original_query, self._last_graph_data)
                if memory_result and memory_result.get('data'):
                    return memory_result

            keywords = self._extract_meaningful_keywords(original_query, processed_query)
            
            if not keywords:
                return {
                    "type": "suggestions",
                    "data": [
                        "temperature â†’ pumping temperature fields",
                        "pressure â†’ suction/discharge pressure",
                        "capacity â†’ flow rate information", 
                        "npsha â†’ foundation notes",
                        "startup â†’ start-up conditions",
                        "api â†’ material classifications"
                    ],
                    "message": "Try these enhanced search examples:"
                }
            
            # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ í™•ì¥
            expanded_keywords = set(keywords)
            for keyword in keywords:
                for context, context_keywords in self.context_keywords.items():
                    if keyword in context_keywords:
                        expanded_keywords.update(context_keywords[:3])  # ìµœëŒ€ 3ê°œ ì¶”ê°€
            
            keyword_conditions = " OR ".join([
                f"LOWER(n.id) LIKE '%{kw}%' OR LOWER(n.ontology_class) LIKE '%{kw}%'"
                for kw in list(expanded_keywords)[:8]  # ìµœëŒ€ 8ê°œ í‚¤ì›Œë“œ
            ])
            
            search_query = f"""
                SELECT n.id, n.node_type, n.ontology_class, n.confidence, d.filename
                FROM nodes n
                JOIN documents d ON n.document_id = d.id
                WHERE {keyword_conditions}
                ORDER BY n.confidence DESC, 
                    CASE n.ontology_class 
                        WHEN 'Field' THEN 1 
                        WHEN 'Note' THEN 2 
                        WHEN 'ProcessRequirement' THEN 3 
                        ELSE 4 
                    END
                LIMIT 20
            """
            
            result_df = self.db_manager.execute_query(search_query)
            
            if not result_df.empty:
                return {
                    "type": "table",
                    "data": result_df.to_dict('records'),
                    "message": f"Enhanced search found {len(result_df)} items (expanded from: {', '.join(keywords)})",
                    "columns": ["id", "node_type", "ontology_class", "confidence", "filename"]
                }
            else:
                return {
                    "type": "suggestions",
                    "data": [
                        "Try 'database statistics' to see available data",
                        "Try 'show recent documents' to see processed files",
                        "Try specific terms like 'temperature', 'pressure', 'pump'"
                    ],
                    "message": "No matches found. Suggestions:"
                }
        
        except Exception as e:
            print(f"âŒ Enhanced database fallback error: {e}")
            return {
                "type": "error",
                "message": f"Enhanced database search failed: {str(e)}"
            }

    def _handle_rgcn_query(self, matches, graph_data):
        """R-GCN ê´€ë ¨ ì¿¼ë¦¬ ì²˜ë¦¬"""
        # global rgcn_manager ë¶€ë¶„ì„ ì œê±°í•˜ê³  ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
        try:
            # rgcn_managerê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if not hasattr(self, 'rgcn_manager') or not self.rgcn_manager:
                return {"type": "error", "message": "R-GCN not available. Check system initialization."}
            
            if not graph_data:
                return {"type": "error", "message": "No graph data available for R-GCN prediction"}
            
            result = self.rgcn_manager.predict(graph_data)
            if result:
                predictions = result["predictions"]
                confidences = result["confidence_scores"]
                class_dist = result["class_distribution"]
                
                # í†µê³„ ìƒì„±
                pred_counts = {}
                class_names = ["Unknown", "Entity", "Literal", "Type"]
                for i, name in enumerate(class_names):
                    count = class_dist.get(i, 0)
                    if count > 0:
                        pred_counts[f"{name} Count"] = count
                
                avg_confidence = result["average_confidence"]
                pred_counts["Average Confidence"] = round(avg_confidence, 3)
                
                return {
                    "type": "stat",
                    "data": pred_counts,
                    "message": f"R-GCN prediction completed for {len(predictions)} nodes"
                }
            else:
                return {"type": "error", "message": "R-GCN prediction failed"}
                
        except Exception as e:
            return {"type": "error", "message": f"R-GCN error: {str(e)}"}

# ê¸€ë¡œë²Œ ë³€ìˆ˜ (ë°ì´í„°ë² ì´ìŠ¤ í†µí•©)



db_manager = DatabaseManager()

# R-GCN ê´€ë¦¬ì ì´ˆê¸°í™” (ìˆ˜ì •ë¨)
rgcn_manager = None
if RGCN_AVAILABLE:
    try:
        rgcn_manager = RGCNManager()  # â† "ontology.ttl" ì œê±°
        print("âœ… R-GCN Manager initialized")
    except Exception as e:
        print(f"âŒ R-GCN Manager initialization failed: {e}")
        rgcn_manager = None
else:
    print("âš ï¸ R-GCN not available - running without R-GCN support")

# NaturalLanguageQueryProcessor ì´ˆê¸°í™” ì‹œ rgcn_manager ì „ë‹¬
# TTL ì˜¨í†¨ë¡œì§€ í•™ìŠµì„ í¬í•¨í•œ í–¥ìƒëœ ì´ˆê¸°í™”

print("ğŸ§  Initializing Enhanced Ontology Learning System...")

# TTL ê¸°ë°˜ ì˜¨í†¨ë¡œì§€ í•™ìŠµê¸° ì´ˆê¸°í™”
try:
    ontology_learner = EnhancedOntologyLearner("ontology.ttl")
    print("âœ… TTL Ontology loaded successfully!")
    
    # TTL í•™ìŠµ ê²°ê³¼ ì¶œë ¥
    ttl_stats = ontology_learner.get_enhanced_statistics()
    print(f"ğŸ“š Ontology Statistics:")
    print(f"   ğŸ·ï¸ Total Classes: {ttl_stats['total_classes']} ({ttl_stats['ttl_classes']} from TTL)")
    print(f"   ğŸ”— Total Properties: {ttl_stats['total_properties']}")
    print(f"   ğŸ“Š TTL Triples: {ttl_stats['ttl_triples']}")
    print(f"   ğŸ“¦ TTL Instances: {ttl_stats['ttl_instances']}")
    print(f"   ğŸŒ³ Class Hierarchies: {ttl_stats['class_hierarchy_depth']}")
    print(f"   ğŸ“– Source: {ttl_stats['ontology_source']}")
    
except Exception as e:
    print(f"âš ï¸ TTL loading failed: {e}")
    print("ğŸ”„ Falling back to basic ontology learner...")
    ontology_learner = AdvancedOntologyLearner()  # ê¸°ì¡´ ë°©ì‹

# ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
db_manager = DatabaseManager()

# R-GCN ê´€ë¦¬ì ì´ˆê¸°í™” (TTL ì§€ì‹ í™œìš©)
rgcn_manager = None
if RGCN_AVAILABLE:
    try:
        rgcn_manager = RGCNManager()
        print("âœ… R-GCN Manager initialized with TTL support")
    except Exception as e:
        print(f"âŒ R-GCN Manager initialization failed: {e}")
        rgcn_manager = None
else:
    print("âš ï¸ R-GCN not available - running without R-GCN support")

# NaturalLanguageQueryProcessor ì´ˆê¸°í™” (TTL ì§€ì‹ í¬í•¨)
query_processor = NaturalLanguageQueryProcessor(ontology_learner, db_manager, rgcn_manager)

# TTL ì§€ì‹ì„ ì¿¼ë¦¬ í”„ë¡œì„¸ì„œì— ì¶”ê°€ ì„¤ì •
if hasattr(ontology_learner, 'get_ttl_knowledge'):
    ttl_knowledge = ontology_learner.get_ttl_knowledge()
    
    # TTL ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë™ì˜ì–´ ë§¤í•‘ì— ì¶”ê°€
    for class_name, instances in ttl_knowledge['instances'].items():
        class_key = class_name.lower()
        if class_key not in query_processor.synonyms:
            query_processor.synonyms[class_key] = []
        query_processor.synonyms[class_key].extend([inst.lower() for inst in instances])
    
    # TTL ì†ì„±ì„ ë™ì˜ì–´ ë§¤í•‘ì— ì¶”ê°€
    for prop in ttl_knowledge['datatype_properties']:
        prop_key = prop.lower()
        if prop_key not in query_processor.synonyms:
            query_processor.synonyms[prop_key] = [prop.lower()]
    
    print("âœ… Query processor enhanced with TTL knowledge")

graph_data_store = {}

print("ğŸ¯ Enhanced system initialization complete!")
graph_data_store = {}


# Dash ì•± ìƒì„±
app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])

app.layout = dbc.Container([
    html.H1("ğŸ­ Industrial Knowledge Graph Analyzer", className="text-primary mb-4"),
    
    dbc.Row([
        # ì™¼ìª½ íŒ¨ë„
        dbc.Col([
            dbc.Card([
                dbc.CardBody([
                    html.H5("ğŸ“„ Document Upload"),
                    dcc.Upload(
                        id="upload-pdf",
                        children=html.Div([
                            "Drag and drop PDF files or ",
                            html.A("click to select", style={"color": "#007bff"})
                        ]),
                        style={
                            'width': '100%',
                            'height': '60px',
                            'lineHeight': '60px',
                            'borderWidth': '2px',
                            'borderStyle': 'dashed',
                            'borderRadius': '5px',
                            'textAlign': 'center',
                            'backgroundColor': '#f8f9fa'
                        },
                        multiple=False
                    )
                ])
            ], className="mb-3"),

            dbc.Card([
                dbc.CardBody([
                    html.H5("ğŸ¯ Quick Actions"),
                    dbc.Button("ğŸ§ª Test Connection", id="test-btn", color="secondary", className="w-100 mb-2", size="sm"),
                    dbc.Button("ğŸ“Š Load Sample Data", id="sample-btn", color="info", className="w-100 mb-2", size="sm"),
                    dbc.Button("ğŸ”„ Update Charts", id="update-btn", color="warning", className="w-100 mb-2", size="sm"),
                ])
            ], className="mb-3"),
            
            # ìƒˆë¡œ ì¶”ê°€ë˜ëŠ” R-GCN ì¹´ë“œ
            dbc.Card([
                dbc.CardBody([
                    html.H5("ğŸ¤– R-GCN Model"),
                    dbc.Button("ğŸ”® R-GCN Predict", id="rgcn-predict-btn", color="success", className="w-100 mb-2", size="sm"),
                    dbc.Button("ğŸ“Š R-GCN vs Rule", id="rgcn-compare-btn", color="primary", className="w-100 mb-2", size="sm"),
                    html.Div(id="rgcn-status", className="mt-2")
                ])
            ], className="mb-3"),

            html.Div(id="status-output", className="mb-4"),
            
            # TTL ì˜¨í†¨ë¡œì§€ ì •ë³´ë¥¼ í¬í•¨í•œ ë™ì  ì‹œìŠ¤í…œ ì •ë³´ ì¹´ë“œ
            dbc.Card([
                dbc.CardBody([
                    html.H6("ğŸ“ˆ Enhanced System Info"),
                    html.Div(id="dynamic-system-info")  # ë™ì  ì—…ë°ì´íŠ¸ìš©
                ])
            ]),
            
            # í•™ìŠµ í†µê³„ ì¹´ë“œ ì¶”ê°€
            html.Div(id="learning-stats", className="mt-3")
        ], width=4),
        
        # ì˜¤ë¥¸ìª½ íŒ¨ë„
        dbc.Col([
            # ìì—°ì–´ ì¿¼ë¦¬ ì¸í„°í˜ì´ìŠ¤ ì¶”ê°€
            dbc.Card([
                dbc.CardBody([
                    html.H5("ğŸ—£ï¸ Natural Language Query"),
                    dbc.InputGroup([
                        dbc.Input(
                            id="nl-query-input",
                            placeholder="e.g., 'show all equipment with high confidence', 'database statistics', 'search for pump'",
                            type="text"
                        ),
                        dbc.Button("ğŸ” Query", id="query-btn", color="primary")
                    ]),
                    html.Div(id="query-output", className="mt-3")
                ])
            ], className="mb-4"),
            
            html.H4("ğŸ“Š Node Classification"),
            dcc.Graph(id="pie-chart", style={'height': '400px'}),
            
            html.Hr(),
            
            html.H4("ğŸ•¸ï¸ Knowledge Graph"),
            dcc.Graph(id="network-graph", style={'height': '500px'})
        ], width=8)
    ]),
    
    # ìˆ¨ê²¨ì§„ ë°ì´í„° ì €ì¥ì†Œ
    dcc.Store(id="data-store", data={})
], fluid=True)

# ì½œë°±ë“¤
@app.callback(
    [Output("status-output", "children"), Output("data-store", "data")],
    [Input("test-btn", "n_clicks"), Input("sample-btn", "n_clicks"), Input("upload-pdf", "contents")],
    [State("upload-pdf", "filename")],
    prevent_initial_call=True
)
def handle_all_inputs(test_clicks, sample_clicks, pdf_contents, pdf_filename):
    global db_manager, rgcn_manager, query_processor, ontology_learner  # ontology_learner ì¶”ê°€
    from dash import ctx
    
    trigger_id = ctx.triggered[0]['prop_id'].split('.')[0] if ctx.triggered else None
    print(f"ğŸ”¥ Input callback triggered: {trigger_id}")
    
    if trigger_id == "test-btn":
        print("âœ… Test button clicked")
        # TTL ì˜¨í†¨ë¡œì§€ ìƒíƒœë„ í…ŒìŠ¤íŠ¸ì— í¬í•¨
        ttl_status = "âœ… TTL Loaded" if hasattr(ontology_learner, 'ttl_classes') and ontology_learner.ttl_classes else "âš ï¸ TTL Not Loaded"
        return dbc.Alert([
            html.P("âœ… Connection test successful! Database ready.", className="mb-1"),
            html.P(f"ğŸ§  Ontology: {ttl_status}", className="mb-0")
        ], color="success"), {}
    
    elif trigger_id == "sample-btn":
        print("ğŸ“Š Sample button clicked - generating data...")
        
        G, node_features, predictions = create_sample_graph()
        
        # ìƒ˜í”Œ ë°ì´í„°ì—ë„ R-GCN ì˜ˆì¸¡ ì¶”ê°€
        rgcn_predictions = None
        if rgcn_manager:
            try:
                # R-GCNìš© ë…¸ë“œ ë°ì´í„° ì •ë¦¬ (í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©)
                rgcn_nodes = []
                for node in G.nodes():
                    node_data = node_features[node]
                    # ì›ë³¸ í…ìŠ¤íŠ¸ ë˜ëŠ” ë…¸ë“œ ID ì‚¬ìš©
                    original_text = node_data.get('original_text', node)
                    rgcn_nodes.append({
                        "id": original_text,  # ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
                        "type": node_data.get("type", "entity"),
                        "ontology_class": node_data.get("ontology_class", "Other"),
                        "confidence": node_data.get("confidence", 0.5)
                    })
                
                graph_data_for_rgcn = {
                    "nodes": rgcn_nodes,
                    "edges": [{"source": u, "target": v, "relation": data.get("relation", "unknown")} 
                            for u, v, data in G.edges(data=True)]
                }
                rgcn_result = rgcn_manager.predict(sample_graph_data)
                if rgcn_result:
                    rgcn_predictions = rgcn_result["predictions"]
                    print(f"âœ… R-GCN predictions for sample: {len(rgcn_predictions)} nodes")
            except Exception as e:
                print(f"âŒ R-GCN prediction error: {e}")
        
        # TTL ì˜¨í†¨ë¡œì§€ ì •ë³´ ì¶”ê°€
        ttl_enhanced = hasattr(ontology_learner, 'ttl_classes') and len(ontology_learner.ttl_classes) > 0
        
        graph_data = {
            "nodes": [{"id": node, **node_features[node]} for node in G.nodes()],
            "edges": [{"source": u, "target": v, "relation": data.get("relation", "unknown")} 
                     for u, v, data in G.edges(data=True)],
            "predictions": predictions,
            "rgcn_predictions": rgcn_predictions,
            "stats": {
                "total_nodes": G.number_of_nodes(),
                "total_edges": G.number_of_edges(),
                "source": "sample",
                "rgcn_available": rgcn_predictions is not None,
                "ttl_enhanced": ttl_enhanced,
                "ontology_source": "TTL + Default" if ttl_enhanced else "Default only"
            }
        }
        
        # query_processorì— ê·¸ë˜í”„ ë°ì´í„° ì „ë‹¬ (ë©”ëª¨ë¦¬ ì €ì¥)
        if query_processor:
            query_processor._last_graph_data = graph_data
        
        print(f"âœ… Sample data created: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
        
        message = dbc.Alert([
            html.H5("âœ… Sample data loaded!", className="alert-heading"),
            html.P(f"Created {G.number_of_nodes()} nodes and {G.number_of_edges()} edges"),
            html.P(f"ğŸ§  Ontology: {'TTL Enhanced' if ttl_enhanced else 'Basic Mode'}"),
            html.P(f"ğŸ¤– R-GCN: {'âœ… Available' if rgcn_predictions else 'âŒ Not Available'}"),
            html.P("Click 'Update Charts' to see visualizations!")
        ], color="success")
        
        return message, graph_data
    
    elif trigger_id == "upload-pdf" and pdf_contents:
        print(f"ğŸ“„ PDF uploaded: {pdf_filename}")
        
        try:
            if not pdf_filename or not pdf_filename.lower().endswith('.pdf'):
                return dbc.Alert("âŒ Please select a PDF file", color="warning"), {}
            
            content_type, content_string = pdf_contents.split(',')
            decoded = base64.b64decode(content_string)
            
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(decoded)
                tmp_file_path = tmp_file.name
            
            try:
                # OCR ì§€ì› PDF ì²˜ë¦¬
                pdf_content = extract_pdf_content(tmp_file_path)
                pdf_text = pdf_content['text']
                pdf_info = pdf_content['pdf_info']
                ocr_results = pdf_content['ocr_results']
                processing_method = pdf_content['processing_method']
                
                print(f"ğŸ“ PDF ì²˜ë¦¬ ì™„ë£Œ: {len(pdf_text)} characters, ë°©ë²•: {processing_method}")
                print(f"ğŸ“Š PDF ì •ë³´: íƒ€ì…={pdf_info['pdf_type']}, ì´ë¯¸ì§€={pdf_info.get('total_images', 0)}ê°œ")
                
                if len(pdf_text.strip()) < 10:
                    return dbc.Alert("âŒ No meaningful text found in PDF", color="warning"), {}
                
                # ê·¸ë˜í”„ ìƒì„± (TTL ì˜¨í†¨ë¡œì§€ + OCR ë°ì´í„° í¬í•¨)
                import time
                start_time = time.time()
                
                # OCR ê²°ê³¼ë¥¼ ê·¸ë˜í”„ ìƒì„±ì— ì „ë‹¬
                extended_content = {
                    'text': pdf_text,
                    'ocr_results': ocr_results,
                    'pdf_info': pdf_info
                }
                
                # TTL ì˜¨í†¨ë¡œì§€ í™œìš© ê·¸ë˜í”„ ìƒì„±
                G, node_features, predictions, learning_results = process_pdf_to_graph(extended_content, pdf_filename)
                
                # TTL ì˜¨í†¨ë¡œì§€ ì •ë³´ í™•ì¸
                ttl_enhanced = hasattr(ontology_learner, 'ttl_classes') and len(ontology_learner.ttl_classes) > 0
                ttl_classifications = learning_results.get('ttl_classifications', 0) if ttl_enhanced else 0
                
                # R-GCN ì˜ˆì¸¡ ìˆ˜í–‰
                rgcn_predictions = None
                if rgcn_manager:
                    try:
                        graph_data_for_rgcn = {
                            "nodes": [{"id": node, **node_features[node]} for node in G.nodes()],
                            "edges": [{"source": u, "target": v, "relation": data.get("relation", "unknown")} 
                                     for u, v, data in G.edges(data=True)]
                        }
                        rgcn_result = rgcn_manager.predict(graph_data_for_rgcn)
                        if rgcn_result:
                            rgcn_predictions = rgcn_result["predictions"]
                            print(f"âœ… R-GCN predictions: {len(rgcn_predictions)} nodes")
                    except Exception as e:
                        print(f"âŒ R-GCN prediction error: {e}")
                
                # ê·¸ë˜í”„ ë°ì´í„° êµ¬ì„± (TTL + OCR ì •ë³´ í¬í•¨)
                graph_data = {
                    "nodes": [{"id": node, **node_features[node]} for node in G.nodes()],
                    "edges": [{"source": u, "target": v, "relation": data.get("relation", "unknown")} 
                             for u, v, data in G.edges(data=True)],
                    "predictions": predictions,
                    "rgcn_predictions": rgcn_predictions,
                    "learning_results": learning_results,
                    "pdf_info": pdf_info,
                    "ocr_results": ocr_results,
                    "stats": {
                        "total_nodes": G.number_of_nodes(),
                        "total_edges": G.number_of_edges(),
                        "source": "pdf",
                        "filename": pdf_filename,
                        "text_length": len(pdf_text),
                        "processing_method": processing_method,
                        "pdf_type": pdf_info.get('pdf_type', 'unknown'),
                        "ocr_processed": processing_method in ['image', 'hybrid'],
                        "total_images": pdf_info.get('total_images', 0),
                        "patterns_found": len(learning_results.get("patterns", {})),
                        "entities_found": sum(len(v) for v in learning_results.get("new_entities", {}).values()),
                        "relations_found": len(learning_results.get("new_relations", [])),
                        "avg_confidence": sum(learning_results.get("confidence_scores", {}).values()) / 
                                        max(len(learning_results.get("confidence_scores", {})), 1),
                        "domain_insights": learning_results.get("domain_insights", {}),
                        "rgcn_available": rgcn_predictions is not None,
                        "ocr_elements": len(ocr_results),
                        "ttl_enhanced": ttl_enhanced,
                        "ttl_classifications": ttl_classifications,
                        "ontology_source": "TTL + Default" if ttl_enhanced else "Default only"
                    }
                }
                
                # query_processorì— ê·¸ë˜í”„ ë°ì´í„° ì „ë‹¬ (ë©”ëª¨ë¦¬ ì €ì¥)
                if query_processor:
                    query_processor._last_graph_data = graph_data
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ (TTL + OCR ì •ë³´ í¬í•¨)
                try:
                    # PDF ì •ë³´ë¥¼ DB ì €ì¥ìš©ìœ¼ë¡œ ë³€í™˜
                    pdf_info_for_db = {
                        'pdf_type': pdf_info.get('pdf_type', 'unknown'),
                        'has_images': pdf_info.get('has_images', False),
                        'ocr_processed': processing_method in ['image', 'hybrid'],
                        'image_count': pdf_info.get('total_images', 0),
                        'ttl_enhanced': ttl_enhanced,
                        'ttl_classifications': ttl_classifications
                    }
                    
                    doc_id = db_manager.save_processing_results(
                        pdf_filename, pdf_text, graph_data, learning_results, 
                        processing_time=time.time() - start_time,
                        pdf_info=pdf_info_for_db
                    )
                    if doc_id:
                        print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ - Document ID: {doc_id}")
                    else:
                        print("âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨")
                        doc_id = f"temp_{pdf_filename}_{int(time.time())}"
                        
                except Exception as e:
                    print(f"âŒ DB ì €ì¥ ì˜¤ë¥˜: {e}")
                    import traceback
                    traceback.print_exc()
                    doc_id = f"temp_{pdf_filename}_{int(time.time())}"
                
                print(f"âœ… PDF processed and saved: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
                
                # ìƒì„¸í•œ ì„±ê³µ ë©”ì‹œì§€ (TTL + OCR ì •ë³´ í¬í•¨)
                stats = graph_data["stats"]
                domain_insights = stats.get("domain_insights", {})
                
                # ì²˜ë¦¬ ë°©ë²•ì— ë”°ë¥¸ ë©”ì‹œì§€ customization
                processing_emoji = {
                    'text': 'ğŸ“',
                    'image': 'ğŸ–¼ï¸', 
                    'hybrid': 'ğŸ“ğŸ–¼ï¸',
                    'error': 'âŒ'
                }
                
                message = dbc.Alert([
                    html.H5(f"âœ… PDF '{pdf_filename}' processed with TTL + OCR + Ontology Learning!", className="alert-heading"),
                    html.P(f"{processing_emoji.get(processing_method, 'ğŸ“„')} Processing: {processing_method.upper()} | PDF Type: {stats['pdf_type'].upper()}", className="mb-2"),
                    dbc.Row([
                        dbc.Col([
                            html.P(f"ğŸ“Š Text: {stats['text_length']} characters", className="mb-1"),
                            html.P(f"ğŸ¯ Patterns: {stats['patterns_found']} types", className="mb-1"),
                            html.P(f"ğŸ·ï¸ Entities: {stats['entities_found']} found", className="mb-1"),
                            html.P(f"ğŸ–¼ï¸ Images: {stats['total_images']} processed", className="mb-1"),
                        ], width=6),
                        dbc.Col([
                            html.P(f"ğŸ”— Relations: {stats['relations_found']} identified", className="mb-1"),
                            html.P(f"ğŸ•¸ï¸ Graph: {stats['total_nodes']} nodes, {stats['total_edges']} edges", className="mb-1"),
                            html.P(f"ğŸ–ï¸ Confidence: {stats['avg_confidence']:.2f}", className="mb-1"),
                            html.P(f"ğŸ” OCR Elements: {stats['ocr_elements']} extracted", className="mb-1"),
                        ], width=6),
                    ]),
                    html.Hr(),
                    html.P([
                        "ğŸ§  Domain: ", 
                        html.Strong(domain_insights.get("industry_domain", "Unknown")),
                        f" | Complexity: {domain_insights.get('complexity_score', 0):.1f}/5.0",
                        f" | TTL: {'âœ…' if ttl_enhanced else 'âŒ'} ({ttl_classifications} classifications)",
                        f" | R-GCN: {'âœ…' if stats.get('rgcn_available') else 'âŒ'}",
                        f" | OCR: {'âœ…' if stats.get('ocr_processed') else 'âŒ'}",
                        f" | DB ID: {doc_id}" if doc_id else ""
                    ], className="mb-2"),
                    html.P("ğŸ’¾ Results saved to database! Try querying: 'database statistics' or 'show recent documents'", className="mb-0")
                ], color="success")
                
                return message, graph_data
                
            finally:
                if os.path.exists(tmp_file_path):
                    os.unlink(tmp_file_path)
        
        except Exception as e:
            print(f"âŒ PDF processing error: {e}")
            import traceback
            traceback.print_exc()
            return dbc.Alert(f"âŒ Error processing PDF: {str(e)}", color="danger"), {}
    
    return "", {}

@app.callback(
    Output("pie-chart", "figure"),
    [Input("update-btn", "n_clicks"), Input("rgcn-compare-btn", "n_clicks")],
    [State("data-store", "data")]
)
def update_pie_chart(update_clicks, compare_clicks, stored_data):
    """íŒŒì´ ì°¨íŠ¸ ì—…ë°ì´íŠ¸ (R-GCN ë¹„êµ ê¸°ëŠ¥ ì¶”ê°€)"""
    from dash import ctx
    
    print(f"ğŸ¯ Pie chart callback - update: {update_clicks}, compare: {compare_clicks}, data: {bool(stored_data)}")
    
    if not stored_data:
        return go.Figure().add_annotation(
            text="Load sample data or upload PDF, then click 'Update Charts'",
            showarrow=False, x=0.5, y=0.5,
            font=dict(size=16)
        )
    
    trigger_id = ctx.triggered[0]['prop_id'].split('.')[0] if ctx.triggered else None
    
    # R-GCN ë¹„êµ ëª¨ë“œ
    if trigger_id == "rgcn-compare-btn" and stored_data.get("rgcn_predictions"):
        rule_predictions = stored_data.get("predictions", [])
        rgcn_predictions = stored_data.get("rgcn_predictions", [])
        
        print("=== ë””ë²„ê¹… ì •ë³´ ===")
        print(f"rule_predictions: {rule_predictions}")
        print(f"rgcn_predictions: {rgcn_predictions}")
        print(f"rgcn_predictions íƒ€ì…: {type(rgcn_predictions)}")
        if rgcn_predictions:
            print(f"ì²« ë²ˆì§¸ í•­ëª©: {rgcn_predictions[0] if rgcn_predictions else 'None'}")
        print("==================")
        
        if len(rule_predictions) > 0 and len(rgcn_predictions) > 0:
            # í¬ê¸° ë§ì¶”ê¸°
            min_size = min(len(rule_predictions), len(rgcn_predictions))
            
            # Rule-based ê²°ê³¼ ì¹´ìš´íŠ¸ (í¬ê¸° ë§ì¶¤)
            rule_counts = Counter(rule_predictions[:min_size])
            
            # R-GCN ê²°ê³¼ì—ì„œ predicted_clusterë§Œ ì¶”ì¶œí•´ì„œ ì¹´ìš´íŠ¸ (í¬ê¸° ë§ì¶¤)
            rgcn_clusters = []
            for pred in rgcn_predictions[:min_size]:
                if isinstance(pred, dict):
                    rgcn_clusters.append(pred.get('predicted_cluster', 'unknown'))
                else:
                    # ë§Œì•½ predê°€ ë‹¨ìˆœ ê°’ì´ë¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    rgcn_clusters.append(pred)
            
            rgcn_counts = Counter(rgcn_clusters)
            
            print(f"Size adjustment: rule={len(rule_predictions)} -> {min_size}, rgcn={len(rgcn_predictions)} -> {min_size}")
            print(f"rule_counts: {rule_counts}")
            print(f"rgcn_counts: {rgcn_counts}")
            
            # ë¼ë²¨ ì„¤ì •
            labels = ["Class 0", "Entity", "Literal"]
            rule_values = [rule_counts.get(i, 0) for i in range(3)]
            
            # R-GCN ê°’ë“¤ì„ ë§¤í•‘ (importance -> ìˆ«ì ì¸ë±ìŠ¤)
            importance_to_class = {
                'high_importance': 0,      # Class 0ì— ë§¤í•‘
                'medium_importance': 1,    # Entityì— ë§¤í•‘
                'low_importance': 2,       # Literalì— ë§¤í•‘
                'unknown': 2,              # ì•Œ ìˆ˜ ì—†ëŠ” ê²ƒì€ Literalë¡œ
                0: 0,                      # ìˆ«ì ì¸ë±ìŠ¤ë„ ì§€ì›
                1: 1,
                2: 2
            }
            
            rgcn_values = [0, 0, 0]  # [Class 0, Entity, Literal]
            for cluster, count in rgcn_counts.items():
                class_idx = importance_to_class.get(cluster, 2)  # ê¸°ë³¸ê°’ì€ 2 (Literal)
                rgcn_values[class_idx] += count
            
            print(f"ìµœì¢… rule_values: {rule_values}")
            print(f"ìµœì¢… rgcn_values: {rgcn_values}")
            
            fig = go.Figure()
            
            # Rule-based ê²°ê³¼
            fig.add_trace(go.Bar(
                name="Rule-based",
                x=labels,
                y=rule_values,
                marker_color="#ff9999",
                text=rule_values,
                textposition="auto",
                hovertemplate="<b>%{x}</b><br>Rule-based: %{y}<extra></extra>"
            ))
            
            # R-GCN ê²°ê³¼
            fig.add_trace(go.Bar(
                name="R-GCN",
                x=labels,
                y=rgcn_values,
                marker_color="#66b3ff",
                text=rgcn_values,
                textposition="auto",
                hovertemplate="<b>%{x}</b><br>R-GCN: %{y}<extra></extra>"
            ))
            
            # ì •í™•ë„ ê³„ì‚° (ì˜µì…˜)
            total_nodes = min_size  # ì‹¤ì œ ë¹„êµí•œ ë…¸ë“œ ìˆ˜
            accuracy_info = ""
            if total_nodes > 0:
                # í´ë˜ìŠ¤ë³„ ì¼ì¹˜ë„ ê³„ì‚°
                matches = sum(1 for i in range(3) if rule_values[i] > 0 and rgcn_values[i] > 0)
                total_classes = sum(1 for i in range(3) if rule_values[i] > 0)
                if total_classes > 0:
                    class_agreement = matches / total_classes * 100
                    accuracy_info = f" | Class Agreement: {class_agreement:.1f}%"
            
            fig.update_layout(
                title=f"Rule-based vs R-GCN Predictions Comparison<br><sub>Compared nodes: {total_nodes} (Rule: {len(rule_predictions)}, R-GCN: {len(rgcn_predictions)}){accuracy_info}</sub>",
                barmode='group',
                height=400,
                showlegend=True,
                xaxis_title="Node Classes",
                yaxis_title="Count",
                hovermode='x unified'
            )
            
            print(f"âœ… R-GCN comparison chart created with {total_nodes} nodes")
            return fig
        else:
            print(f"âš ï¸ No data available: rule={len(rule_predictions)}, rgcn={len(rgcn_predictions)}")
            return go.Figure().add_annotation(
                text=f"No data available for comparison<br>Rule: {len(rule_predictions)}, R-GCN: {len(rgcn_predictions)}",
                showarrow=False, x=0.5, y=0.5,
                font=dict(size=14, color="orange")
            )
    
    # R-GCN ë¹„êµ ìš”ì²­í–ˆì§€ë§Œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
    elif trigger_id == "rgcn-compare-btn":
        return go.Figure().add_annotation(
            text="R-GCN predictions not available<br>Upload PDF or load sample data with R-GCN support",
            showarrow=False, x=0.5, y=0.5,
            font=dict(size=14, color="orange")
        )
    
    # ê¸°ë³¸ íŒŒì´ ì°¨íŠ¸ (ê¸°ì¡´ ë¡œì§ + R-GCN ì •ë³´)
    if (update_clicks or compare_clicks) and "predictions" in stored_data:
        predictions = stored_data["predictions"]
        pred_counts = Counter(predictions)
        
        labels = ["Class", "Entity", "Literal"]
        values = [pred_counts.get(i, 0) for i in range(3)]
        colors = ["#ff9999", "#66b3ff", "#99ff99"]
        
        fig = go.Figure()
        fig.add_trace(go.Pie(
            labels=labels,
            values=values,
            hole=0.3,
            textinfo="label+percent+value",
            marker=dict(colors=colors),
            hovertemplate="<b>%{label}</b><br>Count: %{value}<br>Percentage: %{percent}<extra></extra>"
        ))
        
        # R-GCN ì •ë³´ ë° ì¶”ê°€ í†µê³„
        rgcn_status = "âœ… Available" if stored_data.get("rgcn_predictions") else "âŒ Not Available"
        
        # ì¶”ê°€ í†µê³„ ì •ë³´
        stats = stored_data.get("stats", {})
        additional_info = ""
        if stats:
            source = stats.get("source", "unknown")
            if source == "pdf":
                filename = stats.get("filename", "unknown")
                additional_info = f" | Source: {filename}"
            elif source == "sample":
                additional_info = " | Source: Sample Data"
        
        title = f"Node Classification Results (Total: {sum(values)})<br><sub>R-GCN: {rgcn_status}{additional_info}</sub>"
        
        fig.update_layout(
            title=title,
            height=400,
            showlegend=True,
            hovermode='closest'
        )
        
        print(f"âœ… Pie chart created: {values}")
        return fig
    
    return go.Figure().add_annotation(
        text="Load sample data or upload PDF, then click 'Update Charts'",
        showarrow=False, x=0.5, y=0.5,
        font=dict(size=16)
    )


@app.callback(
    Output("network-graph", "figure"),
    [Input("update-btn", "n_clicks")],
    [State("data-store", "data")]
)
def update_network_graph(n_clicks, stored_data):
    print(f"ğŸ•¸ï¸ Network graph callback - clicks: {n_clicks}, data: {bool(stored_data)}")
    
    if n_clicks and stored_data and "nodes" in stored_data:
        nodes = stored_data["nodes"]
        edges = stored_data["edges"]
        
        # ë…¸ë“œ ìˆ˜ ì œí•œ (ì„±ëŠ¥ì„ ìœ„í•´)
        max_nodes = 50
        if len(nodes) > max_nodes:
            # ì‹ ë¢°ë„ê°€ ë†’ì€ ë…¸ë“œë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒ
            sorted_nodes = sorted(nodes, key=lambda x: x.get("confidence", 0), reverse=True)
            nodes = sorted_nodes[:max_nodes]
            node_ids = {node["id"] for node in nodes}
            # ì„ íƒëœ ë…¸ë“œë“¤ ì‚¬ì´ì˜ ì—£ì§€ë§Œ í•„í„°ë§
            edges = [edge for edge in edges 
                    if edge["source"] in node_ids and edge["target"] in node_ids]
            print(f"âš ï¸ Limited to top {max_nodes} nodes by confidence")
        
        G = nx.DiGraph()
        
        # ë…¸ë“œ ì¶”ê°€
        for node in nodes:
            G.add_node(node["id"], **node)
        
        # ì—£ì§€ ì¶”ê°€ (ê´€ê³„ ì •ë³´ í¬í•¨)
        for edge in edges:
            if edge["source"] in G.nodes() and edge["target"] in G.nodes():
                G.add_edge(edge["source"], edge["target"], 
                          relation=edge.get("relation", "unknown"))
        
        # ë ˆì´ì•„ì›ƒ ê³„ì‚° (ë” ë‚˜ì€ ì‹œê°í™”ë¥¼ ìœ„í•´)
        try:
            if G.number_of_nodes() > 20:
                pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
            else:
                pos = nx.spring_layout(G, k=5, iterations=100, seed=42)
        except:
            # fallback
            pos = {node: (i % 5, i // 5) for i, node in enumerate(G.nodes())}
        
        # ì—£ì§€ ê·¸ë¦¬ê¸°
        edge_x, edge_y = [], []
        edge_info = []
        
        for edge in G.edges(data=True):
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
            
            # ì—£ì§€ ì •ë³´ ì €ì¥
            relation = edge[2].get("relation", "unknown")
            edge_info.append(f"{edge[0]} â†’ {edge[1]} ({relation})")
        
        # ë…¸ë“œ ìœ„ì¹˜ ë° ì •ë³´
        node_x = [pos[node][0] for node in G.nodes()]
        node_y = [pos[node][1] for node in G.nodes()]
        node_text = []
        
        # ë…¸ë“œ ìƒ‰ìƒ, í¬ê¸°, í˜¸ë²„ í…ìŠ¤íŠ¸
        node_colors = []
        node_sizes = []
        hover_text = []
        
        for node in G.nodes():
            node_info = next((n for n in nodes if n["id"] == node), {})
            confidence = node_info.get("confidence", 0.5)
            node_type = node_info.get("type", "unknown")
            ontology_class = node_info.get("ontology_class", "Unknown")
            
            # ë…¸ë“œ í…ìŠ¤íŠ¸ (ê¸¸ì´ ì œí•œ)
            display_text = node[:12] + "..." if len(node) > 12 else node
            node_text.append(display_text)
            
            # ì‹ ë¢°ë„ì— ë”°ë¥¸ í¬ê¸° (ë” ëª…í™•í•œ ì°¨ì´)
            size = 10 + confidence * 30
            node_sizes.append(size)
            
            # ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ì— ë”°ë¥¸ ìƒ‰ìƒ (ë” ì„¸ë¶„í™”)
            color_map = {
                "Equipment": [70, 130, 180],      # ìŠ¤í‹¸ë¸”ë£¨
                "ProcessRequirement": [255, 165, 0],  # ì˜¤ë Œì§€
                "Field": [50, 205, 50],           # ë¼ì„ê·¸ë¦°
                "Note": [218, 112, 214],          # ì˜¤í‚¤ë“œ
                "Project": [255, 69, 0],          # ë ˆë“œì˜¤ë Œì§€
                "Person": [138, 43, 226],         # ë¸”ë£¨ë°”ì´ì˜¬ë ›
                "Literal": [60, 179, 113],        # ë¯¸ë””ì—„ì”¨ê·¸ë¦°
                "Entity": [70, 130, 180],         # ìŠ¤í‹¸ë¸”ë£¨ (ê¸°ë³¸ ì—”í‹°í‹°)
                "Unknown": [169, 169, 169]        # ë‹¤í¬ê·¸ë ˆì´
            }
            
            # íƒ€ì… ìš°ì„ , ì—†ìœ¼ë©´ ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ ì‚¬ìš©
            color_key = ontology_class if ontology_class != "Unknown" else node_type
            base_color = color_map.get(color_key, color_map["Unknown"])
            
            # ì‹ ë¢°ë„ì— ë”°ë¥¸ íˆ¬ëª…ë„ ë° ì±„ë„
            alpha = 0.5 + confidence * 0.4
            color = f"rgba({base_color[0]}, {base_color[1]}, {base_color[2]}, {alpha})"
            node_colors.append(color)
            
            # ì—°ê²°ëœ ë…¸ë“œ ìˆ˜ ê³„ì‚°
            degree = G.degree(node)
            
            # í˜¸ë²„ í…ìŠ¤íŠ¸ (ë” ìƒì„¸í•œ ì •ë³´)
            hover_text.append(
                f"<b>{node}</b><br>" +
                f"Type: {node_type}<br>" +
                f"Class: {ontology_class}<br>" +
                f"Confidence: {confidence:.3f}<br>" +
                f"Connections: {degree}<br>" +
                f"Size: {size:.1f}"
            )
        
        fig = go.Figure()
        
        # ì—£ì§€ ì¶”ê°€ (ê´€ê³„ë³„ ìƒ‰ìƒ êµ¬ë¶„)
        fig.add_trace(go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=1.5, color='rgba(125,125,125,0.3)'),
            mode='lines',
            showlegend=False,
            hoverinfo='none',
            name='Relationships'
        ))
        
        # ë…¸ë“œ ì¶”ê°€ (í–¥ìƒëœ ì‹œê°í™”)
        fig.add_trace(go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            text=node_text,
            textposition="middle center",
            textfont=dict(size=7, color="white", family="Arial Black"),
            marker=dict(
                size=node_sizes,
                color=node_colors,
                line=dict(width=2, color='rgba(50,50,50,0.8)'),
                sizemode='diameter'
            ),
            showlegend=False,
            hoverinfo='text',
            hovertext=hover_text,
            name='Nodes'
        ))
        
        # í†µê³„ ì •ë³´
        stats = stored_data.get("stats", {})
        source_info = ""
        if stats:
            source = stats.get("source", "unknown")
            if source == "pdf":
                filename = stats.get("filename", "unknown")
                source_info = f"Source: {filename[:20]}..."
            elif source == "sample":
                source_info = "Source: Sample Data"
        
        # R-GCN ì •ë³´
        rgcn_info = ""
        if stored_data.get("rgcn_predictions"):
            rgcn_info = " | R-GCN: âœ…"
        
        # ë²”ë¡€ ì •ë³´
        legend_info = "Color by Ontology Class | Size by Confidence"
        
        title_text = (
            f"Knowledge Graph Network ({G.number_of_nodes()} nodes, {G.number_of_edges()} edges)<br>"
            f"<sub>{legend_info}{rgcn_info}<br>{source_info}</sub>"
        )
        
        fig.update_layout(
            title=title_text,
            height=500,
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            showlegend=False,
            plot_bgcolor='rgba(248,249,250,0.8)',
            paper_bgcolor='white',
            margin=dict(l=20, r=20, t=100, b=20),
            hovermode='closest',
            font=dict(size=12)
        )
        
        print(f"âœ… Enhanced network graph created: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
        return fig
    
    return go.Figure().add_annotation(
        text="Load sample data or upload PDF, then click 'Update Charts'<br><sub>Network visualization will show node relationships</sub>",
        showarrow=False, x=0.5, y=0.5,
        font=dict(size=16)
    )

@app.callback(
    Output("learning-stats", "children"),
    [Input("data-store", "data")]
)
def update_learning_stats(stored_data):
    """ì˜¨í†¨ë¡œì§€ í•™ìŠµ í†µê³„ í‘œì‹œ (í–¥ìƒëœ ë²„ì „)"""
    if not stored_data or "learning_results" not in stored_data:
        return ""
    
    learning_results = stored_data["learning_results"]
    stats = stored_data.get("stats", {})
    
    confidence_scores = learning_results.get("confidence_scores", {})
    domain_insights = learning_results.get("domain_insights", {})
    patterns = learning_results.get("patterns", {})
    new_entities = learning_results.get("new_entities", {})
    new_relations = learning_results.get("new_relations", [])
    
    if not confidence_scores and not domain_insights:
        return ""
    
    # ì‹ ë¢°ë„ ë¶„í¬ ê³„ì‚°
    high_conf = sum(1 for c in confidence_scores.values() if c > 0.8)
    med_conf = sum(1 for c in confidence_scores.values() if 0.5 < c <= 0.8)
    low_conf = sum(1 for c in confidence_scores.values() if c <= 0.5)
    total_items = len(confidence_scores)
    
    # í‰ê·  ì‹ ë¢°ë„
    avg_confidence = sum(confidence_scores.values()) / max(len(confidence_scores), 1)
    
    # ë„ë©”ì¸ í†µì°° ì •ë³´
    industry = domain_insights.get('industry_domain', 'Unknown')
    complexity = domain_insights.get('complexity_score', 0)
    technical_density = domain_insights.get('technical_density', 0)
    document_type = domain_insights.get('document_type', 'Unknown')
    
    # í•™ìŠµëœ íŒ¨í„´ ìˆ˜
    pattern_count = len(patterns)
    entity_count = sum(len(v) for v in new_entities.values()) if new_entities else 0
    relation_count = len(new_relations)
    
    # ì²˜ë¦¬ ì •ë³´
    source = stats.get("source", "unknown")
    processing_method = stats.get("processing_method", "unknown")
    
    # ìƒ‰ìƒ ë° ì§„í–‰ë¥  ê³„ì‚°
    if total_items > 0:
        high_pct = (high_conf / total_items) * 100
        med_pct = (med_conf / total_items) * 100
        low_pct = (low_conf / total_items) * 100
    else:
        high_pct = med_pct = low_pct = 0
    
    # ë³µì¡ë„ì— ë”°ë¥¸ ìƒ‰ìƒ
    complexity_color = "success" if complexity >= 3.5 else "warning" if complexity >= 2.0 else "info"
    
    # ì‹ ë¢°ë„ì— ë”°ë¥¸ ì „ì²´ í‰ê°€
    if avg_confidence > 0.8:
        overall_badge = dbc.Badge("Excellent", color="success", className="me-1")
    elif avg_confidence > 0.6:
        overall_badge = dbc.Badge("Good", color="primary", className="me-1")
    elif avg_confidence > 0.4:
        overall_badge = dbc.Badge("Fair", color="warning", className="me-1")
    else:
        overall_badge = dbc.Badge("Needs Review", color="danger", className="me-1")
    
    return dbc.Card([
        dbc.CardBody([
            # í—¤ë”
            html.Div([
                html.H6("ğŸ§  Learning Analytics", className="card-title d-inline"),
                overall_badge,
                dbc.Badge(f"Avg: {avg_confidence:.3f}", color="secondary", className="float-end")
            ], className="mb-3"),
            
            # ì‹ ë¢°ë„ ë¶„í¬ (í”„ë¡œê·¸ë ˆìŠ¤ ë°” í¬í•¨)
            html.Div([
                html.P("ğŸ“Š Confidence Distribution:", className="mb-2 fw-bold"),
                html.Div([
                    html.Small(f"ğŸ–ï¸ High ({high_conf})", className="text-success"),
                    dbc.Progress(value=high_pct, color="success", className="mb-1", style={"height": "8px"})
                ]),
                html.Div([
                    html.Small(f"ğŸƒ Medium ({med_conf})", className="text-warning"),
                    dbc.Progress(value=med_pct, color="warning", className="mb-1", style={"height": "8px"})
                ]),
                html.Div([
                    html.Small(f"ğŸ¤” Low ({low_conf})", className="text-danger"),
                    dbc.Progress(value=low_pct, color="danger", className="mb-2", style={"height": "8px"})
                ]),
            ], className="mb-3"),
            
            # í•™ìŠµ ì„±ê³¼
            html.Div([
                html.P("ğŸ¯ Learning Results:", className="mb-2 fw-bold"),
                dbc.Row([
                    dbc.Col([
                        html.P(f"ğŸ“‹ Patterns: {pattern_count}", className="mb-1 small"),
                        html.P(f"ğŸ·ï¸ Entities: {entity_count}", className="mb-1 small"),
                    ], width=6),
                    dbc.Col([
                        html.P(f"ğŸ”— Relations: {relation_count}", className="mb-1 small"),
                        html.P(f"ğŸ“„ Total Items: {total_items}", className="mb-1 small"),
                    ], width=6),
                ])
            ], className="mb-3"),
            
            # ë„ë©”ì¸ ë¶„ì„
            html.Div([
                html.P("ğŸ”¬ Domain Analysis:", className="mb-2 fw-bold"),
                html.P([
                    "ğŸ­ Industry: ", 
                    dbc.Badge(industry, color="info", className="me-2"),
                    "ğŸ“ˆ Complexity: ",
                    dbc.Badge(f"{complexity:.1f}/5.0", color=complexity_color)
                ], className="mb-1 small"),
                html.P(f"ğŸ“Š Technical Density: {technical_density:.3f}", className="mb-1 small"),
                html.P(f"ğŸ“‹ Document Type: {document_type}", className="mb-1 small"),
            ], className="mb-3"),
            
            # ì²˜ë¦¬ ì •ë³´
            html.Div([
                html.P("âš™ï¸ Processing Info:", className="mb-2 fw-bold"),
                html.P([
                    f"ğŸ“‚ Source: {source.upper()}"
                ], className="mb-1 small"),
                html.P([
                    f"ğŸ”§ Method: {processing_method.upper()}" if processing_method != "unknown" else "ğŸ”§ Method: Standard"
                ], className="mb-1 small"),
                
                # R-GCN ìƒíƒœ
                html.P([
                    "ğŸ¤– R-GCN: ",
                    dbc.Badge("Available", color="success") if stored_data.get("rgcn_predictions") 
                    else dbc.Badge("Not Available", color="secondary")
                ], className="mb-0 small"),
            ])
        ])
    ], className="border-info")


@app.callback(
    Output("query-output", "children"),
    [Input("query-btn", "n_clicks")],
    [State("nl-query-input", "value"), State("data-store", "data")]
)
def handle_natural_language_query(n_clicks, query_text, stored_data):
    """ìì—°ì–´ ì¿¼ë¦¬ ì²˜ë¦¬ (ë°ì´í„°ë² ì´ìŠ¤ í†µí•© + í–¥ìƒëœ ê¸°ëŠ¥)"""
    if not n_clicks or not query_text:
        return html.Div([
            html.P("ğŸ’¡ Enhanced Query Examples:", className="mb-2 fw-bold"),
            dbc.Row([
                dbc.Col([
                    html.H6("ğŸ“Š Database Queries:", className="text-primary"),
                    html.Ul([
                        html.Li("database statistics"),
                        html.Li("show recent documents"),
                        html.Li("search for temperature"),
                        html.Li("search for npsha")
                    ], className="small")
                ], width=6),
                dbc.Col([
                    html.H6("ğŸ•¸ï¸ Graph Queries:", className="text-success"),
                    html.Ul([
                        html.Li("show all entities with high confidence"),
                        html.Li("count all notes"),
                        html.Li("show relationships"),
                        html.Li("field pumping temperature")
                    ], className="small")
                ], width=6)
            ]),
            dbc.Row([
                dbc.Col([
                    html.H6("ğŸ¤– Advanced Queries:", className="text-warning"),
                    html.Ul([
                        html.Li("r-gcn predict"),
                        html.Li("show all equipment"),
                        html.Li("note 1"),
                        html.Li("startup conditions")
                    ], className="small")
                ], width=6),
                dbc.Col([
                    html.H6("ğŸ” Search Tips:", className="text-info"),
                    html.Ul([
                        html.Li("Use specific terms like 'pump', 'pressure'"),
                        html.Li("Try 'high confidence' for quality filters"),
                        html.Li("Search by ontology classes"),
                        html.Li("Combine keywords for better results")
                    ], className="small")
                ], width=6)
            ])
        ])
    
    print(f"ğŸ—£ï¸ Natural language query: '{query_text}'")
    
    try:
        # ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
        import time
        start_time = time.time()
        
        # ì¿¼ë¦¬ ì²˜ë¦¬ (ë°ì´í„°ë² ì´ìŠ¤ ìš°ì„ )
        result = query_processor.process_query(query_text, stored_data)
        
        processing_time = time.time() - start_time
        result["processing_time"] = processing_time
        
        # ê²°ê³¼ ë Œë”ë§
        return render_query_result(result, query_text)
        
    except Exception as e:
        print(f"âŒ Query processing error: {e}")
        import traceback
        traceback.print_exc()
        return dbc.Alert([
            html.H5("âŒ Query Processing Error", className="alert-heading"),
            html.P(f"Error: {str(e)}"),
            html.Hr(),
            html.P("Try a simpler query or check the examples above.", className="mb-0")
        ], color="danger")

def render_query_result(result, original_query=""):
    """ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ë Œë”ë§ (í–¥ìƒëœ ë²„ì „)"""
    result_type = result.get("type", "error")
    message = result.get("message", "")
    data = result.get("data", [])
    processing_time = result.get("processing_time", 0)
    
    # í—¤ë” ì»´í¬ë„ŒíŠ¸
    header_components = []
    
    if result_type != "error":
        header_components.append(
            dbc.Alert([
                html.Div([
                    html.Strong("âœ… Query Result: "),
                    message,
                    html.Span(f" ({processing_time:.3f}s)", className="text-muted float-end")
                ])
            ], color="success", className="mb-3")
        )
    
    components = header_components.copy()
    
    if result_type == "table" and data:
        # í…Œì´ë¸” í˜•íƒœë¡œ ê²°ê³¼ í‘œì‹œ (í–¥ìƒëœ ë²„ì „)
        columns = result.get("columns", [])
        if not columns and data:
            columns = list(data[0].keys()) if data else []
        
        # ë°ì´í„° ì •ë¦¬ ë° í¬ë§·íŒ…
        clean_data = []
        for i, item in enumerate(data[:50]):  # ìµœëŒ€ 50ê°œ í•­ëª©ìœ¼ë¡œ ì œí•œ
            clean_item = {"#": i + 1}  # í–‰ ë²ˆí˜¸ ì¶”ê°€
            for key, value in item.items():
                if isinstance(value, float):
                    clean_item[key] = round(value, 3)
                elif isinstance(value, str) and len(value) > 60:
                    clean_item[key] = value[:57] + "..."
                else:
                    clean_item[key] = str(value) if value is not None else "N/A"
            clean_data.append(clean_item)
        
        if clean_data:
            df = pd.DataFrame(clean_data)
            
            # í…Œì´ë¸” ìŠ¤íƒ€ì¼ë§
            table = dbc.Table.from_dataframe(
                df, 
                striped=True, 
                bordered=True, 
                hover=True, 
                responsive=True,
                size="sm",
                className="table-hover"
            )
            
            # í…Œì´ë¸”ì„ ì¹´ë“œë¡œ ê°ì‹¸ê¸°
            components.append(
                dbc.Card([
                    dbc.CardHeader([
                        html.H6(f"ğŸ“‹ Results Table ({len(clean_data)} of {len(data)} items)", className="mb-0")
                    ]),
                    dbc.CardBody([
                        table
                    ], className="p-2")
                ], className="mb-3")
            )
            
            # ë” ë§ì€ í•­ëª©ì´ ìˆì„ ë•Œ ì•Œë¦¼
            if len(data) > 50:
                components.append(
                    dbc.Alert([
                        html.I(className="fas fa-info-circle me-2"),
                        f"Showing first 50 of {len(data)} results. Refine your query for more specific results."
                    ], color="info", className="mb-3")
                )
    
    elif result_type == "stat" and data:
        # í†µê³„ í˜•íƒœë¡œ ê²°ê³¼ í‘œì‹œ (í–¥ìƒëœ ë²„ì „)
        stat_cards = []
        for key, value in data.items():
            # ê°’ í¬ë§·íŒ…
            if isinstance(value, float):
                display_value = f"{value:.3f}" if value < 100 else f"{value:.1f}"
            else:
                display_value = str(value)
            
            # ì•„ì´ì½˜ ë§¤í•‘
            icon_map = {
                "total": "ğŸ“Š",
                "count": "ğŸ”¢",
                "confidence": "ğŸ–ï¸",
                "average": "ğŸ“ˆ",
                "high": "â¬†ï¸",
                "low": "â¬‡ï¸",
                "nodes": "ğŸ•¸ï¸",
                "edges": "ğŸ”—",
                "documents": "ğŸ“„",
                "patterns": "ğŸ¯"
            }
            
            icon = "ğŸ“Š"  # ê¸°ë³¸ ì•„ì´ì½˜
            for keyword, emoji in icon_map.items():
                if keyword.lower() in key.lower():
                    icon = emoji
                    break
            
            stat_cards.append(
                dbc.Col([
                    dbc.Card([
                        dbc.CardBody([
                            html.H4([icon, " ", display_value], className="text-primary mb-1"),
                            html.P(key, className="mb-0 small text-muted")
                        ], className="text-center py-2")
                    ], className="h-100 shadow-sm")
                ], width=12, md=6, lg=3, className="mb-2")
            )
        
        # í†µê³„ ì¹´ë“œë“¤ì„ ì—¬ëŸ¬ í–‰ìœ¼ë¡œ ë°°ì¹˜
        rows = []
        for i in range(0, len(stat_cards), 4):
            rows.append(dbc.Row(stat_cards[i:i+4], className="mb-2"))
        
        components.extend(rows)
    
    elif result_type == "suggestions" and data:
        # ì œì•ˆì‚¬í•­ í‘œì‹œ (í–¥ìƒëœ ë²„ì „)
        components.append(
            dbc.Card([
                dbc.CardHeader([
                    html.H6("ğŸ’¡ Query Suggestions", className="mb-0 text-primary")
                ]),
                dbc.CardBody([
                    dbc.ListGroup([
                        dbc.ListGroupItem([
                            html.I(className="fas fa-lightbulb me-2 text-warning"),
                            suggestion
                        ], action=True, className="border-0") 
                        for suggestion in data
                    ], flush=True)
                ])
            ], className="mb-3")
        )
    
    elif result_type == "error":
        components = [
            dbc.Alert([
                html.H5("âŒ Query Error", className="alert-heading"),
                html.P(message),
                html.Hr(),
                html.P([
                    "Try rephrasing your query or check the examples. ",
                    html.A("View examples", href="#", className="alert-link")
                ], className="mb-0")
            ], color="danger")
        ]
    
    else:
        components.append(
            dbc.Alert([
                html.I(className="fas fa-search me-2"),
                "No results found for your query. Try different keywords or check the examples above."
            ], color="warning")
        )
    
    # ì¿¼ë¦¬ ê¸°ë¡ (ì˜µì…˜)
    if original_query and result_type != "error":
        components.append(
            html.Div([
                html.Hr(),
                html.Small([
                    "Query: ",
                    html.Code(original_query, className="bg-light px-2 py-1 rounded"),
                    f" | Processed in {processing_time:.3f}s"
                ], className="text-muted")
            ])
        )
    
    return html.Div(components)

@app.callback(
    Output("rgcn-status", "children"),
    [Input("rgcn-predict-btn", "n_clicks")],
    [State("data-store", "data")]
)
def update_rgcn_status(n_clicks, stored_data):
    """R-GCN ìƒíƒœ ì—…ë°ì´íŠ¸ ë° ì˜ˆì¸¡ ìˆ˜í–‰ (í–¥ìƒëœ ë²„ì „)"""
    if not rgcn_manager:
        return dbc.Alert([
            html.I(className="fas fa-exclamation-triangle me-2"),
            "R-GCN not available - Check system configuration"
        ], color="warning", className="p-2 small")
    
    # ê¸°ë³¸ ìƒíƒœ í‘œì‹œ (ë²„íŠ¼ì„ ëˆ„ë¥´ì§€ ì•Šì•˜ì„ ë•Œ)
    if not n_clicks:
        try:
            status = rgcn_manager.get_status()
            
            # ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ ë° ì•„ì´ì½˜
            ready_status = status.get('initialized', False)
            status_color = "success" if ready_status else "warning"
            status_icon = "âœ…" if ready_status else "âš ï¸"
            
            device_info = status.get('device', 'unknown')
            device_icon = "ğŸš€" if "cuda" in device_info.lower() else "ğŸ’»"
            
            return dbc.Card([
                dbc.CardBody([
                    html.Div([
                        html.P([
                            "ğŸ¤– R-GCN Status: ",
                            dbc.Badge(f"{status_icon} {'Ready' if ready_status else 'Not Ready'}", 
                                    color=status_color, className="ms-1")
                        ], className="mb-2 small"),
                        
                        html.P(f"ğŸ”— Relations: {status.get('num_relations', 0)}", className="mb-2 small"),
                        
                        html.P([
                            f"{device_icon} Device: ",
                            dbc.Badge(device_info.upper(), color="info", className="ms-1")
                        ], className="mb-0 small")
                    ])
                ], className="p-2")
            ], className="border-primary", style={"fontSize": "0.85rem"})
            
        except Exception as e:
            return dbc.Alert([
                html.I(className="fas fa-exclamation-circle me-2"),
                f"Status check failed: {str(e)}"
            ], color="danger", className="p-2 small")
    
    # R-GCN ì˜ˆì¸¡ ìˆ˜í–‰ (ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œ)
    if n_clicks and stored_data:
        # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
        if not stored_data.get("nodes") or not stored_data.get("edges"):
            return dbc.Alert([
                html.I(className="fas fa-database me-2"),
                "No graph data available - Upload PDF or load sample data first"
            ], color="info", className="p-2 small")
        
        try:
            # ì˜ˆì¸¡ ì‹œê°„ ì¸¡ì •
            import time
            start_time = time.time()
            
            # R-GCN ì˜ˆì¸¡ ì‹¤í–‰
            rgcn_result = rgcn_manager.predict(stored_data)
            
            prediction_time = time.time() - start_time
            
            if rgcn_result:
                predictions = rgcn_result["predictions"]
                avg_confidence = rgcn_result["average_confidence"]
                class_dist = rgcn_result["class_distribution"]
                
                # í´ë˜ìŠ¤ ë¶„í¬ ì •ë¦¬
                class_names = {0: "Class 0", 1: "Entity", 2: "Literal", 3: "Type"}
                formatted_dist = {}
                for class_id, count in class_dist.items():
                    if count > 0:
                        class_name = class_names.get(class_id, f"Class {class_id}")
                        formatted_dist[class_name] = count
                
                # ì„±ëŠ¥ í‰ê°€
                confidence_level = "High" if avg_confidence > 0.8 else "Medium" if avg_confidence > 0.6 else "Low"
                confidence_color = "success" if avg_confidence > 0.8 else "warning" if avg_confidence > 0.6 else "danger"
                
                return dbc.Alert([
                    html.Div([
                        html.H6([
                            html.I(className="fas fa-check-circle me-2"),
                            "R-GCN Prediction Complete"
                        ], className="alert-heading mb-3"),
                        
                        dbc.Row([
                            dbc.Col([
                                html.P([
                                    "ğŸ“Š Nodes Processed: ",
                                    html.Strong(len(predictions))
                                ], className="mb-1 small"),
                                html.P([
                                    "â±ï¸ Processing Time: ",
                                    html.Strong(f"{prediction_time:.3f}s")
                                ], className="mb-1 small"),
                            ], width=6),
                            dbc.Col([
                                html.P([
                                    "ğŸ–ï¸ Avg Confidence: ",
                                    dbc.Badge(f"{avg_confidence:.3f} ({confidence_level})", 
                                            color=confidence_color)
                                ], className="mb-1 small"),
                                html.P([
                                    "ğŸ“ˆ Classes Found: ",
                                    html.Strong(len(formatted_dist))
                                ], className="mb-1 small"),
                            ], width=6)
                        ]),
                        
                        # í´ë˜ìŠ¤ ë¶„í¬ í‘œì‹œ
                        html.Div([
                            html.P("ğŸ“Š Class Distribution:", className="mb-2 small fw-bold"),
                            html.Div([
                                dbc.Badge(f"{class_name}: {count}", color="secondary", className="me-1 mb-1")
                                for class_name, count in formatted_dist.items()
                            ])
                        ], className="mt-2"),
                        
                        html.Hr(className="my-2"),
                        html.P([
                            html.I(className="fas fa-info-circle me-1"),
                            "Use 'R-GCN vs Rule' button to compare with rule-based results"
                        ], className="mb-0 small text-muted")
                    ])
                ], color="success", className="p-3")
                
            else:
                return dbc.Alert([
                    html.I(className="fas fa-times-circle me-2"),
                    "R-GCN prediction failed - Check model configuration"
                ], color="danger", className="p-2 small")
                
        except Exception as e:
            print(f"âŒ R-GCN prediction error: {e}")
            import traceback
            traceback.print_exc()
            
            return dbc.Alert([
                html.Div([
                    html.H6([
                        html.I(className="fas fa-exclamation-triangle me-2"),
                        "R-GCN Prediction Error"
                    ], className="alert-heading mb-2"),
                    html.P(f"Error: {str(e)}", className="mb-1 small"),
                    html.P("Check console for detailed error information", className="mb-0 small text-muted")
                ])
            ], color="danger", className="p-3")
    
    # ë°ì´í„° ì—†ì´ ë²„íŠ¼ë§Œ ëˆŒë €ì„ ë•Œ
    elif n_clicks:
        return dbc.Alert([
            html.I(className="fas fa-upload me-2"),
            "No data available - Please upload a PDF or load sample data first"
        ], color="info", className="p-2 small")
    
    # ê¸°ë³¸ ìƒíƒœ (ì˜ˆìƒì¹˜ ëª»í•œ ê²½ìš°)
    return dbc.Alert([
        html.I(className="fas fa-question-circle me-2"),
        "R-GCN ready - Click 'R-GCN Predict' to run prediction"
    ], color="secondary", className="p-2 small")


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ Starting Advanced Industrial Knowledge Graph Analyzer...")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬ ë° ì¶œë ¥
    print("ğŸ“‹ System Status Check:")
    print("   ğŸ§  Enhanced ontology learning: âœ… Enabled")
    print("   ğŸ’¾ Database system: âœ… Integrated")
    print("   ğŸ” Natural language queries: âœ… Supported")
    print("   ğŸ¯ Advanced pattern recognition: âœ… Active")
    
    # R-GCN ìƒíƒœ í™•ì¸
    if rgcn_manager:
        try:
            status = rgcn_manager.get_status()
            print(f"   ğŸ¤– R-GCN Model: âœ… Ready ({status.get('device', 'unknown')})")
            print(f"   ğŸ”— Relation Types: {status.get('num_relations', 0)} configured")
        except:
            print("   ğŸ¤– R-GCN Model: âš ï¸ Available but status check failed")
    else:
        print("   ğŸ¤– R-GCN Model: âŒ Not available")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
    try:
        db_stats = db_manager.get_database_stats()
        print(f"   ğŸ“Š Database: âœ… Connected ({db_stats.get('total_documents', 0)} documents)")
    except:
        print("   ğŸ“Š Database: âš ï¸ Connected but status check failed")
    
    print("=" * 60)
    print("ğŸ‰ Features Available:")
    print("   ğŸ“„ PDF Upload & OCR Processing")
    print("   ğŸ•¸ï¸ Knowledge Graph Visualization")
    print("   ğŸ—£ï¸ Natural Language Queries")
    print("   ğŸ¤– R-GCN Neural Network Predictions")
    print("   ğŸ“Š Rule-based vs Neural Comparison")
    print("   ğŸ’¾ Persistent Database Storage")
    
    print("=" * 60)
    print("ğŸ’¡ Quick Start Guide:")
    print("   1. Upload a PDF or click 'Load Sample Data'")
    print("   2. Click 'Update Charts' to see visualizations")
    print("   3. Try natural language queries like:")
    print("      â€¢ 'database statistics'")
    print("      â€¢ 'show all equipment with high confidence'")
    print("      â€¢ 'search for temperature'")
    print("   4. Use 'R-GCN Predict' for neural network analysis")
    
    print("=" * 60)
    print("ğŸŒ Server Starting...")
    print("ğŸ”— URL: http://127.0.0.1:8050")
    print("ğŸ“± Mobile friendly interface available")
    print("ğŸ”„ Hot reload enabled (debug mode)")
    print("=" * 60)
    
    try:
        app.run(debug=True, port=8050, host='127.0.0.1')
    except KeyboardInterrupt:
        print("\n" + "=" * 60)
        print("ğŸ›‘ Server stopped by user")
        print("ğŸ’¾ All data has been saved to database")
        print("ğŸ‘‹ Thank you for using the Knowledge Graph Analyzer!")
        print("=" * 60)
    except Exception as e:
        print(f"\nâŒ Server error: {e}")
        print("ğŸ”§ Please check the configuration and try again")