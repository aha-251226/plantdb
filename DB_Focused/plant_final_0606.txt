import os
import base64
import io
import json
import pandas as pd
import networkx as nx
import plotly.graph_objects as go
from dash import Dash, dcc, html, Input, Output, State
import dash_bootstrap_components as dbc
from collections import Counter
import tempfile
import re
import sqlite3
import time  # ← 이 줄 추가
from datetime import datetime
from rgcn import RGCNManager
import cv2
import numpy as np
from paddleocr import PaddleOCR
from PIL import Image
import fitz  # PyMuPDF (PDF를 이미지로 변환용)
from rdflib import Graph, Namespace, RDF, RDFS, OWL

# R-GCN 모듈 임포트 (강제 활성화)
try:
    from rgcn import RGCNManager
    RGCN_AVAILABLE = True
    print("✅ R-GCN module loaded successfully")
    
except ImportError as e:
    print(f"❌ R-GCN import 실패: {e}")
    RGCN_AVAILABLE = False
    RGCNManager = None
except Exception as e:
    print(f"❌ R-GCN 모듈 오류: {e}")
    RGCN_AVAILABLE = False
    RGCNManager = None

# PDF 처리 라이브러리 (선택적)
try:
    from pdfminer.high_level import extract_text
    PDF_AVAILABLE = True
except ImportError:
    print("PDFMiner not available - using mock text extraction")
    PDF_AVAILABLE = False

# DuckDB 지원 (선택적)
try:
    import duckdb
    DUCKDB_AVAILABLE = True
except ImportError:
    print("DuckDB not available - using SQLite instead")
    DUCKDB_AVAILABLE = False

# OCR 관련 설정
OCR_AVAILABLE = False  # 안전하게 비활성화
ocr_reader = None
PDF_IMAGE_DPI = 300
OCR_CONFIDENCE_THRESHOLD = 0.5


# 데이터베이스 관리 클래스 (수정된 버전)
class DatabaseManager:
    def __init__(self, db_path="knowledge_graph.db"):
        self.db_path = db_path
        self.use_duckdb = DUCKDB_AVAILABLE
        self.init_database()
    
    def get_connection(self):
        """데이터베이스 연결 반환"""
        if self.use_duckdb:
            return duckdb.connect(self.db_path.replace('.db', '.duckdb'))
        else:
            return sqlite3.connect(self.db_path)
    
    def init_database(self):
        """데이터베이스 테이블 초기화"""
        conn = None
        try:
            conn = self.get_connection()
            
            # 문서 테이블 (OCR 관련 필드 추가)
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS documents (
                        id INTEGER PRIMARY KEY,
                        filename TEXT NOT NULL,
                        upload_time TEXT NOT NULL,
                        content_length INTEGER,
                        processing_time REAL,
                        text_preview TEXT,
                        pdf_type TEXT DEFAULT 'text',
                        has_images BOOLEAN DEFAULT FALSE,
                        ocr_processed BOOLEAN DEFAULT FALSE,
                        image_count INTEGER DEFAULT 0
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS documents (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        filename TEXT NOT NULL,
                        upload_time TEXT NOT NULL,
                        content_length INTEGER,
                        processing_time REAL,
                        text_preview TEXT,
                        pdf_type TEXT DEFAULT 'text',
                        has_images BOOLEAN DEFAULT FALSE,
                        ocr_processed BOOLEAN DEFAULT FALSE,
                        image_count INTEGER DEFAULT 0
                    );
                """)
            
            # 노드 테이블 (OCR 관련 필드 추가)
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS nodes (
                        id TEXT PRIMARY KEY,
                        document_id INTEGER,
                        node_type TEXT,
                        ontology_class TEXT,
                        confidence REAL,
                        predicted_class INTEGER,
                        creation_time TEXT,
                        source_type TEXT DEFAULT 'text',
                        bbox_x REAL,
                        bbox_y REAL,
                        bbox_width REAL,
                        bbox_height REAL,
                        page_number INTEGER,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS nodes (
                        id TEXT PRIMARY KEY,
                        document_id INTEGER,
                        node_type TEXT,
                        ontology_class TEXT,
                        confidence REAL,
                        predicted_class INTEGER,
                        creation_time TEXT,
                        source_type TEXT DEFAULT 'text',
                        bbox_x REAL,
                        bbox_y REAL,
                        bbox_width REAL,
                        bbox_height REAL,
                        page_number INTEGER,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            
            # 엣지 테이블
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS edges (
                        id INTEGER PRIMARY KEY,
                        document_id INTEGER,
                        source_node TEXT,
                        target_node TEXT,
                        relation_type TEXT,
                        confidence REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS edges (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        document_id INTEGER,
                        source_node TEXT,
                        target_node TEXT,
                        relation_type TEXT,
                        confidence REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            
            # 온톨로지 패턴 테이블
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS ontology_patterns (
                        id INTEGER PRIMARY KEY,
                        pattern_type TEXT,
                        pattern_value TEXT,
                        frequency INTEGER,
                        confidence_avg REAL,
                        last_seen TEXT,
                        document_count INTEGER
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS ontology_patterns (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        pattern_type TEXT,
                        pattern_value TEXT,
                        frequency INTEGER,
                        confidence_avg REAL,
                        last_seen TEXT,
                        document_count INTEGER
                    );
                """)
            
            # 도메인 인사이트 테이블
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS domain_insights (
                        id INTEGER PRIMARY KEY,
                        document_id INTEGER,
                        document_type TEXT,
                        industry_domain TEXT,
                        complexity_score REAL,
                        technical_density REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS domain_insights (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        document_id INTEGER,
                        document_type TEXT,
                        industry_domain TEXT,
                        complexity_score REAL,
                        technical_density REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            
            conn.commit()
            print(f"✅ Database initialized: {'DuckDB' if self.use_duckdb else 'SQLite'}")
            
        except Exception as e:
            print(f"❌ Database initialization error: {e}")
        finally:
            if conn:
                try:
                    conn.close()
                except:
                    pass
    
    def save_processing_results(self, filename, text_content, graph_data, learning_results, processing_time=0, pdf_info=None):
        """처리 결과를 데이터베이스에 저장 (UPSERT 로직, OCR 지원)"""
        conn = None
        try:
            conn = self.get_connection()
            current_time = datetime.now().isoformat()
            
            # PDF 정보 기본값 설정
            if pdf_info is None:
                pdf_info = {
                    'pdf_type': 'text',
                    'has_images': False,
                    'ocr_processed': False,
                    'image_count': 0
                }
            
            # 1. 문서 정보 저장/업데이트 (filename 기준)
            text_preview = text_content[:500] if text_content else ""
            
            if self.use_duckdb:
                # DuckDB 처리
                existing_doc = conn.execute("SELECT id FROM documents WHERE filename = ?", [filename]).fetchone()
                
                if existing_doc:
                    # 기존 문서 업데이트
                    doc_id = existing_doc[0]
                    conn.execute("""
                        UPDATE documents 
                        SET upload_time = ?, content_length = ?, processing_time = ?, text_preview = ?,
                            pdf_type = ?, has_images = ?, ocr_processed = ?, image_count = ?
                        WHERE id = ?
                    """, [
                        current_time, len(text_content), processing_time, text_preview,
                        pdf_info['pdf_type'], pdf_info['has_images'], 
                        pdf_info['ocr_processed'], pdf_info['image_count'], doc_id
                    ])
                    print(f"📝 기존 문서 업데이트: {filename} (ID: {doc_id})")
                    
                    # 기존 관련 데이터 삭제 (새로 저장하기 위해)
                    conn.execute("DELETE FROM nodes WHERE document_id = ?", [doc_id])
                    conn.execute("DELETE FROM edges WHERE document_id = ?", [doc_id])
                    conn.execute("DELETE FROM domain_insights WHERE document_id = ?", [doc_id])
                    print(f"🗑️ 기존 노드/엣지/인사이트 데이터 삭제 완료")
                    
                else:
                    # 새 문서 삽입
                    doc_count = conn.execute("SELECT COUNT(*) FROM documents").fetchone()[0]
                    doc_id = doc_count + 1
                    
                    conn.execute("""
                        INSERT INTO documents (id, filename, upload_time, content_length, processing_time, text_preview,
                                             pdf_type, has_images, ocr_processed, image_count)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, [
                        doc_id, filename, current_time, len(text_content), processing_time, text_preview,
                        pdf_info['pdf_type'], pdf_info['has_images'], 
                        pdf_info['ocr_processed'], pdf_info['image_count']
                    ])
                    print(f"📄 새 문서 생성: {filename} (ID: {doc_id})")
            else:
                # SQLite 처리 (UPSERT 로직 추가)
                cursor = conn.cursor()
                
                # 기존 문서 확인
                cursor.execute("SELECT id FROM documents WHERE filename = ?", (filename,))
                existing_doc = cursor.fetchone()
                
                if existing_doc:
                    # 기존 문서 업데이트
                    doc_id = existing_doc[0]
                    cursor.execute("""
                        UPDATE documents 
                        SET upload_time = ?, content_length = ?, processing_time = ?, text_preview = ?,
                            pdf_type = ?, has_images = ?, ocr_processed = ?, image_count = ?
                        WHERE id = ?
                    """, (
                        current_time, len(text_content), processing_time, text_preview,
                        pdf_info['pdf_type'], pdf_info['has_images'], 
                        pdf_info['ocr_processed'], pdf_info['image_count'], doc_id
                    ))
                    print(f"📝 기존 문서 업데이트: {filename} (ID: {doc_id})")
                    
                    # 기존 관련 데이터 삭제
                    cursor.execute("DELETE FROM nodes WHERE document_id = ?", (doc_id,))
                    cursor.execute("DELETE FROM edges WHERE document_id = ?", (doc_id,))
                    cursor.execute("DELETE FROM domain_insights WHERE document_id = ?", (doc_id,))
                    print(f"🗑️ 기존 노드/엣지/인사이트 데이터 삭제 완료")
                    
                else:
                    # 새 문서 삽입
                    cursor.execute("""
                        INSERT INTO documents (filename, upload_time, content_length, processing_time, text_preview,
                                             pdf_type, has_images, ocr_processed, image_count)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        filename, current_time, len(text_content), processing_time, text_preview,
                        pdf_info['pdf_type'], pdf_info['has_images'], 
                        pdf_info['ocr_processed'], pdf_info['image_count']
                    ))
                    doc_id = cursor.lastrowid
                    print(f"📄 새 문서 생성: {filename} (ID: {doc_id})")
            
            # 2. 노드 저장
            nodes = graph_data.get("nodes", [])
            predictions = graph_data.get("predictions", [])
            
            print(f"💾 노드 저장 시작: {len(nodes)}개")
            
            for i, node in enumerate(nodes):
                predicted_class = predictions[i] if i < len(predictions) else 0
                node_id = node["id"]
                
                # OCR 위치 정보 추출
                bbox = node.get("bbox", {})
                
                if self.use_duckdb:
                    # DuckDB: 새 노드 삽입 (기존 데이터는 이미 삭제됨)
                    conn.execute("""
                        INSERT INTO nodes (id, document_id, node_type, ontology_class, confidence, predicted_class, 
                                         creation_time, source_type, bbox_x, bbox_y, bbox_width, bbox_height, page_number)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, [
                        node_id, doc_id, node.get("type", "unknown"), 
                        node.get("ontology_class", "Unknown"), node.get("confidence", 0.5),
                        predicted_class, current_time, node.get("source_type", "text"),
                        bbox.get("x"), bbox.get("y"), bbox.get("width"), bbox.get("height"), 
                        node.get("page_number")
                    ])
                else:
                    # SQLite: 새 노드 삽입 (기존 데이터는 이미 삭제됨)
                    conn.execute("""
                        INSERT INTO nodes (id, document_id, node_type, ontology_class, confidence, predicted_class, 
                                         creation_time, source_type, bbox_x, bbox_y, bbox_width, bbox_height, page_number)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        node_id, doc_id, node.get("type", "unknown"), 
                        node.get("ontology_class", "Unknown"), node.get("confidence", 0.5),
                        predicted_class, current_time, node.get("source_type", "text"),
                        bbox.get("x"), bbox.get("y"), bbox.get("width"), bbox.get("height"), 
                        node.get("page_number")
                    ))
            
            print(f"✅ 노드 저장 완료: {len(nodes)}개")
                
            # 3. 엣지 저장
            edges = graph_data.get("edges", [])
            print(f"🔗 엣지 저장 시작: {len(edges)}개")
            
            if self.use_duckdb:
                # DuckDB: 엣지 저장
                max_edge_result = conn.execute("SELECT MAX(id) FROM edges").fetchone()
                edge_id = (max_edge_result[0] if max_edge_result[0] is not None else 0) + 1
                
                for edge in edges:
                    conn.execute("""
                        INSERT INTO edges (id, document_id, source_node, target_node, relation_type, confidence, creation_time)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, [
                        edge_id, doc_id, edge.get("source", ""), edge.get("target", ""),
                        edge.get("relation", "unknown"), 0.8, current_time
                    ])
                    edge_id += 1
            else:
                # SQLite: 엣지 저장
                for edge in edges:
                    conn.execute("""
                        INSERT INTO edges (document_id, source_node, target_node, relation_type, confidence, creation_time)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        doc_id, edge.get("source", ""), edge.get("target", ""),
                        edge.get("relation", "unknown"), 0.8, current_time
                    ))
            
            print(f"✅ 엣지 저장 완료: {len(edges)}개")
                
            # 4. 온톨로지 패턴 저장
            patterns = learning_results.get("patterns", {})
            confidence_scores = learning_results.get("confidence_scores", {})
            
            print(f"🧠 온톨로지 패턴 저장 시작")
            
            for pattern_type, pattern_list in patterns.items():
                for pattern_value in pattern_list:
                    pattern_str = str(pattern_value)[:200]  # 길이 제한
                    avg_confidence = sum(confidence_scores.values()) / max(len(confidence_scores), 1)
                    
                    if self.use_duckdb:
                        # DuckDB: 패턴 UPSERT
                        existing_pattern = conn.execute("""
                            SELECT id, frequency, document_count FROM ontology_patterns 
                            WHERE pattern_type = ? AND pattern_value = ?
                        """, [pattern_type, pattern_str]).fetchone()
                        
                        if existing_pattern:
                            # 업데이트
                            conn.execute("""
                                UPDATE ontology_patterns 
                                SET frequency = ?, confidence_avg = ?, last_seen = ?, document_count = ?
                                WHERE id = ?
                            """, [
                                existing_pattern[1] + 1, avg_confidence, current_time, 
                                existing_pattern[2] + 1, existing_pattern[0]
                            ])
                        else:
                            # 삽입
                            max_pattern_result = conn.execute("SELECT MAX(id) FROM ontology_patterns").fetchone()
                            pattern_id = (max_pattern_result[0] if max_pattern_result[0] is not None else 0) + 1
                            
                            conn.execute("""
                                INSERT INTO ontology_patterns (id, pattern_type, pattern_value, frequency, confidence_avg, last_seen, document_count)
                                VALUES (?, ?, ?, ?, ?, ?, ?)
                            """, [pattern_id, pattern_type, pattern_str, 1, avg_confidence, current_time, 1])
                    else:
                        # SQLite: 패턴 삽입 (단순화)
                        conn.execute("""
                            INSERT INTO ontology_patterns (pattern_type, pattern_value, frequency, confidence_avg, last_seen, document_count)
                            VALUES (?, ?, ?, ?, ?, ?)
                        """, (pattern_type, pattern_str, 1, avg_confidence, current_time, 1))
            
            print(f"✅ 온톨로지 패턴 저장 완료")
                
            # 5. 도메인 인사이트 저장
            domain_insights = learning_results.get("domain_insights", {})
            if domain_insights:
                print(f"🎯 도메인 인사이트 저장 시작")
                
                if self.use_duckdb:
                    max_insight_result = conn.execute("SELECT MAX(id) FROM domain_insights").fetchone()
                    insight_id = (max_insight_result[0] if max_insight_result[0] is not None else 0) + 1
                    
                    conn.execute("""
                        INSERT INTO domain_insights (id, document_id, document_type, industry_domain, complexity_score, technical_density, creation_time)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, [
                        insight_id, doc_id, domain_insights.get("document_type", "unknown"),
                        domain_insights.get("industry_domain", "unknown"),
                        domain_insights.get("complexity_score", 0),
                        domain_insights.get("technical_density", 0),
                        current_time
                    ])
                else:
                    conn.execute("""
                        INSERT INTO domain_insights (document_id, document_type, industry_domain, complexity_score, technical_density, creation_time)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        doc_id, domain_insights.get("document_type", "unknown"),
                        domain_insights.get("industry_domain", "unknown"),
                        domain_insights.get("complexity_score", 0),
                        domain_insights.get("technical_density", 0),
                        current_time
                    ))
                
                print(f"✅ 도메인 인사이트 저장 완료")
            
            # 트랜잭션 커밋
            conn.commit()
            
            print(f"🎉 전체 데이터 저장 완료 - Document ID: {doc_id}")
            print(f"📊 저장된 데이터: 노드 {len(nodes)}개, 엣지 {len(edges)}개")
            return doc_id
            
        except Exception as e:
            print(f"❌ Database save error: {e}")
            import traceback
            traceback.print_exc()
            return None
        finally:
            if conn:
                try:
                    conn.close()
                except:
                    pass
    
    def execute_query(self, query):
        """SQL 쿼리 실행"""
        conn = None
        try:
            conn = self.get_connection()
            
            if self.use_duckdb:
                result = conn.execute(query).fetch_df()
            else:
                result = pd.read_sql_query(query, conn)
            
            return result
            
        except Exception as e:
            print(f"❌ Query execution error: {e}")
            return pd.DataFrame()
        finally:
            if conn:
                try:
                    conn.close()
                except:
                    pass
    
    def get_database_stats(self):
        """데이터베이스 통계 반환"""
        try:
            stats = {}
            
            # 기본 통계
            stats["total_documents"] = self.execute_query("SELECT COUNT(*) as count FROM documents")["count"].iloc[0]
            stats["total_nodes"] = self.execute_query("SELECT COUNT(*) as count FROM nodes")["count"].iloc[0]
            stats["total_edges"] = self.execute_query("SELECT COUNT(*) as count FROM edges")["count"].iloc[0]
            stats["total_patterns"] = self.execute_query("SELECT COUNT(*) as count FROM ontology_patterns")["count"].iloc[0]
            
            # OCR 관련 통계 추가
            ocr_stats = self.execute_query("""
                SELECT 
                    pdf_type,
                    COUNT(*) as count,
                    SUM(CASE WHEN ocr_processed THEN 1 ELSE 0 END) as ocr_processed_count,
                    SUM(image_count) as total_images
                FROM documents 
                GROUP BY pdf_type
            """)
            stats["ocr_statistics"] = ocr_stats.to_dict('records') if not ocr_stats.empty else []
            
            # 최근 문서
            recent_docs = self.execute_query("""
                SELECT filename, upload_time, content_length, pdf_type, ocr_processed
                FROM documents 
                ORDER BY upload_time DESC 
                LIMIT 5
            """)
            stats["recent_documents"] = recent_docs.to_dict('records') if not recent_docs.empty else []
            
            # 노드 타입별 분포
            node_distribution = self.execute_query("""
                SELECT node_type, COUNT(*) as count 
                FROM nodes 
                GROUP BY node_type
            """)
            stats["node_distribution"] = node_distribution.to_dict('records') if not node_distribution.empty else []
            
            # 소스 타입별 분포 (텍스트 vs OCR)
            source_distribution = self.execute_query("""
                SELECT source_type, COUNT(*) as count 
                FROM nodes 
                GROUP BY source_type
            """)
            stats["source_distribution"] = source_distribution.to_dict('records') if not source_distribution.empty else []
            
            # 온톨로지 클래스별 분포  
            class_distribution = self.execute_query("""
                SELECT ontology_class, COUNT(*) as count, AVG(confidence) as avg_confidence
                FROM nodes 
                GROUP BY ontology_class 
                ORDER BY count DESC
            """)
            stats["class_distribution"] = class_distribution.to_dict('records') if not class_distribution.empty else []
            
            # 고신뢰도 엔티티 수
            high_confidence_count = self.execute_query("""
                SELECT COUNT(*) as count 
                FROM nodes 
                WHERE confidence > 0.8
            """)["count"].iloc[0]
            stats["high_confidence_entities"] = high_confidence_count
            
            return stats
            
        except Exception as e:
            print(f"❌ Database stats error: {e}")
            return {}

def initialize_ocr():
    """OCR 시스템 초기화"""
    global OCR_AVAILABLE, ocr_reader
    
    # OCR이 전역적으로 비활성화된 경우 건너뛰기
    if not OCR_AVAILABLE:
        print("⏭️ OCR이 전역적으로 비활성화되어 있습니다.")
        return False
    
    try:
        print("🔄 PaddleOCR 초기화 중...")
        
        # 필요한 라이브러리 import 확인
        try:
            from paddleocr import PaddleOCR
            import cv2
            import numpy as np
        except ImportError as e:
            print(f"❌ OCR 라이브러리 import 실패: {e}")
            OCR_AVAILABLE = False
            return False
        
        # PaddleOCR 초기화 (안전한 설정)
        ocr_reader = PaddleOCR(
            use_angle_cls=False,    # 회전 감지 비활성화 (안정성)
            lang='en',              # 영어 인식
            show_log=False,         # 로그 숨기기
            use_gpu=False,          # CPU 사용
            enable_mkldnn=False,    # 멀티스레딩 비활성화 (안정성)
            cpu_threads=1           # 단일 스레드 (충돌 방지)
        )
        
        # 테스트 실행
        print("🧪 OCR 테스트 중...")
        test_image = np.ones((100, 100, 3), dtype=np.uint8) * 255  # 흰색 이미지
        test_result = ocr_reader.ocr(test_image, cls=False)
        
        OCR_AVAILABLE = True
        print("✅ PaddleOCR 초기화 및 테스트 완료")
        return True
        
    except Exception as e:
        print(f"❌ PaddleOCR 초기화 실패: {e}")
        OCR_AVAILABLE = False
        ocr_reader = None
        return False

def get_ocr_status():
    """OCR 상태 확인 (개선된 버전)"""
    return {
        'available': OCR_AVAILABLE,
        'reader_loaded': ocr_reader is not None,
        'settings': {
            'dpi': PDF_IMAGE_DPI,
            'confidence_threshold': OCR_CONFIDENCE_THRESHOLD,
            'enabled_globally': OCR_AVAILABLE  # 전역 설정 상태
        },
        'recommendation': 'OCR is disabled for better performance' if not OCR_AVAILABLE else 'OCR is ready'
    }
def enable_ocr():
    """OCR을 활성화하고 초기화"""
    global OCR_AVAILABLE
    
    print("🔄 OCR 활성화 시도...")
    OCR_AVAILABLE = True  # 전역 플래그 활성화
    
    if initialize_ocr():
        print("✅ OCR 활성화 성공")
        return True
    else:
        print("❌ OCR 활성화 실패")
        OCR_AVAILABLE = False
        return False

def disable_ocr():
    """OCR을 비활성화"""
    global OCR_AVAILABLE, ocr_reader
    
    print("⏹️ OCR 비활성화...")
    OCR_AVAILABLE = False
    ocr_reader = None
    print("✅ OCR 비활성화 완료")

# 조건부 OCR 사용을 위한 헬퍼 함수
def should_use_ocr(text_length, pdf_type):
    """OCR 사용 여부를 결정"""
    MIN_TEXT_THRESHOLD = 100
    
    if not OCR_AVAILABLE or not ocr_reader:
        return False, "OCR not available"
    
    if text_length < MIN_TEXT_THRESHOLD:
        return True, f"Text too short ({text_length} < {MIN_TEXT_THRESHOLD})"
    
    if pdf_type == 'image':
        return True, "Image-based PDF detected"
    
    return False, "Sufficient text available"

def detect_pdf_type(pdf_path):
    """PDF 타입 판별 (텍스트/이미지/하이브리드)"""
    try:
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        text_pages = 0
        image_pages = 0
        total_images = 0
        
        print(f"📄 PDF 분석 중: {total_pages}페이지")
        
        for page_num in range(total_pages):
            page = doc[page_num]
            
            # 텍스트 추출 시도
            text = page.get_text().strip()
            if len(text) > 50:  # 50자 이상이면 텍스트 페이지로 간주
                text_pages += 1
            
            # 이미지 개수 확인
            image_list = page.get_images()
            if image_list:
                image_pages += 1
                total_images += len(image_list)
        
        doc.close()
        
        # 타입 결정 로직
        text_ratio = text_pages / total_pages if total_pages > 0 else 0
        image_ratio = image_pages / total_pages if total_pages > 0 else 0
        
        if text_ratio > 0.8:
            pdf_type = 'text'
        elif image_ratio > 0.5 or total_images > total_pages:
            pdf_type = 'image'
        else:
            pdf_type = 'hybrid'
        
        result = {
            'pdf_type': pdf_type,
            'total_pages': total_pages,
            'text_pages': text_pages,
            'image_pages': image_pages,
            'total_images': total_images,
            'text_ratio': text_ratio,
            'image_ratio': image_ratio,
            'has_images': total_images > 0
        }
        
        print(f"📊 PDF 타입: {pdf_type} (텍스트: {text_pages}p, 이미지: {image_pages}p)")
        return result
        
    except Exception as e:
        print(f"❌ PDF 타입 판별 실패: {e}")
        return {
            'pdf_type': 'unknown',
            'total_pages': 0,
            'text_pages': 0,
            'image_pages': 0,
            'total_images': 0,
            'text_ratio': 0,
            'image_ratio': 0,
            'has_images': False
        }
    
def preprocess_image(image):
    """OpenCV를 사용한 이미지 전처리 (OCR 정확도 향상) - 개선된 버전"""
    
    # OCR이 비활성화된 경우 원본 반환
    if not OCR_AVAILABLE:
        return image
    
    try:
        # 필요한 라이브러리 확인
        try:
            import cv2
            import numpy as np
            from PIL import Image
        except ImportError as e:
            print(f"❌ 이미지 처리 라이브러리 없음: {e}")
            return image
        
        # NumPy 배열로 변환
        if isinstance(image, Image.Image):
            image_np = np.array(image)
        else:
            image_np = image
        
        # 입력 검증
        if image_np is None or image_np.size == 0:
            print("⚠️ 빈 이미지입니다.")
            return image
        
        # 그레이스케일 변환
        if len(image_np.shape) == 3:
            if image_np.shape[2] == 4:  # RGBA
                gray = cv2.cvtColor(image_np, cv2.COLOR_RGBA2GRAY)
            else:  # RGB
                gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
        else:
            gray = image_np
        
        # 이미지 크기 확인 (너무 작으면 전처리 건너뛰기)
        height, width = gray.shape
        if height < 32 or width < 32:
            print("⚠️ 이미지가 너무 작아 전처리를 건너뜁니다.")
            return Image.fromarray(gray) if not isinstance(image, Image.Image) else image
        
        # 이미지 품질 개선 파이프라인 (단계별 오류 처리)
        processed = gray.copy()
        
        try:
            # 1. 노이즈 제거 (블러 크기 조정)
            blur_size = 3 if min(height, width) > 100 else 1
            if blur_size > 1:
                processed = cv2.medianBlur(processed, blur_size)
        except Exception as e:
            print(f"⚠️ 노이즈 제거 실패: {e}")
        
        try:
            # 2. 대비 개선 (CLAHE)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            processed = clahe.apply(processed)
        except Exception as e:
            print(f"⚠️ 대비 개선 실패: {e}")
        
        try:
            # 3. 선택적 샤프닝 (작은 이미지는 건너뛰기)
            if min(height, width) > 50:
                kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]], dtype=np.float32)
                processed = cv2.filter2D(processed, -1, kernel)
                processed = np.clip(processed, 0, 255).astype(np.uint8)
        except Exception as e:
            print(f"⚠️ 샤프닝 실패: {e}")
        
        try:
            # 4. 적응형 이진화 (Otsu보다 안전)
            processed = cv2.adaptiveThreshold(
                processed, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                cv2.THRESH_BINARY, 11, 2
            )
        except Exception as e:
            print(f"⚠️ 이진화 실패, Otsu 사용: {e}")
            try:
                _, processed = cv2.threshold(processed, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            except:
                pass  # 이진화도 실패하면 그냥 진행
        
        try:
            # 5. 모폴로지 연산 (선택적)
            if min(height, width) > 50:
                kernel = np.ones((2,2), np.uint8)
                processed = cv2.morphologyEx(processed, cv2.MORPH_CLOSE, kernel)
        except Exception as e:
            print(f"⚠️ 모폴로지 연산 실패: {e}")
        
        # PIL Image로 변환
        if isinstance(image, Image.Image):
            return Image.fromarray(processed)
        else:
            return processed
        
    except Exception as e:
        print(f"❌ 이미지 전처리 실패: {e}")
        
        # 전처리 실패시 안전한 원본 반환
        try:
            if isinstance(image, Image.Image):
                return image
            else:
                return Image.fromarray(image) if hasattr(image, 'shape') else image
        except:
            print("❌ 원본 이미지 반환도 실패")
            return image

def pdf_to_images(pdf_path, dpi=None):
    """PDF를 이미지로 변환 - 개선된 버전"""
    
    # OCR이 비활성화된 경우 빈 리스트 반환
    if not OCR_AVAILABLE:
        print("⏭️ OCR이 비활성화되어 PDF 이미지 변환을 건너뜁니다.")
        return []
    
    if dpi is None:
        dpi = PDF_IMAGE_DPI
    
    doc = None
    try:
        # 필요한 라이브러리 확인
        try:
            import fitz  # PyMuPDF
            from PIL import Image
            import io
        except ImportError as e:
            print(f"❌ PDF 처리 라이브러리 없음: {e}")
            return []
        
        # 파일 존재 확인
        if not os.path.exists(pdf_path):
            print(f"❌ PDF 파일을 찾을 수 없습니다: {pdf_path}")
            return []
        
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        images = []
        
        # 페이지 수 제한 (메모리 보호)
        MAX_PAGES = 10
        if total_pages > MAX_PAGES:
            print(f"⚠️ 페이지 수가 많습니다 ({total_pages}). 처음 {MAX_PAGES}페이지만 처리합니다.")
            total_pages = MAX_PAGES
        
        print(f"🖼️ PDF를 이미지로 변환 중 (DPI: {dpi}, 페이지: {total_pages})")
        
        for page_num in range(total_pages):
            try:
                page = doc[page_num]
                
                # DPI 조정 (너무 높으면 메모리 문제)
                safe_dpi = min(dpi, 200)  # 최대 200 DPI로 제한
                
                # 페이지를 이미지로 변환
                mat = fitz.Matrix(safe_dpi/72, safe_dpi/72)  # 72 DPI가 기본값
                pix = page.get_pixmap(matrix=mat)
                
                # 이미지 크기 확인 (너무 크면 건너뛰기)
                if pix.width * pix.height > 4000000:  # 4MP 이상
                    print(f"⚠️ 페이지 {page_num + 1}: 이미지가 너무 큽니다. 건너뜁니다.")
                    continue
                
                img_data = pix.tobytes("ppm")
                
                # PIL Image로 변환
                image = Image.open(io.BytesIO(img_data))
                
                # 전처리 적용 (조건부)
                try:
                    processed_image = preprocess_image(image)
                except Exception as e:
                    print(f"⚠️ 페이지 {page_num + 1} 전처리 실패: {e}")
                    processed_image = image  # 전처리 실패시 원본 사용
                
                # 결과 저장
                page_result = {
                    'page_number': page_num + 1,
                    'image': processed_image,
                    'original_size': image.size,
                    'processed_size': processed_image.size if processed_image != image else image.size,
                    'dpi_used': safe_dpi
                }
                
                images.append(page_result)
                
                # 메모리 정리
                pix = None
                
            except Exception as e:
                print(f"❌ 페이지 {page_num + 1} 변환 실패: {e}")
                continue  # 이 페이지는 건너뛰고 다음 페이지 처리
        
        print(f"✅ PDF 변환 완료: {len(images)}페이지 (총 {len(doc)}페이지 중)")
        return images
        
    except Exception as e:
        print(f"❌ PDF 이미지 변환 실패: {e}")
        return []
        
    finally:
        # 안전한 문서 해제
        if doc:
            try:
                doc.close()
            except:
                pass
        
def extract_pdf_text(pdf_path, debug_mode=False):
    """PDF에서 텍스트 추출 - 개선된 버전"""
    
    if PDF_AVAILABLE:
        try:
            # 파일 존재 확인
            if not os.path.exists(pdf_path):
                print(f"❌ PDF 파일을 찾을 수 없습니다: {pdf_path}")
                return ""
            
            # pdfminer를 사용한 텍스트 추출
            text = extract_text(pdf_path)
            
            # 텍스트 기본 정리
            if text:
                text = text.strip()
                # 과도한 공백 정리
                text = re.sub(r'\n\s*\n', '\n\n', text)  # 연속된 빈 줄 정리
                text = re.sub(r' +', ' ', text)  # 연속된 공백 정리
            
            print(f"📝 텍스트 추출 완료: {len(text)} characters")
            
            # 선택적 디버깅 (기본적으로 비활성화)
            if debug_mode and text:
                print("🔍 PDF TEXT DEBUG:")
                print("="*50)
                print(text[:500])  # 첫 500자만 출력 (축소)
                print("="*50)
                
                # 핵심 키워드만 검색
                keywords = ["REVISION", "REV", "PROJECT", "JOB", "EQUIPMENT"]
                found_keywords = [kw for kw in keywords if kw in text.upper()]
                if found_keywords:
                    print(f"🔍 Key terms found: {found_keywords}")
                print("="*50)
            
            return text if text else ""
            
        except Exception as e:
            print(f"❌ PDF 텍스트 추출 실패: {e}")
            
            # Fallback: PyMuPDF 시도
            try:
                import fitz
                doc = fitz.open(pdf_path)
                text = ""
                for page in doc:
                    text += page.get_text()
                doc.close()
                
                if text.strip():
                    print(f"✅ PyMuPDF fallback 성공: {len(text)} characters")
                    return text.strip()
                else:
                    print("⚠️ PyMuPDF fallback: 텍스트 없음")
                    
            except Exception as fallback_error:
                print(f"❌ PyMuPDF fallback 실패: {fallback_error}")
            
            return ""
    else:
        # PDF 라이브러리가 없는 경우 Mock 데이터 반환
        print("⚠️ PDF 라이브러리 없음 - Mock 텍스트 사용")
        return """Industrial Equipment Specification
Project: 7T04 - Centrifugal Pump Process Data
Equipment ID: P-2105 A/B
Pump Type: CENTRIFUGAL
Driver Type: MOTOR / MOTOR
Capacity: 71.1 m3/h (AM Feed)
Temperature: 384 ℃
Pressure: 17.1 kg/cm2A
Viscosity: 1.0 cP
Revision: 14
Checked by: HJL / SKL
Date: 2012-12-26
JOB NO: 7T04
PROJECT: ERC 12345
DOC. NO.: 7T04-PR-21-DS-505
ITEM NO: P-2105 A/B
SERVICE: OVERFLASH PUMPS
PUMP TYPE: CENTRIFUGAL PUMP
REQUIRED TYPE: HORIZONTAL
DRIVER TYPE: MOTOR
OPERATING: ONE CONTINUOUS
STAND-BY: MOTOR ONE
LIQUID NAME: AM FEED, AH FEED, OVERFLASH
PUMPING TEMPERATURE: 384℃
VAPOR PRESSURE: 0.071 kg/cm2A
SPECIFIC GRAVITY: 1.0
VISCOSITY: 1.0 cP
CAPACITY: 71.1 m3/hr
SUCTION PRESSURE: 5.9 kg/cm2g
DIFFERENTIAL HEAD: 17.1 kg/cm2
BY/CHECKED: HJL / SKL, WSJ / WGK
REVISION: 14
DATE: 2012-12-26"""

# 디버깅용 헬퍼 함수 (선택적 사용)
def debug_pdf_text(pdf_path, keywords=None):
    """PDF 텍스트 디버깅 전용 함수"""
    if keywords is None:
        keywords = ["REVISION", "PROJECT", "EQUIPMENT", "PUMP", "MOTOR"]
    
    text = extract_pdf_text(pdf_path, debug_mode=False)
    
    if not text:
        print("❌ 텍스트 추출 실패")
        return
    
    print("🔍 DETAILED PDF TEXT DEBUG:")
    print("="*60)
    print(f"Total length: {len(text)} characters")
    print(f"Lines: {len(text.splitlines())}")
    print("="*60)
    print("First 1000 characters:")
    print(text[:1000])
    print("="*60)
    
    print("🔍 KEYWORD ANALYSIS:")
    for keyword in keywords:
        if keyword.upper() in text.upper():
            print(f"   ✅ Found: {keyword}")
            # 컨텍스트 표시
            idx = text.upper().find(keyword.upper())
            context = text[max(0, idx-30):idx+50]
            print(f"      Context: {repr(context)}")
        else:
            print(f"   ❌ Missing: {keyword}")
    print("="*60)

def extract_pdf_content(pdf_path):
    """PDF에서 콘텐츠 추출 (조건부 OCR 지원) - 개선된 버전"""
    try:
        print(f"📄 PDF 콘텐츠 추출 시작: {pdf_path}")
        
        # 파일 존재 확인
        if not os.path.exists(pdf_path):
            print(f"❌ PDF 파일을 찾을 수 없습니다: {pdf_path}")
            return {
                'text': "",
                'ocr_results': [],
                'pdf_info': {'pdf_type': 'unknown', 'has_images': False},
                'processing_method': 'error'
            }
        
        # 1. PDF 타입 분석
        pdf_info = detect_pdf_type(pdf_path)
        pdf_type = pdf_info['pdf_type']
        
        print(f"📊 PDF 타입 분석: {pdf_type}")
        print(f"   - 총 페이지: {pdf_info['total_pages']}")
        print(f"   - 텍스트 페이지: {pdf_info['text_pages']}")
        print(f"   - 이미지 페이지: {pdf_info['image_pages']}")
        
        text_content = ""
        ocr_results = []
        processing_method = "text"
        
        # 2. 먼저 기본 텍스트 추출 시도
        text_content = extract_pdf_text(pdf_path)
        text_length = len(text_content.strip())
        
        print(f"📝 기본 텍스트 추출: {text_length} characters")
        
        # 3. OCR 사용 조건 판단
        MIN_TEXT_THRESHOLD = 100  # 최소 텍스트 길이
        use_ocr, ocr_reason = should_use_ocr(text_length, pdf_type)
        
        print(f"🔍 OCR 사용 판단: {'사용' if use_ocr else '건너뛰기'} - {ocr_reason}")
        
        # 4. 조건부 OCR 실행
        if use_ocr:
            try:
                print("🔍 OCR 처리 시작...")
                images = pdf_to_images(pdf_path)
                
                if not images:
                    print("⚠️ PDF 이미지 변환 실패 - OCR 건너뛰기")
                else:
                    ocr_text_added = ""
                    
                    for image_info in images:
                        try:
                            page_num = image_info['page_number']
                            image = image_info['image']
                            
                            # OCR 수행 (안전한 호출)
                            if ocr_reader:
                                ocr_result = ocr_reader.ocr(np.array(image), cls=True)
                                
                                # OCR 결과 처리
                                if ocr_result and len(ocr_result) > 0:
                                    for line in ocr_result:
                                        if line:  # 빈 라인 확인
                                            for word_info in line:
                                                if len(word_info) >= 2:  # 올바른 구조 확인
                                                    bbox, (text, confidence) = word_info
                                                    
                                                    if confidence > OCR_CONFIDENCE_THRESHOLD and len(text.strip()) > 0:
                                                        # 중복 방지: 이미 추출된 텍스트에 없는 경우만 추가
                                                        if text.strip() not in text_content:
                                                            ocr_results.append({
                                                                'text': text.strip(),
                                                                'confidence': confidence,
                                                                'bbox': {
                                                                    'x': float(bbox[0][0]),
                                                                    'y': float(bbox[0][1]),
                                                                    'width': float(bbox[2][0] - bbox[0][0]),
                                                                    'height': float(bbox[2][1] - bbox[0][1])
                                                                },
                                                                'page_number': page_num
                                                            })
                                                            ocr_text_added += text.strip() + " "
                            else:
                                print(f"⚠️ 페이지 {page_num}: OCR reader가 None입니다.")
                                
                        except Exception as page_error:
                            print(f"❌ 페이지 {image_info.get('page_number', '?')} OCR 실패: {page_error}")
                            continue
                    
                    # OCR로 추가된 텍스트가 있으면 기존 텍스트에 합치기
                    if ocr_text_added.strip():
                        text_content += " " + ocr_text_added.strip()
                        processing_method = "hybrid" if text_length >= MIN_TEXT_THRESHOLD else "image"
                        print(f"🔍 OCR 완료: {len(ocr_results)}개 요소 추가, 총 텍스트: {len(text_content)} characters")
                    else:
                        print("🔍 OCR 완료: 추가된 텍스트 없음")
                        processing_method = "text"
                    
            except Exception as e:
                print(f"❌ OCR 처리 실패: {e}")
                import traceback
                traceback.print_exc()
                processing_method = "text"
        else:
            print("✅ 충분한 텍스트가 추출되었습니다. OCR을 건너뜁니다.")
            processing_method = "text"
        
        # 5. 최종 결과 확인
        final_text_length = len(text_content.strip())
        if final_text_length < MIN_TEXT_THRESHOLD:
            print(f"⚠️ 최종 텍스트 길이가 부족합니다: {final_text_length} characters")
        
        # 6. 결과 반환
        result = {
            'text': text_content.strip(),
            'ocr_results': ocr_results,
            'pdf_info': pdf_info,
            'processing_method': processing_method
        }
        
        print(f"✅ PDF 콘텐츠 추출 완료:")
        print(f"   - 처리 방법: {processing_method}")
        print(f"   - 최종 텍스트: {len(text_content)} characters")
        print(f"   - OCR 요소: {len(ocr_results)}개")
        print(f"   - OCR 사용됨: {'Yes' if use_ocr and ocr_results else 'No'}")
        
        return result
        
    except Exception as e:
        print(f"❌ PDF 콘텐츠 추출 실패: {e}")
        import traceback
        traceback.print_exc()
        
        return {
            'text': "",
            'ocr_results': [],
            'pdf_info': {'pdf_type': 'unknown', 'has_images': False},
            'processing_method': 'error'
        }


# 온톨로지 클래스 (완전 수정된 버전)
class AdvancedOntologyLearner:
    def __init__(self):
        # 기본 온톨로지 스키마
        self.classes = {"Project", "Equipment", "ProcessRequirement", "Person", "Document", "Revision", "Note", "Field", "Date"}
        self.object_properties = {"hasEquipment", "hasProcessReq", "reviewedBy", "hasProject", "hasValue", "hasRevision", "hasNote", "hasField"}
        self.datatype_properties = {"jobNo", "itemNo", "value", "noteText", "revisionNumber", "revisionDate", "fieldName", "fieldValue"}
        
        # 학습된 패턴 저장소
        self.learned_patterns = {}
        self.entity_instances = {}
        self.confidence_scores = {}
        
        # 확장된 도메인 특화 키워드
        self.domain_keywords = {
            "Project": ["job", "project", "doc", "document", "specification", "drawing", "client", "location"],
            "Equipment": ["pump", "motor", "driver", "centrifugal", "equipment", "vessel", "tank", "compressor", "exchanger"],
            "ProcessRequirement": ["temperature", "pressure", "capacity", "flow", "viscosity", "density", "npsh", "head", "suction", "discharge"],
            "Person": ["checked", "reviewed", "approved", "by", "engineer", "manager"],
            "Revision": ["revision", "rev", "version", "updated", "modified"],
            "Note": ["note", "remark", "comment", "description", "notes"],
            "Field": ["type", "required", "operating", "duty", "liquid", "vapor", "specific", "differential", "material", "method"],
            "Date": ["date", "time", "year", "month", "day"]
        }
        
        # 확장된 단위 및 측정값 패턴
        self.unit_patterns = {
            "temperature": ["℃", "°C", "°F", "K", "DEG C"],
            "pressure": ["kg/cm2", "kg/cm2g", "kg/cm2A", "bar", "psi", "Pa", "kPa", "MPa"],
            "flow": ["m3/h", "m3/hr", "M3/HR", "l/min", "gpm", "bph"],
            "viscosity": ["cP", "Pa·s", "cSt"],
            "length": ["mm", "cm", "m", "in", "ft", "METER"],
            "percentage": ["%", "wt%", "WT%"],
            "density": ["KG/M3", "kg/m3"],
            "time": ["hr", "yr", "hour", "year"]
        }
        
        # 화학공정 전문용어
        self.process_terms = {
            "fluids": ["OVERFLASH", "GAS OIL", "AH FEED", "AM FEED", "SULFUR"],
            "materials": ["API CLASS", "FLAMMABLE", "TOXIC", "H2S", "CHLORIDE"],
            "operations": ["CONTINUOUS", "INTERMITTENT", "MANUAL", "AUTOMATIC", "INDOOR", "OUTDOOR"],
            "equipment_types": ["HORIZONTAL", "CENTRIFUGAL", "STEAM TRACING", "STEAM JACKET", "INSULATION"]
        }
        
    def learn_from_text(self, text):
        print("🧠 Advanced ontology learning from text...")
        
        # 1. 기본 패턴 추출 (대폭 강화)
        basic_patterns = self._extract_basic_patterns(text)
        
        # 2. 필드-값 쌍 추출
        field_value_pairs = self._extract_field_value_pairs(text)
        
        # 3. NOTE 섹션 추출  
        notes = self._extract_notes_section(text)
        
        # 4. 컨텍스트 기반 엔티티 인식 + 기본 패턴을 엔티티로 변환
        contextual_entities = self._extract_contextual_entities(text, basic_patterns)
        
        # 5. 관계 추출
        relations = self._extract_relations(text, contextual_entities)
        
        # 6. 신뢰도 계산
        confidence_scores = self._calculate_confidence(basic_patterns, contextual_entities)
        
        # 7. 온톨로지 매칭
        ontology_matches = self._match_to_ontology(contextual_entities)
        
        # 8. 학습 결과 저장
        self._update_learned_knowledge(basic_patterns, contextual_entities, relations)
        
        # 통합 결과
        results = {
            "patterns": basic_patterns,
            "field_value_pairs": field_value_pairs,
            "notes": notes,
            "new_entities": contextual_entities,  # 이제 기본 패턴이 포함됨
            "new_relations": relations,
            "matched_entities": ontology_matches,
            "confidence_scores": confidence_scores,
            "domain_insights": self._generate_domain_insights(text)
        }
        
        # 총 엔티티 수 계산
        total_entities = (
            sum(len(v) if isinstance(v, list) else 1 for v in basic_patterns.values()) +
            len(field_value_pairs) +
            len(notes) +
            sum(len(v) for v in contextual_entities.values())
        )
        
        print(f"📊 Enhanced learning completed:")
        print(f"   - Basic Patterns: {len(basic_patterns)} types")
        print(f"   - Field-Value Pairs: {len(field_value_pairs)}")
        print(f"   - Notes: {len(notes)}")
        print(f"   - Contextual Entities: {sum(len(v) for v in contextual_entities.values())}")
        print(f"   - Total Entities: {total_entities}")
        print(f"   - Relations: {len(relations)} identified")
        print(f"   - Avg Confidence: {sum(confidence_scores.values())/len(confidence_scores) if confidence_scores else 0:.2f}")
        
        return results
    
    def _extract_basic_patterns(self, text):
        """기본 패턴 추출 - 완전 개선된 버전"""
        patterns = {}
        
        try:
            # 1. 프로젝트/문서 번호 패턴 (확장)
            project_patterns = [
                r'(?:JOB\s*NO\.?\s*:?\s*)([A-Z0-9]{2,}(?:-[A-Z0-9]+)*)',
                r'(?:PROJECT\s*:?\s*)([A-Z0-9]{2,}(?:\s+[A-Z0-9]+)*)',
                r'(?:DOC\.?\s*NO\.?\s*:?\s*)([A-Z0-9]{2,}(?:-[A-Z0-9]+)*)',
                r'(?:ITEM\s*NO\.?\s*:?\s*)([A-Z0-9]{2,}(?:-[A-Z0-9]+)*)',
                r'\b([A-Z]\d+[A-Z]?\d*(?:-[A-Z0-9]+)*)\b'
            ]
            
            project_ids = []
            for pattern in project_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                project_ids.extend(matches)
            
            if project_ids:
                patterns["project_ids"] = list(set(project_ids))
            
            # 2. 모든 리비전 번호 추출
            revision_patterns = [
                r'(?:REVISION|REV\.?|REV)\s*:?\s*(\d+[A-Z]?)',
                r'(?:revision|rev)\s+(\d+[A-Z]?)',
                r'\bR(\d+[A-Z]?)\b',
                r'\bRev\s*(\d+[A-Z]?)\b',
                r'\b(1[0-9][A-Z]?)\b',  # 10-19
                r'\b([2-9]\d[A-Z]?)\b'   # 20+
            ]
            
            revision_numbers = []
            for pattern in revision_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                revision_numbers.extend(matches)
            
            if revision_numbers:
                patterns["revision_numbers"] = list(set(revision_numbers))
            
            # 3. 모든 날짜 패턴 추출
            date_patterns = [
                r'(\d{4}-\d{1,2}-\d{1,2})',  # YYYY-MM-DD
                r'(\d{1,2}/\d{1,2}/\d{4})',  # MM/DD/YYYY
                r'(\d{1,2}-\d{1,2}-\d{4})',  # MM-DD-YYYY
                r'(20\d{2}-\d{1,2}-\d{1,2})', # 20XX-XX-XX
            ]
            
            dates = []
            for pattern in date_patterns:
                matches = re.findall(pattern, text)
                dates.extend(matches)
            
            if dates:
                patterns["dates"] = list(set(dates))
            
            # 4. 모든 사람 이름/이니셜 추출
            person_patterns = [
                r'(?:BY/CHECKED|BY CHECKED|Checked\s+by|Reviewed\s+by|Approved\s+by)\s*:?\s*([A-Z]{2,4}(?:\s*/\s*[A-Z]{2,4})*)',
                r'\b([A-Z]{2,4})\s*/\s*([A-Z]{2,4})\b',
                r'\b([A-Z]{3})\s+/\s+([A-Z]{3})\b',
            ]
            
            person_names = []
            for pattern in person_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    if isinstance(match, tuple):
                        person_names.extend([m for m in match if m and len(m) >= 2])
                    else:
                        names = re.split(r'\s*/\s*', match)
                        person_names.extend([n.strip() for n in names if n.strip() and len(n.strip()) >= 2])
            
            if person_names:
                patterns["person_names"] = list(set(person_names))
            
            # 5. 장비 식별자 패턴
            equipment_patterns = [
                r'([A-Z]-\d+(?:\s*[A-Z](?:/[A-Z])?)?)',
                r'([A-Z]{2,3}-\d+)',
                r'([A-Z]\d{3,}[A-Z]?)',
                r'(P-\d+\s*[A-Z]?/?[A-Z]?)',
            ]
            
            equipment_ids = []
            for pattern in equipment_patterns:
                matches = re.findall(pattern, text)
                equipment_ids.extend(matches)
            
            if equipment_ids:
                patterns["equipment_ids"] = list(set(equipment_ids))
            
            # 6. 수치 값 패턴
            numerical_patterns = []
            for unit_type, units in self.unit_patterns.items():
                for unit in units:
                    pattern = rf'(\d+\.?\d*)\s*{re.escape(unit)}'
                    matches = re.findall(pattern, text, re.IGNORECASE)
                    for match in matches:
                        numerical_patterns.append((match, unit, unit_type))
            
            if numerical_patterns:
                patterns["numerical_values"] = numerical_patterns[:50]  # 제한
            
            # 7. 화학공정 전문용어
            process_entities = []
            for category, terms in self.process_terms.items():
                for term in terms:
                    if term in text:
                        process_entities.append((term, category))
            
            if process_entities:
                patterns["process_terms"] = process_entities
            
            # 8. API 분류 및 재료 등급
            material_patterns = [
                r'(API\s+CLASS\s+[A-Z]-\d+)',
                r'(API\s+[A-Z]-\d+)',
                r'([A-Z]\s*-\s*\d+\s*\([^)]+\))',
            ]
            
            materials = []
            for pattern in material_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                materials.extend(matches)
            
            if materials:
                patterns["materials"] = list(set(materials))
            
            print(f"🔍 Extracted basic patterns: {list(patterns.keys())}")
            for pattern_type, values in patterns.items():
                if isinstance(values, list):
                    print(f"   {pattern_type}: {values[:5]}{'...' if len(values) > 5 else ''}")
                else:
                    print(f"   {pattern_type}: {values}")
            
        except Exception as e:
            print(f"❌ Pattern extraction error: {e}")
            patterns = {}
        
        return patterns
    
    def _extract_field_value_pairs(self, text):
        """필드-값 쌍 추출"""
        field_value_pairs = []
        
        try:
            # 번호가 있는 필드 패턴
            numbered_field_pattern = r'(\d{2})\s+([A-Z\s/()]+?)\s+([A-Z0-9\s:@.℃]+?)(?=\d{2}|\n\n|$)'
            matches = re.findall(numbered_field_pattern, text, re.MULTILINE)
            
            for match in matches:
                field_num, field_name, field_value = match
                field_value_pairs.append({
                    "field_number": field_num.strip(),
                    "field_name": field_name.strip(),
                    "field_value": field_value.strip(),
                    "type": "numbered_field"
                })
            
            print(f"🏷️ Extracted {len(field_value_pairs)} field-value pairs")
            
        except Exception as e:
            print(f"❌ Field-value extraction error: {e}")
        
        return field_value_pairs
    
    def _extract_notes_section(self, text):
        """NOTES 섹션 추출"""
        notes = []
        
        try:
            notes_pattern = r'NOTES?\s*:?\s*(.*?)(?=\n\n|\nREVISION|\nDATE|$)'
            notes_match = re.search(notes_pattern, text, re.DOTALL | re.IGNORECASE)
            
            if notes_match:
                notes_text = notes_match.group(1).strip()
                if notes_text:
                    notes.append({
                        "note_text": notes_text,
                        "type": "general_note"
                    })
            
            print(f"📝 Extracted {len(notes)} notes")
            
        except Exception as e:
            print(f"❌ Notes extraction error: {e}")
        
        return notes
    
    def _extract_contextual_entities(self, text, basic_patterns):
        """컨텍스트 기반 엔티티 추출 + 기본 패턴 변환"""
        entities = {}
        
        try:
            # 1. 기본 패턴을 엔티티로 변환 (핵심 수정!)
            if "project_ids" in basic_patterns:
                entities["Project"] = basic_patterns["project_ids"][:10]
            
            if "equipment_ids" in basic_patterns:
                entities["Equipment"] = basic_patterns["equipment_ids"][:10]
            
            if "person_names" in basic_patterns:
                entities["Person"] = basic_patterns["person_names"][:10]
            
            if "dates" in basic_patterns:
                entities["Date"] = basic_patterns["dates"][:10]
            
            if "revision_numbers" in basic_patterns:
                entities["Revision"] = basic_patterns["revision_numbers"][:10]
            
            # ProcessRequirement 엔티티 추가
            process_entities = []
            
            # 수치 값들을 ProcessRequirement로 분류
            if "numerical_values" in basic_patterns:
                for num_tuple in basic_patterns["numerical_values"][:20]:
                    if isinstance(num_tuple, tuple) and len(num_tuple) >= 3:
                        value, unit, unit_type = num_tuple
                        if unit:
                            process_entities.append(f"{value} {unit}")
                        else:
                            process_entities.append(value)
            
            # 프로세스 용어들
            if "process_terms" in basic_patterns:
                for term_tuple in basic_patterns["process_terms"]:
                    if isinstance(term_tuple, tuple):
                        term, category = term_tuple
                        process_entities.append(term)
            
            if process_entities:
                entities["ProcessRequirement"] = process_entities[:15]
            
            # 2. 추가 컨텍스트 기반 엔티티 (기존 로직)
            sentences = re.split(r'[.!?\n]+', text)
            
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) < 5:
                    continue
                
                for class_name, keywords in self.domain_keywords.items():
                    if any(keyword.lower() in sentence.lower() for keyword in keywords):
                        if class_name not in entities:
                            entities[class_name] = []
                        
                        specific_entities = self._extract_specific_entities(sentence, class_name)
                        entities[class_name].extend(specific_entities)
            
            # 3. 중복 제거 및 정리
            for class_name in entities:
                entities[class_name] = list(set(entities[class_name]))[:15]  # 최대 15개로 제한
                
        except Exception as e:
            print(f"❌ Contextual entity extraction error: {e}")
            entities = {}
        
        return entities
    
    def _extract_specific_entities(self, sentence, class_name):
        """특정 클래스 엔티티 추출"""
        entities = []
        
        try:
            if class_name == "Equipment":
                equipment_types = re.findall(r'\b(CENTRIFUGAL|PUMP|MOTOR|DRIVER|COMPRESSOR|VESSEL|TANK|HORIZONTAL)\b', sentence.upper())
                entities.extend(equipment_types)
                
            elif class_name == "ProcessRequirement":
                process_vars = re.findall(r'\b(temperature|pressure|flow|capacity|viscosity|density|npsh|head|suction|discharge)\b', sentence.lower())
                entities.extend(process_vars)
                
                # 단위가 있는 수치값
                for unit_type, units in self.unit_patterns.items():
                    for unit in units:
                        pattern = rf'(\d+\.?\d*\s*{re.escape(unit)})'
                        matches = re.findall(pattern, sentence, re.IGNORECASE)
                        entities.extend(matches)
            
            elif class_name == "Person":
                if any(word in sentence.lower() for word in ['by', 'checked', 'reviewed', 'approved']):
                    names = re.findall(r'\b([A-Z]{2,4})\b', sentence)
                    entities.extend(names)
                    
        except Exception as e:
            print(f"❌ Specific entity extraction error for {class_name}: {e}")
        
        return entities
    
    def _extract_relations(self, text, entities):
        """관계 추출"""
        relations = []
        
        try:
            # 간단한 관계 패턴
            relation_patterns = {
                "hasEquipment": (r'(project|job).*?(equipment|pump|motor)', "Project", "Equipment"),
                "hasProcessReq": (r'(equipment|pump).*?(temperature|pressure|flow)', "Equipment", "ProcessRequirement"),
                "reviewedBy": (r'(revision|document).*?(?:by|checked|reviewed).*?([A-Z]{2,4})', "Revision", "Person"),
            }
            
            for relation_name, (pattern, domain_class, range_class) in relation_patterns.items():
                matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    if isinstance(match, tuple) and len(match) >= 2:
                        relations.append({
                            "source": match[0],
                            "target": match[1],
                            "relation": relation_name
                        })
                        
        except Exception as e:
            print(f"❌ Relation extraction error: {e}")
        
        return relations
    
    def _calculate_confidence(self, patterns, entities):
        """신뢰도 계산"""
        confidence_scores = {}
        
        try:
            for class_name, entity_list in entities.items():
                for entity in entity_list:
                    confidence = 0.8  # 기본 신뢰도
                    confidence_scores[entity] = confidence
                    
        except Exception as e:
            print(f"❌ Confidence calculation error: {e}")
        
        return confidence_scores
    
    def _match_to_ontology(self, entities):
        """온톨로지 매칭"""
        matches = {}
        return matches
    
    def _generate_domain_insights(self, text):
        """도메인 인사이트 생성"""
        insights = {
            "document_type": "technical_specification",
            "industry_domain": "process_engineering",
            "complexity_score": min(len(text) / 1000, 5.0),
            "technical_density": len(re.findall(r'\d+\.?\d*\s*[a-zA-Z/%]+', text)) / max(len(text.split()), 1)
        }
        return insights
    
    def _update_learned_knowledge(self, patterns, entities, relations):
        """학습 지식 업데이트"""
        try:
            print(f"📚 Knowledge updated: {len(patterns)} pattern types, "
                  f"{sum(len(entity_list) for entity_list in entities.values())} entity instances")
        except Exception as e:
            print(f"❌ Knowledge update error: {e}")

def classify_ocr_text(text):
    """OCR 텍스트를 온톨로지 클래스로 분류"""
    try:
        text = text.strip().upper()
        
        # 숫자 패턴
        if re.match(r'^\d+\.?\d*$', text):
            return "Dimension"
        
        # 좌표/측정값 패턴
        if re.match(r'^\d+(\.\d+)?[A-Z]*$', text):
            return "Value"
        
        # 라벨 패턴 (LT18, MH2 등)
        if re.match(r'^[A-Z]{1,3}\d+$', text):
            return "Label"
        
        # 부품명 패턴
        if any(word in text for word in ['BREAKER', 'PUMP', 'VALVE', 'TANK', 'PIPE']):
            return "Component"
        
        # 기본값
        return "Text"
    except:
        return "Unknown"

def integrate_ocr_results(learning_results, ocr_results):
    """OCR 결과를 학습 결과에 통합"""
    try:
        # OCR 엔티티를 기존 엔티티에 추가
        ocr_entities = {}
        for ocr_item in ocr_results:
            ocr_class = classify_ocr_text(ocr_item['text'])
            if ocr_class not in ocr_entities:
                ocr_entities[ocr_class] = []
            ocr_entities[ocr_class].append(ocr_item['text'])
        
        # 기존 엔티티와 병합
        new_entities = learning_results.get("new_entities", {})
        for class_name, entity_list in ocr_entities.items():
            if class_name in new_entities:
                new_entities[class_name].extend(entity_list)
            else:
                new_entities[class_name] = entity_list
        
        learning_results["new_entities"] = new_entities
        
        # OCR 신뢰도를 전체 신뢰도에 추가
        confidence_scores = learning_results.get("confidence_scores", {})
        for ocr_item in ocr_results:
            confidence_scores[f"ocr_{ocr_item['text']}"] = ocr_item['confidence']
        
        learning_results["confidence_scores"] = confidence_scores
        
        print(f"✅ OCR 통합 완료: {sum(len(v) for v in ocr_entities.values())}개 OCR 엔티티 추가")
        return learning_results
        
    except Exception as e:
        print(f"❌ OCR 통합 실패: {e}")
        return learning_results

def detect_spatial_relations(ocr_results):
    """OCR 결과에서 공간적 관계 감지"""
    relations = []
    
    try:
        # 위치 기반 관계 감지
        for i, item1 in enumerate(ocr_results):
            for j, item2 in enumerate(ocr_results[i+1:], i+1):
                if item1.get('page_number') == item2.get('page_number'):
                    bbox1 = item1.get('bbox', {})
                    bbox2 = item2.get('bbox', {})
                    
                    if bbox1 and bbox2:
                        # 수직 관계 (위/아래)
                        if abs(bbox1.get('x', 0) - bbox2.get('x', 0)) < 50:  # 비슷한 x 좌표
                            if bbox1.get('y', 0) < bbox2.get('y', 0):
                                relations.append({
                                    'source_id': i,
                                    'target_id': j,
                                    'source_text': item1['text'],
                                    'target_text': item2['text'],
                                    'relation_type': 'above'
                                })
                        
                        # 수평 관계 (좌/우)
                        if abs(bbox1.get('y', 0) - bbox2.get('y', 0)) < 20:  # 비슷한 y 좌표
                            if bbox1.get('x', 0) < bbox2.get('x', 0):
                                relations.append({
                                    'source_id': i,
                                    'target_id': j,
                                    'source_text': item1['text'],
                                    'target_text': item2['text'],
                                    'relation_type': 'leftOf'
                                })
        
        print(f"🔗 공간 관계 감지: {len(relations)}개")
        return relations
        
    except Exception as e:
        print(f"❌ 공간 관계 감지 실패: {e}")
        return []

def process_pdf_to_graph_with_ttl(pdf_content, filename):
    """TTL 온톨로지 지식을 활용한 PDF 처리"""
    global ontology_learner  # 글로벌 변수 접근
    
    text = pdf_content['text'] if isinstance(pdf_content, dict) else pdf_content
    
    print(f"🧠 Processing with TTL-enhanced ontology learning...")
    
    # TTL 지식 활용 엔티티 추출
    entities, learning_results = ontology_learner.extract_entities_and_learn(text, filename)
    
    # ← 여기에 추가
    print(f"⚡ 기존 추출: {len(entities)}개 → 대폭 확장 중...")

    # 극강 분할: 모든 가능한 방법으로 분할
    all_words = []

    # 1. 기본 분할
    all_words.extend(text.split())

    # 2. 모든 비알파벳 문자로 분할
    import re
    all_words.extend(re.findall(r'[A-Za-z]+', text))
    all_words.extend(re.findall(r'\d+\.?\d*', text))

    # 3. 줄별 분할
    for line in text.split('\n'):
        all_words.extend(line.split())
        # 탭으로도 분할
        all_words.extend(line.split('\t'))

    # 4. 특수문자로 분할
    for char in '.,()[]{}:;"\'!?/-_|':
        temp_words = []
        for word in all_words:
            temp_words.extend(word.split(char))
        all_words.extend(temp_words)

    # 중복 제거
    all_words = list(set([w for w in all_words if w.strip()]))

    print(f"💥 극강 분할: {len(all_words)}개 단어 추출")


    for word in all_words:
        clean_word = word.strip('.,()[]{}:;"\'!?').strip()
        if len(clean_word) > 0:
            # 중복 체크
            existing = any(e['text'].lower() == clean_word.lower() for e in entities)
            if not existing:
                entities.append({
                    'text': clean_word,
                    'context': "",
                    'ontology_class': 'Other',
                    'confidence': 0.4,
                    'classification_method': 'word-split'
                })

    print(f"🚀 확장 완료: {len(entities)}개 엔티티")

    # 기존 단어 분할에 추가
    all_words = text.split()  # 기존

    # 추가 분할 방법들
    import re
    # 1. 모든 문자와 숫자 조합
    alpha_num = re.findall(r'[A-Za-z0-9]+', text)
    # 2. 특수문자로 분할
    special_split = re.split(r'[^\w]', text)
    # 3. 대문자로 시작하는 단어들
    capital_words = re.findall(r'[A-Z][a-z]*', text)
    # 4. 숫자와 소수점
    numbers = re.findall(r'\d+\.?\d*', text)

    # 모든 추출 결과 합치기
    all_extractions = all_words + alpha_num + special_split + capital_words + numbers

    # 필터링 후 엔티티 추가
    for word in all_extractions:
        clean_word = word.strip('.,()[]{}:;"\'!?').strip()
        if len(clean_word) > 0:
            # 중복 체크 후 추가
            existing = any(e['text'].lower() == clean_word.lower() for e in entities)
            if not existing:
                entities.append({
                    'text': clean_word,
                    'context': "",
                    'ontology_class': 'Other',
                    'confidence': 0.4,
                    'classification_method': 'enhanced-split'
                })

    # TTL 기반 분류 적용
    enhanced_entities = []
    ttl_classifications = 0
    
    for entity in entities:
        entity_text = entity['text']
        context = entity.get('context', '')
        
        # TTL 지식 활용 분류
        if hasattr(ontology_learner, 'classify_entity_with_ttl'):
            ttl_class, ttl_confidence, ttl_reason = ontology_learner.classify_entity_with_ttl(entity_text, context)
            
            if ttl_confidence > entity.get('confidence', 0):
                entity['ontology_class'] = ttl_class
                entity['confidence'] = ttl_confidence
                entity['classification_method'] = 'TTL-enhanced'
                entity['classification_reason'] = ttl_reason
                ttl_classifications += 1
            else:
                entity['classification_method'] = 'rule-based'
        
        enhanced_entities.append(entity)
    
    print(f"✅ TTL-enhanced classification: {ttl_classifications}/{len(entities)} entities")
    
    # 그래프 생성
    G = nx.DiGraph()
    
    # 노드 추가 (TTL 정보 포함)
    node_features = {}
    predictions = []
    
    for i, entity in enumerate(enhanced_entities):
        node_id = entity['text']
        
        # TTL 클래스 매핑
        ttl_class = entity.get('ontology_class', 'Other')
        confidence = entity.get('confidence', 0.5)
        
        G.add_node(node_id)
        node_features[node_id] = {
            'id': node_id,
            'type': 'entity',
            'ontology_class': ttl_class,
            'confidence': confidence,
            'classification_method': entity.get('classification_method', 'unknown'),
            'ttl_enhanced': entity.get('classification_method') == 'TTL-enhanced'
        }
        
        # 예측값 (TTL 클래스를 숫자로 매핑)
        class_mapping = {
            'Equipment': 1, 'ProcessRequirement': 1, 'Project': 1,
            'Note': 2, 'Other': 2, 'Revision': 2,
            'Person': 0, 'FeedType': 0, 'ConditionType': 0
        }
        predictions.append(class_mapping.get(ttl_class, 2))
    
    # TTL 기반 관계 추출
    if hasattr(ontology_learner, 'ttl_object_properties'):
        ttl_relations = ontology_learner.ttl_object_properties
        print(f"🔗 Using TTL relations: {ttl_relations}")
        
        # 간단한 관계 추출 (키워드 기반)
        relation_patterns = {
            'hasEquipment': ['pump', 'motor', 'equipment'],
            'hasProcessReq': ['temperature', 'pressure', 'capacity'],
            'hasNote': ['note', 'remark'],
            'hasRevision': ['revision', 'rev'],
            'reviewedBy': ['checked', 'reviewed', 'by']
        }
        
        nodes = list(G.nodes())
        for i, node1 in enumerate(nodes):
            for j, node2 in enumerate(nodes[i+1:], i+1):
                # TTL 관계 패턴 매칭
                for relation, keywords in relation_patterns.items():
                    if any(keyword in node1.lower() or keyword in node2.lower() for keyword in keywords):
                        if relation in ttl_relations:  # TTL에 정의된 관계만 사용
                            G.add_edge(node1, node2, relation=relation, source='TTL-pattern')
                            break
    
    # 학습 결과에 TTL 정보 추가
    learning_results['ttl_enhanced'] = True
    learning_results['ttl_classifications'] = ttl_classifications
    learning_results['ttl_relations_used'] = len([1 for _, _, data in G.edges(data=True) if data.get('source') == 'TTL-pattern'])
    
    if hasattr(ontology_learner, 'get_enhanced_statistics'):
        learning_results['ttl_statistics'] = ontology_learner.get_enhanced_statistics()
    
    print(f"🕸️ TTL-enhanced graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
    
    return G, node_features, predictions, learning_results


def process_pdf_to_graph(content, filename):
    """PDF 처리 - TTL 지원 확인 후 적절한 방법 선택"""
    global ontology_learner  # 글로벌 변수 접근
    
    if hasattr(ontology_learner, 'classify_entity_with_ttl'):
        return process_pdf_to_graph_with_ttl(content, filename)
    else:
        # 기존 로직 실행
        try:
            # 입력 데이터 타입 확인 및 처리
            if isinstance(content, str):
                # 기존 방식: 텍스트만 전달된 경우
                text_content = content
                ocr_results = []
                pdf_info = {'pdf_type': 'text', 'has_images': False}
                source_type = 'text'
            elif isinstance(content, dict):
                # 새로운 방식: OCR 결과 포함된 경우
                text_content = content.get('text', '')
                ocr_results = content.get('ocr_results', [])
                pdf_info = content.get('pdf_info', {})
                source_type = pdf_info.get('pdf_type', 'text')
            else:
                raise ValueError("Invalid content type")
            
            # 온톨로지 학습 및 그래프 생성 (글로벌 ontology_learner 사용)
            learning_results = ontology_learner.learn_from_text(text_content)
            
            # OCR 결과를 엔티티로 추가 처리
            if ocr_results:
                print(f"🔍 OCR 데이터를 그래프에 통합 중...")
                learning_results = integrate_ocr_results(learning_results, ocr_results)
            
            # 네트워크 그래프 생성
            G = nx.Graph()
            node_features = {}
            predictions = []
            entity_counter = 0  # 핵심 수정: entity_counter 초기화
            
            # 엔티티 수집 및 통합
            entities = {}

            # 기본 패턴에서 엔티티 추가
            patterns = learning_results.get("patterns", {})
            for pattern_type, pattern_list in patterns.items():
                if pattern_type == "project_ids":
                    entities["Project"] = pattern_list[:10]  # 제한 추가
                elif pattern_type == "equipment_ids": 
                    entities["Equipment"] = pattern_list[:10]
                elif pattern_type == "person_names":
                    entities["Person"] = pattern_list[:10]
                elif pattern_type == "dates":
                    entities["Date"] = pattern_list[:10]
                elif pattern_type == "revision_numbers":
                    entities["Revision"] = pattern_list[:10]
                elif pattern_type == "numerical_values":
                    # 수치값들을 ProcessRequirement로 분류
                    numerical_entities = []
                    for num_tuple in pattern_list[:20]:
                        if isinstance(num_tuple, tuple) and len(num_tuple) >= 3:
                            value, unit, unit_type = num_tuple
                            if unit:
                                numerical_entities.append(f"{value} {unit}")
                            else:
                                numerical_entities.append(str(value))
                    if numerical_entities:
                        entities["ProcessRequirement"] = numerical_entities
                elif pattern_type == "process_terms":
                    # 프로세스 용어들
                    process_entities = []
                    for term_tuple in pattern_list:
                        if isinstance(term_tuple, tuple):
                            term, category = term_tuple
                            process_entities.append(term)
                    if process_entities:
                        if "ProcessRequirement" in entities:
                            entities["ProcessRequirement"].extend(process_entities[:10])
                        else:
                            entities["ProcessRequirement"] = process_entities[:10]

            # 필드-값 쌍 추가
            field_value_pairs = learning_results.get("field_value_pairs", [])
            field_entities = [pair.get("field_name", "") for pair in field_value_pairs if pair.get("field_name", "").strip()]
            if field_entities:
                entities["Field"] = field_entities[:10]

            # 컨텍스트 엔티티도 추가 (중복 방지)
            contextual_entities = learning_results.get("new_entities", {})
            for class_name, entity_list in contextual_entities.items():
                if class_name in entities:
                    # 기존 엔티티와 중복 제거
                    existing = set(entities[class_name])
                    new_entities = [e for e in entity_list if e not in existing]
                    entities[class_name].extend(new_entities[:5])  # 추가 제한
                else:
                    entities[class_name] = entity_list[:10]
            
            # 엔티티를 노드로 변환
            print(f"🔗 엔티티 → 노드 변환: {sum(len(v) for v in entities.values())}개 엔티티")
            
            for class_name, entity_list in entities.items():
                for entity in entity_list:
                    if not entity or not str(entity).strip():  # 빈 엔티티 건너뛰기
                        continue
                        
                    # 안전한 노드 ID 생성
                    safe_entity = str(entity).replace(' ', '_').replace('/', '_').replace('\\', '_')[:50]
                    node_id = f"{class_name}_{safe_entity}_{entity_counter}"
                    entity_counter += 1
                    
                    G.add_node(node_id)
                    node_features[node_id] = {
                        "type": "entity",
                        "ontology_class": class_name,
                        "label": str(entity),
                        "confidence": 0.8,
                        "source_type": "text"
                    }
                    
                    # 규칙 기반 예측
                    if class_name == "Document":
                        predictions.append(0)
                    elif class_name in ["Entity", "Person", "Location", "Equipment"]:
                        predictions.append(1)
                    else:
                        predictions.append(2)
            
            # OCR 엔티티 추가
            for ocr_item in ocr_results:
                if not ocr_item.get('text', '').strip():  # 빈 텍스트 건너뛰기
                    continue
                    
                safe_text = ocr_item['text'].replace(' ', '_').replace('/', '_').replace('\\', '_')[:50]
                node_id = f"OCR_{safe_text}_{entity_counter}"
                entity_counter += 1
                
                # OCR 텍스트 분류
                ocr_class = classify_ocr_text(ocr_item['text'])
                
                G.add_node(node_id)
                node_features[node_id] = {
                    "type": "entity",
                    "ontology_class": ocr_class,
                    "label": ocr_item['text'],
                    "confidence": ocr_item.get('confidence', 0.5),
                    "source_type": "ocr",
                    "bbox": ocr_item.get('bbox', {}),
                    "page_number": ocr_item.get('page_number', 1)
                }
                
                # OCR 기반 예측
                if ocr_class in ["Dimension", "Value"]:
                    predictions.append(0)
                elif ocr_class in ["Label", "Component"]:
                    predictions.append(1)
                else:
                    predictions.append(2)
            
            # 관계 추가
            relations = learning_results.get("new_relations", [])
            edge_count = 0
            for relation in relations:
                source = relation.get("source", "")
                target = relation.get("target", "")
                relation_type = relation.get("relation", "unknown")
                
                if not source or not target:  # 빈 관계 건너뛰기
                    continue
                
                # 노드 매칭
                source_nodes = [n for n in G.nodes() if source.lower() in node_features[n]["label"].lower()]
                target_nodes = [n for n in G.nodes() if target.lower() in node_features[n]["label"].lower()]
                
                for s_node in source_nodes[:1]:  # 첫 번째 매치만 사용
                    for t_node in target_nodes[:1]:
                        if s_node != t_node:
                            G.add_edge(s_node, t_node, relation=relation_type)
                            edge_count += 1
            
            # OCR 기반 공간 관계 추가
            if ocr_results:
                spatial_relations = detect_spatial_relations(ocr_results)
                for relation in spatial_relations:
                    source_text = relation.get('source_text', '')
                    target_text = relation.get('target_text', '')
                    relation_type = relation.get('relation_type', 'spatial')
                    
                    if not source_text or not target_text:
                        continue
                    
                    # 해당하는 노드 찾기
                    source_nodes = [n for n in G.nodes() if source_text in node_features[n]["label"]]
                    target_nodes = [n for n in G.nodes() if target_text in node_features[n]["label"]]
                    
                    for s_node in source_nodes[:1]:
                        for t_node in target_nodes[:1]:
                            if s_node != t_node:
                                G.add_edge(s_node, t_node, relation=relation_type)
                                edge_count += 1
            
            print(f"✅ 그래프 생성 완료: {G.number_of_nodes()}개 노드, {G.number_of_edges()}개 엣지")
            print(f"📊 소스별 노드: 텍스트 {len([n for n in G.nodes() if node_features[n]['source_type'] == 'text'])}개, OCR {len([n for n in G.nodes() if node_features[n]['source_type'] == 'ocr'])}개")
            
            return G, node_features, predictions, learning_results
            
        except Exception as e:
            print(f"❌ 그래프 생성 실패: {e}")
            import traceback
            traceback.print_exc()
            
            # 빈 그래프 반환
            G = nx.Graph()
            empty_node_features = {}
            empty_predictions = []
            empty_learning_results = {
                "patterns": {},
                "confidence_scores": {},
                "new_entities": {},
                "new_relations": [],
                "field_value_pairs": [],
                "notes": [],
                "domain_insights": {}
            }
            return G, empty_node_features, empty_predictions, empty_learning_results

        
def classify_ocr_text(text):
    """OCR 텍스트를 온톨로지 클래스로 분류"""
    text = text.strip().upper()
    
    # 숫자 패턴
    if re.match(r'^\d+\.?\d*$', text):
        return "Dimension"
    
    # 좌표/측정값 패턴
    if re.match(r'^\d+(\.\d+)?[A-Z]*$', text):
        return "Value"
    
    # 라벨 패턴 (LT18, MH2 등)
    if re.match(r'^[A-Z]{1,3}\d+$', text):
        return "Label"
    
    # 부품명 패턴
    if any(word in text for word in ['BREAKER', 'PUMP', 'VALVE', 'TANK', 'PIPE']):
        return "Component"
    
    # 기본값
    return "Text"

def integrate_ocr_results(learning_results, ocr_results):
    """OCR 결과를 학습 결과에 통합"""
    try:
        # OCR 엔티티를 기존 엔티티에 추가
        ocr_entities = {}
        for ocr_item in ocr_results:
            ocr_class = classify_ocr_text(ocr_item['text'])
            if ocr_class not in ocr_entities:
                ocr_entities[ocr_class] = []
            ocr_entities[ocr_class].append(ocr_item['text'])
        
        # 기존 엔티티와 병합
        new_entities = learning_results.get("new_entities", {})
        for class_name, entity_list in ocr_entities.items():
            if class_name in new_entities:
                new_entities[class_name].extend(entity_list)
            else:
                new_entities[class_name] = entity_list
        
        learning_results["new_entities"] = new_entities
        
        # OCR 신뢰도를 전체 신뢰도에 추가
        confidence_scores = learning_results.get("confidence_scores", {})
        for ocr_item in ocr_results:
            confidence_scores[f"ocr_{ocr_item['text']}"] = ocr_item['confidence']
        
        learning_results["confidence_scores"] = confidence_scores
        
        print(f"✅ OCR 통합 완료: {sum(len(v) for v in ocr_entities.values())}개 OCR 엔티티 추가")
        return learning_results
        
    except Exception as e:
        print(f"❌ OCR 통합 실패: {e}")
        return learning_results

def detect_spatial_relations(ocr_results):
    """OCR 결과에서 공간적 관계 감지"""
    relations = []
    
    try:
        # 위치 기반 관계 감지 (간단한 예시)
        for i, item1 in enumerate(ocr_results):
            for j, item2 in enumerate(ocr_results[i+1:], i+1):
                if item1.get('page_number') == item2.get('page_number'):
                    bbox1 = item1.get('bbox', {})
                    bbox2 = item2.get('bbox', {})
                    
                    if bbox1 and bbox2:
                        # 수직 관계 (위/아래)
                        if abs(bbox1.get('x', 0) - bbox2.get('x', 0)) < 50:  # 비슷한 x 좌표
                            if bbox1.get('y', 0) < bbox2.get('y', 0):
                                relations.append({
                                    'source_id': i,
                                    'target_id': j,
                                    'source_text': item1['text'],
                                    'target_text': item2['text'],
                                    'relation_type': 'above'
                                })
                        
                        # 수평 관계 (좌/우)
                        if abs(bbox1.get('y', 0) - bbox2.get('y', 0)) < 20:  # 비슷한 y 좌표
                            if bbox1.get('x', 0) < bbox2.get('x', 0):
                                relations.append({
                                    'source_id': i,
                                    'target_id': j,
                                    'source_text': item1['text'],
                                    'target_text': item2['text'],
                                    'relation_type': 'leftOf'
                                })
        
        print(f"🔗 공간 관계 감지: {len(relations)}개")
        return relations
        
    except Exception as e:
        print(f"❌ 공간 관계 감지 실패: {e}")
        return []
    
    
# 샘플 데이터 생성 함수
def create_sample_graph():
    G = nx.DiGraph()
    
    nodes = [
        ("Project_7T04", {"node_type": "entity", "ontology_class": "Project"}),
        ("Equipment_P2105", {"node_type": "entity", "ontology_class": "Equipment"}), 
        ("ProcessReq_Temp", {"node_type": "entity", "ontology_class": "ProcessRequirement"}),
        ("7T04", {"node_type": "literal", "ontology_class": "string"}),
        ("P-2105 A/B", {"node_type": "literal", "ontology_class": "string"}),
        ("384 ℃", {"node_type": "literal", "ontology_class": "decimal"})
    ]
    
    for node_id, attrs in nodes:
        G.add_node(node_id, **attrs)
    
    edges = [
        ("Project_7T04", "7T04", "hasValue"),
        ("Equipment_P2105", "P-2105 A/B", "hasValue"),
        ("ProcessReq_Temp", "384 ℃", "hasValue"),
        ("Project_7T04", "Equipment_P2105", "hasEquipment"),
        ("Equipment_P2105", "ProcessReq_Temp", "hasProcessReq")
    ]
    
    for src, dst, rel in edges:
        G.add_edge(src, dst, relation=rel)
    
    node_features = {}
    for node in G.nodes():
        node_data = G.nodes[node]
        node_features[node] = {
            "type": node_data.get("node_type", "entity"),
            "ontology_class": node_data.get("ontology_class", "Unknown"),
            "confidence": 0.8 if node_data.get("node_type") == "entity" else 0.9
        }
    
    predictions = []
    for node in G.nodes():
        node_type = node_features[node]["type"]
        if node_type == "entity":
            predictions.append(1)
        elif node_type == "literal":
            predictions.append(2)
        else:
            predictions.append(0)
    
    return G, node_features, predictions

# ontology.ttl 학습 기능 추가
from rdflib import Graph, Namespace, RDF, RDFS, OWL
import os

class EnhancedOntologyLearner:
    def __init__(self, ttl_file_path="ontology.ttl"):
        """온톨로지 TTL 파일을 학습하는 고급 온톨로지 학습기"""
        self.ttl_file_path = ttl_file_path
        self.ontology_graph = Graph()
        self.ex_namespace = None
        
        # TTL에서 학습한 구조
        self.ttl_classes = []
        self.ttl_object_properties = []
        self.ttl_datatype_properties = []
        self.ttl_instances = {}
        self.ttl_class_hierarchy = {}
        
        # 기존 하드코딩된 구조 (fallback용)
        self.default_ontology_classes = [
            "Equipment", "ProcessRequirement", "Field", "Note", 
            "Project", "Person", "Literal", "Other", "Revision", 
            "FeedType", "ConditionType"
        ]
        
        self.default_object_properties = [
            "hasEquipment", "hasProcessReq", "hasOther", "hasRevision",
            "hasNote", "hasFeedType", "hasCondition", "reviewedBy"
        ]
        
        self.default_datatype_properties = [
            "jobNo", "projectName", "docNo", "itemNo", "client", "service",
            "pumpType", "driverType", "value", "conditionType", "noteText"
        ]
        
        # TTL 파일 로드 및 학습
        self.load_ontology()
        
        # 최종 온톨로지 구조 (TTL + 기본)
        self.ontology_classes = self.merge_ontology_structures()
        self.object_properties = self.merge_object_properties()
        self.datatype_properties = self.merge_datatype_properties()
        
        print(f"✅ Enhanced ontology loaded: {len(self.ontology_classes)} classes, {len(self.object_properties)} object properties")
    
    def load_ontology(self):
        """ontology.ttl 파일을 로드하고 구조를 추출"""
        if not os.path.exists(self.ttl_file_path):
            print(f"⚠️ TTL file not found: {self.ttl_file_path}. Using default ontology.")
            return
        
        try:
            # TTL 파일 파싱
            self.ontology_graph.parse(self.ttl_file_path, format="turtle")
            print(f"✅ TTL file loaded: {len(self.ontology_graph)} triples")
            
            # 네임스페이스 설정
            self.ex_namespace = Namespace("http://example.org/plant#")
            
            # 온톨로지 구조 추출
            self._extract_classes()
            self._extract_object_properties()
            self._extract_datatype_properties()
            self._extract_instances()
            self._extract_class_hierarchy()
            
            print(f"📚 TTL Analysis Complete:")
            print(f"   🏷️ Classes: {len(self.ttl_classes)}")
            print(f"   🔗 Object Properties: {len(self.ttl_object_properties)}")
            print(f"   📝 Datatype Properties: {len(self.ttl_datatype_properties)}")
            print(f"   📦 Instances: {sum(len(v) for v in self.ttl_instances.values())}")
            
        except Exception as e:
            print(f"❌ Error loading TTL file: {e}")
            print("🔄 Falling back to default ontology")
    
    def _extract_classes(self):
        """TTL에서 클래스들을 추출"""
        classes = set()
        
        # owl:Class로 정의된 클래스들
        for subj, pred, obj in self.ontology_graph.triples((None, RDF.type, OWL.Class)):
            class_name = self._get_local_name(subj)
            if class_name:
                classes.add(class_name)
        
        self.ttl_classes = sorted(list(classes))
        print(f"   📋 Classes found: {self.ttl_classes}")
    
    def _extract_object_properties(self):
        """TTL에서 객체 속성들을 추출"""
        properties = set()
        
        # owl:ObjectProperty로 정의된 속성들
        for subj, pred, obj in self.ontology_graph.triples((None, RDF.type, OWL.ObjectProperty)):
            prop_name = self._get_local_name(subj)
            if prop_name:
                properties.add(prop_name)
        
        self.ttl_object_properties = sorted(list(properties))
        print(f"   🔗 Object Properties: {self.ttl_object_properties}")
    
    def _extract_datatype_properties(self):
        """TTL에서 데이터타입 속성들을 추출"""
        properties = set()
        
        # owl:DatatypeProperty로 정의된 속성들
        for subj, pred, obj in self.ontology_graph.triples((None, RDF.type, OWL.DatatypeProperty)):
            prop_name = self._get_local_name(subj)
            if prop_name:
                properties.add(prop_name)
        
        self.ttl_datatype_properties = sorted(list(properties))
        print(f"   📝 Datatype Properties: {self.ttl_datatype_properties}")
    
    def _extract_instances(self):
        """TTL에서 인스턴스들을 추출"""
        instances = {}
        
        for class_name in self.ttl_classes:
            class_uri = self.ex_namespace[class_name]
            class_instances = []
            
            # 해당 클래스의 인스턴스들 찾기
            for subj, pred, obj in self.ontology_graph.triples((None, RDF.type, class_uri)):
                instance_name = self._get_local_name(subj)
                if instance_name:
                    class_instances.append(instance_name)
            
            if class_instances:
                instances[class_name] = sorted(class_instances)
        
        self.ttl_instances = instances
        print(f"   📦 Instances by class: {[(k, len(v)) for k, v in instances.items()]}")
    
    def _extract_class_hierarchy(self):
        """TTL에서 클래스 계층구조를 추출"""
        hierarchy = {}
        
        # rdfs:domain과 rdfs:range 관계 분석
        for subj, pred, obj in self.ontology_graph.triples((None, RDFS.domain, None)):
            prop_name = self._get_local_name(subj)
            domain_class = self._get_local_name(obj)
            
            if prop_name and domain_class:
                if domain_class not in hierarchy:
                    hierarchy[domain_class] = {"properties": [], "ranges": []}
                hierarchy[domain_class]["properties"].append(prop_name)
        
        for subj, pred, obj in self.ontology_graph.triples((None, RDFS.range, None)):
            prop_name = self._get_local_name(subj)
            range_class = self._get_local_name(obj)
            
            if prop_name and range_class:
                # domain을 찾아서 연결
                for domain_subj, domain_pred, domain_obj in self.ontology_graph.triples((subj, RDFS.domain, None)):
                    domain_class = self._get_local_name(domain_obj)
                    if domain_class and domain_class in hierarchy:
                        hierarchy[domain_class]["ranges"].append(range_class)
        
        self.ttl_class_hierarchy = hierarchy
        print(f"   🌳 Class hierarchy: {len(hierarchy)} classes with relationships")
    
    def _get_local_name(self, uri):
        """URI에서 로컬 이름 추출"""
        try:
            if hasattr(uri, 'split'):
                return str(uri).split('#')[-1].split('/')[-1]
            return str(uri).split('#')[-1].split('/')[-1]
        except:
            return None
    
    def merge_ontology_structures(self):
        """TTL과 기본 온톨로지 구조를 병합"""
        merged_classes = set(self.default_ontology_classes)
        merged_classes.update(self.ttl_classes)
        return sorted(list(merged_classes))
    
    def merge_object_properties(self):
        """TTL과 기본 객체 속성을 병합"""
        merged_props = set(self.default_object_properties)
        merged_props.update(self.ttl_object_properties)
        return sorted(list(merged_props))
    
    def merge_datatype_properties(self):
        """TTL과 기본 데이터타입 속성을 병합"""
        merged_props = set(self.default_datatype_properties)
        merged_props.update(self.ttl_datatype_properties)
        return sorted(list(merged_props))
    
    def get_ttl_knowledge(self):
        """TTL에서 학습한 지식 반환"""
        return {
            "classes": self.ttl_classes,
            "object_properties": self.ttl_object_properties,
            "datatype_properties": self.ttl_datatype_properties,
            "instances": self.ttl_instances,
            "class_hierarchy": self.ttl_class_hierarchy,
            "total_triples": len(self.ontology_graph)
        }
    
    def classify_entity_with_ttl(self, entity_text, context=""):
        """TTL 지식을 활용한 엔티티 분류"""
        entity_lower = entity_text.lower()
        
        # 1. TTL 인스턴스와 직접 매칭
        for class_name, instances in self.ttl_instances.items():
            for instance in instances:
                if instance.lower() in entity_lower or entity_lower in instance.lower():
                    return class_name, 0.95, f"TTL instance match: {instance}"
        
        # 2. TTL 데이터타입 속성과 매칭
        for prop in self.ttl_datatype_properties:
            if prop.lower() in entity_lower:
                # 속성의 도메인 클래스 찾기
                for class_name, hierarchy in self.ttl_class_hierarchy.items():
                    if prop in hierarchy.get("properties", []):
                        return class_name, 0.85, f"TTL property domain: {prop}"
        
        # 3. TTL 클래스 이름과 직접 매칭
        for class_name in self.ttl_classes:
            if class_name.lower() in entity_lower or entity_lower in class_name.lower():
                return class_name, 0.8, f"TTL class match: {class_name}"
        
        # 4. 기존 규칙 기반 분류 (fallback)
        return self._classify_with_rules(entity_text, context)
    
    def _classify_with_rules(self, entity_text, context=""):
        """기존 규칙 기반 분류 (fallback)"""
        entity_lower = entity_text.lower()
        
        # 기존 분류 로직...
        if any(keyword in entity_lower for keyword in ["pump", "motor", "compressor", "vessel"]):
            return "Equipment", 0.7, "Rule-based: equipment keyword"
        elif any(keyword in entity_lower for keyword in ["temperature", "pressure", "capacity", "flow"]):
            return "ProcessRequirement", 0.7, "Rule-based: process keyword"
        elif entity_lower.startswith("note"):
            return "Note", 0.8, "Rule-based: note prefix"
        elif any(keyword in entity_lower for keyword in ["project", "job", "doc"]):
            return "Project", 0.7, "Rule-based: project keyword"
        else:
            return "Other", 0.3, "Rule-based: default"
    
    def get_enhanced_statistics(self):
        """TTL 지식을 포함한 확장된 통계"""
        ttl_knowledge = self.get_ttl_knowledge()
        
        return {
            "total_classes": len(self.ontology_classes),
            "ttl_classes": len(self.ttl_classes),
            "default_classes": len(self.default_ontology_classes),
            "total_properties": len(self.object_properties) + len(self.datatype_properties),
            "ttl_triples": ttl_knowledge["total_triples"],
            "ttl_instances": sum(len(v) for v in self.ttl_instances.values()),
            "class_hierarchy_depth": len(self.ttl_class_hierarchy),
            "ontology_source": "TTL + Default" if self.ttl_classes else "Default only"
        }

    def extract_entities_and_learn(self, text, filename):
        """TTL 지식을 활용한 엔티티 추출 및 학습"""
        print(f"🧠 Processing with TTL-enhanced ontology learning...")
        
        # 기본 엔티티 추출 로직 (기존과 동일)
        entities = []
        confidence_scores = {}
        patterns = {}
        new_entities = {"Equipment": [], "ProcessRequirement": [], "Note": []}
        new_relations = []
        
        # 간단한 엔티티 추출
        lines = text.split('\n')
        for i, line in enumerate(lines):
            words = line.split()
            for j, word in enumerate(words):
                if len(word) > 3:  # 최소 길이 조건
                    # TTL 지식 활용 분류
                    ttl_class, ttl_confidence, ttl_reason = self.classify_entity_with_ttl(word, line)
                    
                    entity = {
                        'text': word,
                        'context': line,
                        'ontology_class': ttl_class,
                        'confidence': ttl_confidence,
                        'classification_method': 'TTL-enhanced' if ttl_confidence > 0.3 else 'rule-based',
                        'classification_reason': ttl_reason
                    }
                    entities.append(entity)
                    confidence_scores[word] = ttl_confidence
        
        # 학습 결과 구성
        learning_results = {
            "confidence_scores": confidence_scores,
            "patterns": patterns,
            "new_entities": new_entities,
            "new_relations": new_relations,
            "domain_insights": {
                "industry_domain": "Chemical Process",
                "complexity_score": 3.5,
                "technical_density": 0.8,
                "document_type": "Technical Specification"
            },
            "ttl_enhanced": True,
            "ttl_statistics": self.get_enhanced_statistics()
        }
        
        print(f"✅ TTL-enhanced entity extraction complete: {len(entities)} entities")
        return entities, learning_results

class RGCNManager:
    def __init__(self):
        """TTL 통합 R-GCN 매니저 초기화"""
        self.initialized = True
        self.device = "cpu"
        self.num_relations = 0
        print("🤖 TTL-enhanced RGCNManager initialized")
    
    def get_status(self):
        """R-GCN 상태 반환"""
        return {
            "initialized": self.initialized,
            "device": self.device,
            "num_relations": self.num_relations
        }

    def predict(self, graph_data, ttl_ontology=None):
        """TTL 온톨로지를 활용한 향상된 R-GCN 예측"""
        try:
            nodes = graph_data.get("nodes", [])
            edges = graph_data.get("edges", [])
            
            if not nodes:
                return None
            
            print(f"🔮 R-GCN TTL-enhanced prediction starting...")
            print(f"📊 Input: {len(nodes)} nodes, {len(edges)} edges")
            
            # TTL 온톨로지 정보 통합
            if ttl_ontology:
                print(f"🧠 TTL integration: {len(ttl_ontology.get('classes', []))} classes, {len(ttl_ontology.get('instances', {}))} instance groups")
            
            # 노드 특성 벡터 생성 (TTL 정보 포함)
            node_features = self._create_ttl_enhanced_features(nodes, ttl_ontology)
            
            # 엣지 정보 처리 (TTL 관계 포함)
            edge_features = self._process_ttl_relations(edges, ttl_ontology)
            
            # R-GCN 클러스터링 (TTL 제약 조건 적용)
            clusters = self._rgcn_clustering_with_ttl(node_features, edge_features, ttl_ontology)
            
            # TTL 지식 기반 후처리
            refined_predictions = self._refine_with_ttl_knowledge(clusters, nodes, ttl_ontology)
            
            print(f"✅ TTL-enhanced R-GCN prediction complete!")
            
            return {
                "predictions": refined_predictions,
                "confidence_scores": [pred.get("confidence", 0.5) for pred in refined_predictions],
                "class_distribution": self._calculate_ttl_distribution(refined_predictions),
                "average_confidence": sum(pred.get("confidence", 0.5) for pred in refined_predictions) / len(refined_predictions),
                "ttl_enhanced": True,
                "ttl_classes_used": len(ttl_ontology.get('classes', [])) if ttl_ontology else 0
            }
            
        except Exception as e:
            print(f"❌ TTL-enhanced R-GCN prediction failed: {e}")
            return None

    def _create_ttl_enhanced_features(self, nodes, ttl_ontology):
        """TTL 정보를 포함한 노드 특성 벡터 생성"""
        enhanced_features = []
        
        for node in nodes:
            node_id = node.get("id", "")
            ttl_class = node.get("ontology_class", "Other")
            ttl_confidence = node.get("confidence", 0.5)
            
            # 기본 특성
            features = {
                "text_length": len(node_id),
                "is_numeric": node_id.replace(".", "").isdigit(),
                "is_uppercase": node_id.isupper(),
                "has_special_chars": any(c in node_id for c in "-/_()"),
            }
            
            # TTL 온톨로지 특성 추가
            if ttl_ontology:
                # TTL 클래스 원핫 인코딩
                ttl_classes = ttl_ontology.get("classes", [])
                for cls in ttl_classes:
                    features[f"ttl_class_{cls}"] = 1.0 if ttl_class == cls else 0.0
                
                # TTL 인스턴스 매칭
                ttl_instances = ttl_ontology.get("instances", {})
                features["ttl_instance_match"] = 0.0
                for cls, instances in ttl_instances.items():
                    if any(inst.lower() in node_id.lower() for inst in instances):
                        features["ttl_instance_match"] = 1.0
                        features[f"ttl_matched_class_{cls}"] = 1.0
                        break
                
                # TTL 신뢰도
                features["ttl_confidence"] = ttl_confidence
            
            enhanced_features.append(features)
        
        return enhanced_features

    def _process_ttl_relations(self, edges, ttl_ontology):
        """TTL 관계를 R-GCN 엣지로 처리"""
        if not ttl_ontology:
            return edges
        
        ttl_relations = ttl_ontology.get("object_properties", [])
        processed_edges = []
        
        for edge in edges:
            relation = edge.get("relation", "unknown")
            
            # TTL 관계 타입 매핑
            if relation in ttl_relations:
                edge["ttl_relation"] = True
                edge["relation_weight"] = 1.0
            else:
                edge["ttl_relation"] = False
                edge["relation_weight"] = 0.5
            
            processed_edges.append(edge)
        
        return processed_edges

    def _rgcn_clustering_with_ttl(self, node_features, edge_features, ttl_ontology):
        """TTL 제약 조건을 적용한 R-GCN 클러스터링"""
        # 기존 클러스터링 로직 + TTL 제약
        
        # 1. TTL 클래스 기반 초기 클러스터링
        ttl_clusters = {}
        if ttl_ontology:
            ttl_classes = ttl_ontology.get("classes", [])
            for i, features in enumerate(node_features):
                # TTL 클래스 중 가장 높은 확률을 가진 것 선택
                best_class = "Other"
                best_score = 0.0
                
                for cls in ttl_classes:
                    score = features.get(f"ttl_class_{cls}", 0.0)
                    if score > best_score:
                        best_score = score
                        best_class = cls
                
                if best_class not in ttl_clusters:
                    ttl_clusters[best_class] = []
                ttl_clusters[best_class].append(i)
        
        # 2. 기존 R-GCN 클러스터링 결과와 결합
        base_clusters = self._original_clustering(node_features, edge_features)
        
        # 3. TTL 지식으로 클러스터 정제
        refined_clusters = self._merge_clusters_with_ttl(base_clusters, ttl_clusters, ttl_ontology)
        
        return refined_clusters

    def _refine_with_ttl_knowledge(self, clusters, nodes, ttl_ontology):
        """TTL 지식으로 예측 결과 정제"""
        refined_predictions = []
        
        for i, node in enumerate(nodes):
            node_id = node.get("id", "")
            cluster = clusters.get(i, "unknown")
            
            # TTL 기반 신뢰도 조정
            confidence = 0.5
            
            if ttl_ontology:
                # TTL 인스턴스와 매칭되면 신뢰도 증가
                ttl_instances = ttl_ontology.get("instances", {})
                for cls, instances in ttl_instances.items():
                    if any(inst.lower() in node_id.lower() for inst in instances):
                        confidence = min(0.95, confidence + 0.3)
                        cluster = self._map_ttl_class_to_importance(cls)
                        break
                
                # TTL 클래스 정보 활용
                ttl_class = node.get("ontology_class", "Other")
                if ttl_class != "Other":
                    confidence = min(0.9, confidence + 0.2)
                    cluster = self._map_ttl_class_to_importance(ttl_class)
            
            refined_predictions.append({
                "node_id": node_id,
                "predicted_cluster": cluster,
                "confidence": confidence,
                "ttl_enhanced": True,
                "original_ttl_class": node.get("ontology_class", "Other")
            })
        
        return refined_predictions

    def _map_ttl_class_to_importance(self, ttl_class):
        """TTL 클래스를 중요도로 매핑"""
        importance_mapping = {
            "Equipment": "high_importance",
            "ProcessRequirement": "high_importance", 
            "Project": "high_importance",
            "Note": "medium_importance",
            "Revision": "medium_importance",
            "Person": "medium_importance",
            "Other": "low_importance",
            "FeedType": "low_importance",
            "ConditionType": "low_importance"
        }
        return importance_mapping.get(ttl_class, "low_importance")

    def _calculate_ttl_distribution(self, predictions):
        """TTL 기반 클래스 분포 계산"""
        distribution = {}
        importance_to_class = {
            "high_importance": 0,
            "medium_importance": 1, 
            "low_importance": 2
        }
        
        for pred in predictions:
            cluster = pred.get("predicted_cluster", "low_importance")
            class_id = importance_to_class.get(cluster, 2)
            distribution[class_id] = distribution.get(class_id, 0) + 1
        
        return distribution

    def _original_clustering(self, node_features, edge_features):
        """기존 R-GCN 클러스터링 로직 (백업용)"""
        # 기존 클러스터링 로직을 여기에 보존
        clusters = {}
        for i in range(len(node_features)):
            clusters[i] = "medium_importance"  # 기본값
        return clusters

    def _merge_clusters_with_ttl(self, base_clusters, ttl_clusters, ttl_ontology):
        """기존 클러스터와 TTL 클러스터 병합"""
        merged = base_clusters.copy()
        
        # TTL 클러스터가 더 신뢰할 만하면 우선 적용
        for ttl_class, node_indices in ttl_clusters.items():
            importance = self._map_ttl_class_to_importance(ttl_class)
            for idx in node_indices:
                merged[idx] = importance
        
        return merged

class NaturalLanguageQueryProcessor:
    def __init__(self, ontology_learner, db_manager, rgcn_manager=None):
        self.ontology_learner = ontology_learner
        self.db_manager = db_manager
        self.rgcn_manager = rgcn_manager
        self._last_graph_data = None  # 메모리 데이터 저장용
        
        # 확장된 쿼리 패턴 매핑 (새로운 엔티티 타입 지원)
        self.query_patterns = {
            "show_all": [
                r"show\s+all\s+(\w+)",
                r"list\s+all\s+(\w+)",
                r"find\s+all\s+(\w+)",
                r"get\s+all\s+(\w+)"
            ],
            "filter_by_confidence": [
                r"(?:show|find|list)\s+(\w+)\s+with\s+(?:high|good)\s+confidence",
                r"(?:show|find|list)\s+high\s+confidence\s+(\w+)",
                r"(\w+)\s+with\s+confidence\s+>\s*(\d+\.?\d*)"
            ],
            "filter_by_class": [
                r"(?:show|find|list)\s+(\w+)\s+(?:of\s+type|class)\s+(\w+)",
                r"(\w+)\s+that\s+are\s+(\w+)",
                r"all\s+(\w+)\s+(\w+)"
            ],
            "count": [
                r"(?:how\s+many|count)\s+(\w+)",
                r"number\s+of\s+(\w+)",
                r"total\s+(\w+)"
            ],
            "stats": [
                r"(?:statistics|stats)\s+(?:for|of|about)?\s*(\w+)?",
                r"(?:summary|overview)\s+(?:of|about)?\s*(\w+)?",
                r"(?:analyze|analysis)\s+(\w+)?",
                r"database\s+(?:stats|statistics)"
            ],
            "relationships": [
                r"(?:show|find|list)\s+(?:relationships?|relations?|connections?)",
                r"what\s+is\s+connected\s+to\s+(\w+)",
                r"(\w+)\s+(?:connected|related|linked)\s+to\s+(\w+)"
            ],
            "recent": [
                r"(?:recent|latest|newest)\s+(\w+)",
                r"last\s+(\d+)\s+(\w+)",
                r"show\s+recent\s+(?:documents|files)"
            ],
            "search": [
                r"search\s+(?:for\s+)?(.+)",
                r"find\s+(.+)\s+in\s+database",
                r"lookup\s+(.+)"
            ],
            "rgcn": [
                r"r-?gcn\s+(?:predict|prediction|status)",
                r"(?:run|execute)\s+r-?gcn",
                r"neural\s+(?:network|model)\s+predict",
                r"r-?gcn\s+(?:compare|comparison)",
                r"(?:show|display)\s+r-?gcn\s+(?:result|results)"
            ],
            # 새로운 특화 패턴들
            "field_search": [
                r"(?:show|find|list)\s+field\s+(.+)",
                r"field\s+(.+)",
                r"what\s+is\s+(.+)\s+field"
            ],
            "note_search": [
                r"(?:show|find|list)\s+note\s*(\d*)",
                r"note\s+(\d+)",
                r"notes?\s+about\s+(.+)"
            ]
        }
        
        # 대폭 확장된 동의어 매핑 (화학공정 특화)
        self.synonyms = {
            # 기본 장비 관련
            "equipment": ["equipment", "machine", "device", "pump", "motor", "driver", "vessel", "tank", "compressor"],
            "project": ["project", "job", "document", "doc", "specification", "drawing"],
            "requirement": ["requirement", "spec", "specification", "parameter", "condition"],
            "person": ["person", "people", "engineer", "reviewer", "checker", "by", "checked", "reviewed"],
            "entity": ["entity", "entities", "item", "object", "node"],
            "literal": ["literal", "value", "data", "text"],
            "confidence": ["confidence", "certainty", "reliability", "accuracy"],
            "documents": ["documents", "files", "pdfs", "papers"],
            
            # 프로세스 관련 (대폭 확장)
            "temperature": ["temperature", "temp", "pumping temperature", "operating temperature", "minimum temperature", "maximum temperature"],
            "pressure": ["pressure", "suction pressure", "discharge pressure", "differential pressure", "vapor pressure"],
            "capacity": ["capacity", "flow", "flowrate", "flow rate", "rated capacity", "normal capacity"],
            "viscosity": ["viscosity", "fluid viscosity", "liquid viscosity"],
            "density": ["density", "specific gravity", "fluid density"],
            "head": ["head", "differential head", "pump head"],
            "npsh": ["npsh", "npsha", "net positive suction head", "available npsh"],
            
            # 재료/분류
            "material": ["material", "materials", "api class", "classification", "casing", "impeller", "shaft"],
            "api": ["api", "api class", "api classification", "american petroleum institute"],
            
            # 운전 조건
            "duty": ["duty", "operation", "operating", "continuous", "intermittent"],
            "startup": ["startup", "start-up", "start up", "starting", "initial condition"],
            "minimum": ["minimum", "min", "lowest", "bottom"],
            "maximum": ["maximum", "max", "highest", "top"],
            "normal": ["normal", "standard", "typical", "operating"],
            "rated": ["rated", "design", "nominal"],
            
            # 화학공정 유체
            "fluid": ["fluid", "liquid", "gas oil", "overflash", "ah feed", "am feed"],
            "overflash": ["overflash", "over flash", "overhead"],
            "feed": ["feed", "ah feed", "am feed", "feedstock"],
            
            # 안전/위험
            "flammable": ["flammable", "combustible", "fire hazard"],
            "toxic": ["toxic", "poisonous", "hazardous"],
            "sulfur": ["sulfur", "sulphur", "h2s", "hydrogen sulfide"],
            
            # 설치/위치
            "location": ["location", "indoor", "outdoor", "under roof"],
            "insulation": ["insulation", "steam tracing", "steam jacket", "heating"],
            
            # 제어/조작
            "manual": ["manual", "hand operated", "manually operated"],
            "automatic": ["automatic", "auto", "automatically operated"],
            
            # NOTE 관련 키워드
            "note": ["note", "notes", "remark", "comment", "description"],
            "foundation": ["foundation", "base", "mounting", "support"],
            "turndown": ["turndown", "turn down", "reduced operation", "minimum operation"],
            "overdesign": ["overdesign", "over design", "safety margin", "design margin"],
            "mdmt": ["mdmt", "minimum design metal temperature", "minimum temperature"],
            
            # 필드 관련
            "field": ["field", "parameter", "specification", "data", "information"],
            "type": ["type", "classification", "category", "kind"],
            "required": ["required", "specification", "requirement", "needed"]
        }
        
        # 약어/전체명 매핑
        self.abbreviation_mapping = {
            "npsh": "net positive suction head",
            "npsha": "npsh available",
            "mdmt": "minimum design metal temperature",
            "api": "american petroleum institute",
            "h2s": "hydrogen sulfide",
            "cp": "centipoise",
            "gpm": "gallons per minute",
            "bph": "barrels per hour",
            "psi": "pounds per square inch",
            "deg": "degree",
            "wt": "weight",
            "pt": "point",
            "max": "maximum",
            "min": "minimum",
            "nor": "normal",
            "oper": "operating",
            "temp": "temperature"
        }
        
        # 컨텍스트별 키워드 매핑
        self.context_keywords = {
            "temperature_related": ["pumping", "operating", "minimum", "maximum", "design", "metal"],
            "pressure_related": ["suction", "discharge", "differential", "vapor", "rated"],
            "flow_related": ["capacity", "rate", "continuous", "minimum", "maximum", "rated", "normal"],
            "material_related": ["casing", "impeller", "shaft", "api", "class", "classification"],
            "note_related": ["npsha", "foundation", "turndown", "overdesign", "mdmt", "startup", "slop"],
            "safety_related": ["flammable", "toxic", "h2s", "sulfur", "leakage", "hazard"]
        }
    
    def process_query(self, query, graph_data=None):
        """자연어 쿼리를 처리하여 결과 반환 (대폭 개선된 버전)"""
        query = query.lower().strip()
        print(f"🔍 Processing enhanced query: '{query}'")
        
        # 메모리 데이터 저장
        if graph_data:
            self._last_graph_data = graph_data
        
        # 1. 쿼리 전처리 (약어 확장, 동의어 정규화)
        processed_query = self._preprocess_query(query)
        print(f"🔄 Processed query: '{processed_query}'")
        
        # 2. 쿼리 타입 식별
        query_type, matches = self._identify_query_type(processed_query)
        print(f"🎯 Query type: {query_type}, matches: {matches}")
        
        # 3. 데이터베이스 우선 쿼리들
        if query_type in ["stats", "recent", "search", "rgcn"]:
            return self._handle_database_query(query_type, matches, graph_data, processed_query)
        
        # 4. 새로운 특화 쿼리들
        if query_type == "field_search":
            return self._handle_field_search(matches, graph_data)
        elif query_type == "note_search":
            return self._handle_note_search(matches, graph_data)
        
        # 5. 메모리 데이터 쿼리들 (기존 + 개선)
        if graph_data:
            nodes = graph_data.get("nodes", [])
            edges = graph_data.get("edges", [])
            
            if query_type == "show_all":
                return self._handle_show_all(matches, nodes, edges)
            elif query_type == "filter_by_confidence":
                return self._handle_confidence_filter(matches, nodes)
            elif query_type == "filter_by_class":
                return self._handle_class_filter(matches, nodes)
            elif query_type == "count":
                return self._handle_count(matches, nodes)
            elif query_type == "relationships":
                return self._handle_relationships(matches, nodes, edges)
        
        # 6. Fallback: 향상된 데이터베이스 검색
        return self._handle_enhanced_database_fallback(processed_query, query)
    
    def _preprocess_query(self, query):
        """쿼리 전처리: 약어 확장 및 동의어 정규화"""
        processed = query
        
        # 1. 약어 확장
        for abbr, full_form in self.abbreviation_mapping.items():
            processed = re.sub(rf'\b{re.escape(abbr)}\b', full_form, processed, flags=re.IGNORECASE)
        
        # 2. 특수 문자 정리
        processed = re.sub(r'[^\w\s-]', ' ', processed)
        processed = re.sub(r'\s+', ' ', processed).strip()
        
        return processed
    
    def _search_memory_data(self, query, graph_data):
        """메모리 데이터에서 직접 검색"""
        if not graph_data or 'nodes' not in graph_data:
            return None
        
        nodes = graph_data['nodes']
        query_lower = query.lower()
        
        # 모든 노드에서 검색 (Entity + Literal)
        matches = [node for node in nodes 
                  if query_lower in node.get('id', '').lower()]
        
        return {
            "type": "table",
            "data": matches,
            "message": f"Found {len(matches)} items in memory matching '{query}'",
            "columns": ["id", "type", "ontology_class", "confidence"]
        }

    def _identify_query_type(self, query):
        """쿼리 타입과 매칭된 그룹 식별"""
        for query_type, patterns in self.query_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, query, re.IGNORECASE)
                if match:
                    return query_type, match.groups()
        return "unknown", ()
    
    def _normalize_term(self, term):
        """용어 정규화 (동의어 처리)"""
        if not term:
            return term
            
        term = term.lower()
        for canonical, synonyms in self.synonyms.items():
            if term in synonyms:
                return canonical
        return term
    
    def _handle_database_query(self, query_type, matches, graph_data, processed_query):
        """데이터베이스 기반 쿼리 처리 (검색 강화)"""
        try:
            if query_type == "stats":
                db_stats = self.db_manager.get_database_stats()
                stats_data = {
                    "Total Documents": db_stats.get("total_documents", 0),
                    "Total Nodes": db_stats.get("total_nodes", 0),
                    "Total Edges": db_stats.get("total_edges", 0),
                    "Total Patterns": db_stats.get("total_patterns", 0),
                    "High Confidence Entities": db_stats.get("high_confidence_entities", 0)
                }
                return {
                    "type": "stat",
                    "data": stats_data,
                    "message": "Database statistics across all processed documents"
                }
            
            elif query_type == "recent":
                recent_query = """
                    SELECT filename, upload_time, content_length, processing_time
                    FROM documents 
                    ORDER BY upload_time DESC 
                    LIMIT 10
                """
                result_df = self.db_manager.execute_query(recent_query)
                
                if not result_df.empty:
                    result_df['upload_time'] = result_df['upload_time'].apply(
                        lambda x: x.split('T')[0] if 'T' in str(x) else str(x)
                    )
                    return {
                        "type": "table",
                        "data": result_df.to_dict('records'),
                        "message": f"Found {len(result_df)} recent documents",
                        "columns": ["filename", "upload_time", "content_length", "processing_time"]
                    }
                else:
                    return {"type": "table", "data": [], "message": "No documents found in database"}
            
            elif query_type == "search":
                search_term = matches[0] if matches else ""
                return self._enhanced_database_search(search_term, processed_query)

            elif query_type == "rgcn":
                return self._handle_rgcn_query(matches, graph_data)
            
            # Fallback: 향상된 데이터베이스 검색
            return self._handle_enhanced_database_fallback(processed_query, processed_query)

        except Exception as e:
            print(f"❌ Database query error: {e}")
            return {"type": "error", "message": f"Database query failed: {str(e)}"}
    
    def _enhanced_database_search(self, search_term, processed_query):
        """향상된 데이터베이스 검색 (다중 키워드, 컨텍스트 고려)"""
        try:
            # 검색어에서 의미있는 키워드들 추출
            keywords = self._extract_meaningful_keywords(search_term, processed_query)
            print(f"🔍 Extracted keywords: {keywords}")
            
            if not keywords:
                return {"type": "suggestions", "data": ["temperature", "pressure", "capacity", "npsha", "startup"], 
                       "message": "Try searching for specific terms like:"}
            
            # 다중 키워드 검색 조건 생성
            search_conditions = []
            for keyword in keywords[:5]:  # 최대 5개 키워드
                search_conditions.extend([
                    f"LOWER(n.id) LIKE '%{keyword}%'",
                    f"LOWER(n.ontology_class) LIKE '%{keyword}%'",
                    f"LOWER(d.filename) LIKE '%{keyword}%'"
                ])
            
            search_query = f"""
                SELECT DISTINCT n.id, n.node_type, n.ontology_class, n.confidence, d.filename
                FROM nodes n
                JOIN documents d ON n.document_id = d.id
                WHERE {' OR '.join(search_conditions)}
                ORDER BY n.confidence DESC, n.ontology_class
                LIMIT 25
            """
            
            result_df = self.db_manager.execute_query(search_query)
            
            return {
                "type": "table",
                "data": result_df.to_dict('records') if not result_df.empty else [],
                "message": f"Found {len(result_df)} items matching '{search_term}' (keywords: {', '.join(keywords)})",
                "columns": ["id", "node_type", "ontology_class", "confidence", "filename"]
            }
            
        except Exception as e:
            print(f"❌ Enhanced search error: {e}")
            return {"type": "error", "message": f"Enhanced search failed: {str(e)}"}
    
    def _extract_meaningful_keywords(self, search_term, processed_query):
        """검색어에서 의미있는 키워드들 추출"""
        keywords = set()
        
        # 1. 원본 검색어 분할
        original_words = search_term.split()
        keywords.update([w for w in original_words if len(w) > 2])
        
        # 2. 동의어 매핑에서 관련 키워드 찾기
        for canonical, synonyms in self.synonyms.items():
            if any(word in search_term for word in synonyms):
                keywords.add(canonical)
                keywords.update([s for s in synonyms if len(s) > 2])
        
        # 3. 컨텍스트 키워드 추가
        for context, context_keywords in self.context_keywords.items():
            if any(keyword in search_term for keyword in context_keywords):
                keywords.update(context_keywords)
        
        # 4. 불용어 제거
        stop_words = {"for", "the", "and", "or", "in", "on", "at", "to", "from", "with", "by"}
        keywords = keywords - stop_words
        
        return list(keywords)
    
    def _handle_field_search(self, matches, graph_data):
        """필드 검색 처리"""
        if not graph_data or not matches:
            return {"type": "error", "message": "No field search term provided"}
        
        search_term = matches[0].lower()
        nodes = graph_data.get("nodes", [])
        
        # 필드 관련 노드들 찾기
        field_nodes = [n for n in nodes if n.get("ontology_class") == "Field" and 
                      search_term in n.get("id", "").lower()]
        
        return {
            "type": "table",
            "data": field_nodes,
            "message": f"Found {len(field_nodes)} fields matching '{search_term}'",
            "columns": ["id", "ontology_class", "confidence"]
        }
    
    def _handle_note_search(self, matches, graph_data):
        """NOTE 검색 처리"""
        if not graph_data:
            return {"type": "error", "message": "No graph data available"}
        
        nodes = graph_data.get("nodes", [])
        
        if matches and matches[0].isdigit():
            # 특정 NOTE 번호 검색
            note_num = matches[0]
            note_nodes = [n for n in nodes if n.get("ontology_class") == "Note" and 
                         note_num in n.get("id", "")]
            message = f"Found NOTE {note_num}"
        else:
            # 모든 NOTE 검색
            note_nodes = [n for n in nodes if n.get("ontology_class") == "Note"]
            message = f"Found {len(note_nodes)} notes"
        
        return {
            "type": "table",
            "data": note_nodes,
            "message": message,
            "columns": ["id", "ontology_class", "confidence"]
        }
    
    def _handle_show_all(self, matches, nodes, edges):
        """'show all X' 타입 쿼리 처리 (개선된 버전)"""
        if not matches:
            return {"type": "table", "data": nodes[:10], "message": "Showing all nodes (limited to 10)"}
        
        target = self._normalize_term(matches[0])
        
        if target in ["entity", "entities"]:
            entities = [n for n in nodes if n.get("type") == "entity"]
            return {
                "type": "table", 
                "data": entities,
                "message": f"Found {len(entities)} entities",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in ["literal", "literals", "value", "values"]:
            literals = [n for n in nodes if n.get("type") == "literal"]
            return {
                "type": "table", 
                "data": literals,
                "message": f"Found {len(literals)} literals",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in ["field", "fields"]:
            fields = [n for n in nodes if n.get("ontology_class") == "Field"]
            return {
                "type": "table", 
                "data": fields,
                "message": f"Found {len(fields)} fields",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in ["note", "notes"]:
            notes = [n for n in nodes if n.get("ontology_class") == "Note"]
            return {
                "type": "table", 
                "data": notes,
                "message": f"Found {len(notes)} notes",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in self.synonyms.get("equipment", []):
            equipment = [n for n in nodes if n.get("ontology_class", "").lower() == "equipment"]
            return {
                "type": "table", 
                "data": equipment,
                "message": f"Found {len(equipment)} equipment entities",
                "columns": ["id", "confidence"]
            }
        elif target in self.synonyms.get("project", []):
            projects = [n for n in nodes if n.get("ontology_class", "").lower() == "project"]
            return {
                "type": "table", 
                "data": projects,
                "message": f"Found {len(projects)} project entities",
                "columns": ["id", "confidence"]
            }
        else:
            # 일반적인 검색
            filtered = [n for n in nodes if target in n.get("id", "").lower() or 
                       target in n.get("ontology_class", "").lower()]
            return {
                "type": "table", 
                "data": filtered,
                "message": f"Found {len(filtered)} items matching '{target}'",
                "columns": ["id", "type", "ontology_class", "confidence"]
            }
    
    def _handle_confidence_filter(self, matches, nodes):
        """신뢰도 기반 필터링 (개선된 버전)"""
        if len(matches) >= 2 and matches[1].replace('.', '').isdigit():
            # 특정 임계값 지정된 경우
            threshold = float(matches[1])
            target = self._normalize_term(matches[0])
        else:
            # 'high confidence' 등의 경우
            threshold = 0.8
            target = self._normalize_term(matches[0]) if matches else "entity"
        
        filtered_nodes = []
        for node in nodes:
            confidence = node.get("confidence", 0)
            if confidence > threshold:
                if target == "entity" and node.get("type") == "entity":
                    filtered_nodes.append(node)
                elif target in node.get("ontology_class", "").lower():
                    filtered_nodes.append(node)
                elif target in ["all", "any", ""]:
                    filtered_nodes.append(node)
        
        return {
            "type": "table",
            "data": filtered_nodes,
            "message": f"Found {len(filtered_nodes)} items with confidence > {threshold}",
            "columns": ["id", "type", "ontology_class", "confidence"]
        }
    
    def _handle_class_filter(self, matches, nodes):
        """클래스/타입 기반 필터링 (개선된 버전)"""
        if len(matches) >= 2:
            item_type = self._normalize_term(matches[0])
            class_name = self._normalize_term(matches[1])
            
            filtered = [n for n in nodes if 
                       class_name in n.get("ontology_class", "").lower() or
                       class_name in n.get("type", "").lower()]
            
            return {
                "type": "table",
                "data": filtered,
                "message": f"Found {len(filtered)} {item_type} of type {class_name}",
                "columns": ["id", "type", "ontology_class", "confidence"]
            }
        
        return {"type": "error", "message": "Could not parse class filter query"}
    
    def _handle_count(self, matches, nodes):
        """개수 세기 (확장된 버전)"""
        if not matches:
            total = len(nodes)
            return {
                "type": "stat",
                "data": {"Total Nodes": total},
                "message": f"Total count: {total}"
            }
        
        target = self._normalize_term(matches[0])
        
        if target in ["entity", "entities"]:
            count = sum(1 for n in nodes if n.get("type") == "entity")
        elif target in ["literal", "literals"]:
            count = sum(1 for n in nodes if n.get("type") == "literal")
        elif target in ["field", "fields"]:
            count = sum(1 for n in nodes if n.get("ontology_class") == "Field")
        elif target in ["note", "notes"]:
            count = sum(1 for n in nodes if n.get("ontology_class") == "Note")
        elif target in self.synonyms.get("equipment", []):
            count = sum(1 for n in nodes if n.get("ontology_class", "").lower() == "equipment")
        elif target in self.synonyms.get("project", []):
            count = sum(1 for n in nodes if n.get("ontology_class", "").lower() == "project")
        else:
            count = sum(1 for n in nodes if target in n.get("ontology_class", "").lower())
        
        return {
            "type": "stat",
            "data": {f"{target.title()} Count": count},
            "message": f"Count of {target}: {count}"
        }
    
    def _handle_relationships(self, matches, nodes, edges):
        """관계 정보 처리 (확장된 버전)"""
        if not edges:
            return {
                "type": "table",
                "data": [],
                "message": "No relationships found in the current graph"
            }
        
        # 관계 통계
        relation_counts = {}
        for edge in edges:
            rel = edge.get("relation", "unknown")
            relation_counts[rel] = relation_counts.get(rel, 0) + 1
        
        relationship_data = [
            {"Relationship Type": rel, "Count": count}
            for rel, count in relation_counts.items()
        ]
        
        return {
            "type": "table",
            "data": relationship_data,
            "message": f"Found {len(edges)} relationships of {len(relation_counts)} types",
            "columns": ["Relationship Type", "Count"]
        }
    
    def _handle_enhanced_database_fallback(self, processed_query, original_query):
        """향상된 데이터베이스 fallback 검색"""
        try:
            # 메모리 데이터 먼저 검색
            if self._last_graph_data:
                memory_result = self._search_memory_data(original_query, self._last_graph_data)
                if memory_result and memory_result.get('data'):
                    return memory_result

            keywords = self._extract_meaningful_keywords(original_query, processed_query)
            
            if not keywords:
                return {
                    "type": "suggestions",
                    "data": [
                        "temperature → pumping temperature fields",
                        "pressure → suction/discharge pressure",
                        "capacity → flow rate information", 
                        "npsha → foundation notes",
                        "startup → start-up conditions",
                        "api → material classifications"
                    ],
                    "message": "Try these enhanced search examples:"
                }
            
            # 컨텍스트 기반 키워드 확장
            expanded_keywords = set(keywords)
            for keyword in keywords:
                for context, context_keywords in self.context_keywords.items():
                    if keyword in context_keywords:
                        expanded_keywords.update(context_keywords[:3])  # 최대 3개 추가
            
            keyword_conditions = " OR ".join([
                f"LOWER(n.id) LIKE '%{kw}%' OR LOWER(n.ontology_class) LIKE '%{kw}%'"
                for kw in list(expanded_keywords)[:8]  # 최대 8개 키워드
            ])
            
            search_query = f"""
                SELECT n.id, n.node_type, n.ontology_class, n.confidence, d.filename
                FROM nodes n
                JOIN documents d ON n.document_id = d.id
                WHERE {keyword_conditions}
                ORDER BY n.confidence DESC, 
                    CASE n.ontology_class 
                        WHEN 'Field' THEN 1 
                        WHEN 'Note' THEN 2 
                        WHEN 'ProcessRequirement' THEN 3 
                        ELSE 4 
                    END
                LIMIT 20
            """
            
            result_df = self.db_manager.execute_query(search_query)
            
            if not result_df.empty:
                return {
                    "type": "table",
                    "data": result_df.to_dict('records'),
                    "message": f"Enhanced search found {len(result_df)} items (expanded from: {', '.join(keywords)})",
                    "columns": ["id", "node_type", "ontology_class", "confidence", "filename"]
                }
            else:
                return {
                    "type": "suggestions",
                    "data": [
                        "Try 'database statistics' to see available data",
                        "Try 'show recent documents' to see processed files",
                        "Try specific terms like 'temperature', 'pressure', 'pump'"
                    ],
                    "message": "No matches found. Suggestions:"
                }
        
        except Exception as e:
            print(f"❌ Enhanced database fallback error: {e}")
            return {
                "type": "error",
                "message": f"Enhanced database search failed: {str(e)}"
            }

    def _handle_rgcn_query(self, matches, graph_data):
        """R-GCN 관련 쿼리 처리"""
        # global rgcn_manager 부분을 제거하고 다른 방식으로 처리
        try:
            # rgcn_manager가 존재하는지 확인
            if not hasattr(self, 'rgcn_manager') or not self.rgcn_manager:
                return {"type": "error", "message": "R-GCN not available. Check system initialization."}
            
            if not graph_data:
                return {"type": "error", "message": "No graph data available for R-GCN prediction"}
            
            result = self.rgcn_manager.predict(graph_data)
            if result:
                predictions = result["predictions"]
                confidences = result["confidence_scores"]
                class_dist = result["class_distribution"]
                
                # 통계 생성
                pred_counts = {}
                class_names = ["Unknown", "Entity", "Literal", "Type"]
                for i, name in enumerate(class_names):
                    count = class_dist.get(i, 0)
                    if count > 0:
                        pred_counts[f"{name} Count"] = count
                
                avg_confidence = result["average_confidence"]
                pred_counts["Average Confidence"] = round(avg_confidence, 3)
                
                return {
                    "type": "stat",
                    "data": pred_counts,
                    "message": f"R-GCN prediction completed for {len(predictions)} nodes"
                }
            else:
                return {"type": "error", "message": "R-GCN prediction failed"}
                
        except Exception as e:
            return {"type": "error", "message": f"R-GCN error: {str(e)}"}

# 글로벌 변수 (데이터베이스 통합)



db_manager = DatabaseManager()

# R-GCN 관리자 초기화 (수정됨)
rgcn_manager = None
if RGCN_AVAILABLE:
    try:
        rgcn_manager = RGCNManager()  # ← "ontology.ttl" 제거
        print("✅ R-GCN Manager initialized")
    except Exception as e:
        print(f"❌ R-GCN Manager initialization failed: {e}")
        rgcn_manager = None
else:
    print("⚠️ R-GCN not available - running without R-GCN support")

# NaturalLanguageQueryProcessor 초기화 시 rgcn_manager 전달
# TTL 온톨로지 학습을 포함한 향상된 초기화

print("🧠 Initializing Enhanced Ontology Learning System...")

# TTL 기반 온톨로지 학습기 초기화
try:
    ontology_learner = EnhancedOntologyLearner("ontology.ttl")
    print("✅ TTL Ontology loaded successfully!")
    
    # TTL 학습 결과 출력
    ttl_stats = ontology_learner.get_enhanced_statistics()
    print(f"📚 Ontology Statistics:")
    print(f"   🏷️ Total Classes: {ttl_stats['total_classes']} ({ttl_stats['ttl_classes']} from TTL)")
    print(f"   🔗 Total Properties: {ttl_stats['total_properties']}")
    print(f"   📊 TTL Triples: {ttl_stats['ttl_triples']}")
    print(f"   📦 TTL Instances: {ttl_stats['ttl_instances']}")
    print(f"   🌳 Class Hierarchies: {ttl_stats['class_hierarchy_depth']}")
    print(f"   📖 Source: {ttl_stats['ontology_source']}")
    
except Exception as e:
    print(f"⚠️ TTL loading failed: {e}")
    print("🔄 Falling back to basic ontology learner...")
    ontology_learner = AdvancedOntologyLearner()  # 기존 방식

# 데이터베이스 매니저 초기화
db_manager = DatabaseManager()

# R-GCN 관리자 초기화 (TTL 지식 활용)
rgcn_manager = None
if RGCN_AVAILABLE:
    try:
        rgcn_manager = RGCNManager()
        print("✅ R-GCN Manager initialized with TTL support")
    except Exception as e:
        print(f"❌ R-GCN Manager initialization failed: {e}")
        rgcn_manager = None
else:
    print("⚠️ R-GCN not available - running without R-GCN support")

# NaturalLanguageQueryProcessor 초기화 (TTL 지식 포함)
query_processor = NaturalLanguageQueryProcessor(ontology_learner, db_manager, rgcn_manager)

# TTL 지식을 쿼리 프로세서에 추가 설정
if hasattr(ontology_learner, 'get_ttl_knowledge'):
    ttl_knowledge = ontology_learner.get_ttl_knowledge()
    
    # TTL 인스턴스를 동의어 매핑에 추가
    for class_name, instances in ttl_knowledge['instances'].items():
        class_key = class_name.lower()
        if class_key not in query_processor.synonyms:
            query_processor.synonyms[class_key] = []
        query_processor.synonyms[class_key].extend([inst.lower() for inst in instances])
    
    # TTL 속성을 동의어 매핑에 추가
    for prop in ttl_knowledge['datatype_properties']:
        prop_key = prop.lower()
        if prop_key not in query_processor.synonyms:
            query_processor.synonyms[prop_key] = [prop.lower()]
    
    print("✅ Query processor enhanced with TTL knowledge")

graph_data_store = {}

print("🎯 Enhanced system initialization complete!")
graph_data_store = {}


# Dash 앱 생성
app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])

app.layout = dbc.Container([
    html.H1("🏭 Industrial Knowledge Graph Analyzer", className="text-primary mb-4"),
    
    dbc.Row([
        # 왼쪽 패널
        dbc.Col([
            dbc.Card([
                dbc.CardBody([
                    html.H5("📄 Document Upload"),
                    dcc.Upload(
                        id="upload-pdf",
                        children=html.Div([
                            "Drag and drop PDF files or ",
                            html.A("click to select", style={"color": "#007bff"})
                        ]),
                        style={
                            'width': '100%',
                            'height': '60px',
                            'lineHeight': '60px',
                            'borderWidth': '2px',
                            'borderStyle': 'dashed',
                            'borderRadius': '5px',
                            'textAlign': 'center',
                            'backgroundColor': '#f8f9fa'
                        },
                        multiple=False
                    )
                ])
            ], className="mb-3"),

            dbc.Card([
                dbc.CardBody([
                    html.H5("🎯 Quick Actions"),
                    dbc.Button("🧪 Test Connection", id="test-btn", color="secondary", className="w-100 mb-2", size="sm"),
                    dbc.Button("📊 Load Sample Data", id="sample-btn", color="info", className="w-100 mb-2", size="sm"),
                    dbc.Button("🔄 Update Charts", id="update-btn", color="warning", className="w-100 mb-2", size="sm"),
                ])
            ], className="mb-3"),
            
            # 새로 추가되는 R-GCN 카드
            dbc.Card([
                dbc.CardBody([
                    html.H5("🤖 R-GCN Model"),
                    dbc.Button("🔮 R-GCN Predict", id="rgcn-predict-btn", color="success", className="w-100 mb-2", size="sm"),
                    dbc.Button("📊 R-GCN vs Rule", id="rgcn-compare-btn", color="primary", className="w-100 mb-2", size="sm"),
                    html.Div(id="rgcn-status", className="mt-2")
                ])
            ], className="mb-3"),

            html.Div(id="status-output", className="mb-4"),
            
            # TTL 온톨로지 정보를 포함한 동적 시스템 정보 카드
            dbc.Card([
                dbc.CardBody([
                    html.H6("📈 Enhanced System Info"),
                    html.Div(id="dynamic-system-info")  # 동적 업데이트용
                ])
            ]),
            
            # 학습 통계 카드 추가
            html.Div(id="learning-stats", className="mt-3")
        ], width=4),
        
        # 오른쪽 패널
        dbc.Col([
            # 자연어 쿼리 인터페이스 추가
            dbc.Card([
                dbc.CardBody([
                    html.H5("🗣️ Natural Language Query"),
                    dbc.InputGroup([
                        dbc.Input(
                            id="nl-query-input",
                            placeholder="e.g., 'show all equipment with high confidence', 'database statistics', 'search for pump'",
                            type="text"
                        ),
                        dbc.Button("🔍 Query", id="query-btn", color="primary")
                    ]),
                    html.Div(id="query-output", className="mt-3")
                ])
            ], className="mb-4"),
            
            html.H4("📊 Node Classification"),
            dcc.Graph(id="pie-chart", style={'height': '400px'}),
            
            html.Hr(),
            
            html.H4("🕸️ Knowledge Graph"),
            dcc.Graph(id="network-graph", style={'height': '500px'})
        ], width=8)
    ]),
    
    # 숨겨진 데이터 저장소
    dcc.Store(id="data-store", data={})
], fluid=True)

# 콜백들
@app.callback(
    [Output("status-output", "children"), Output("data-store", "data")],
    [Input("test-btn", "n_clicks"), Input("sample-btn", "n_clicks"), Input("upload-pdf", "contents")],
    [State("upload-pdf", "filename")],
    prevent_initial_call=True
)
def handle_all_inputs(test_clicks, sample_clicks, pdf_contents, pdf_filename):
    global db_manager, rgcn_manager, query_processor, ontology_learner  # ontology_learner 추가
    from dash import ctx
    
    trigger_id = ctx.triggered[0]['prop_id'].split('.')[0] if ctx.triggered else None
    print(f"🔥 Input callback triggered: {trigger_id}")
    
    if trigger_id == "test-btn":
        print("✅ Test button clicked")
        # TTL 온톨로지 상태도 테스트에 포함
        ttl_status = "✅ TTL Loaded" if hasattr(ontology_learner, 'ttl_classes') and ontology_learner.ttl_classes else "⚠️ TTL Not Loaded"
        return dbc.Alert([
            html.P("✅ Connection test successful! Database ready.", className="mb-1"),
            html.P(f"🧠 Ontology: {ttl_status}", className="mb-0")
        ], color="success"), {}
    
    elif trigger_id == "sample-btn":
        print("📊 Sample button clicked - generating data...")
        
        G, node_features, predictions = create_sample_graph()
        
        # 샘플 데이터에도 R-GCN 예측 추가
        rgcn_predictions = None
        if rgcn_manager:
            try:
                # R-GCN용 노드 데이터 정리 (텍스트만 사용)
                rgcn_nodes = []
                for node in G.nodes():
                    node_data = node_features[node]
                    # 원본 텍스트 또는 노드 ID 사용
                    original_text = node_data.get('original_text', node)
                    rgcn_nodes.append({
                        "id": original_text,  # 원본 텍스트 사용
                        "type": node_data.get("type", "entity"),
                        "ontology_class": node_data.get("ontology_class", "Other"),
                        "confidence": node_data.get("confidence", 0.5)
                    })
                
                graph_data_for_rgcn = {
                    "nodes": rgcn_nodes,
                    "edges": [{"source": u, "target": v, "relation": data.get("relation", "unknown")} 
                            for u, v, data in G.edges(data=True)]
                }
                rgcn_result = rgcn_manager.predict(sample_graph_data)
                if rgcn_result:
                    rgcn_predictions = rgcn_result["predictions"]
                    print(f"✅ R-GCN predictions for sample: {len(rgcn_predictions)} nodes")
            except Exception as e:
                print(f"❌ R-GCN prediction error: {e}")
        
        # TTL 온톨로지 정보 추가
        ttl_enhanced = hasattr(ontology_learner, 'ttl_classes') and len(ontology_learner.ttl_classes) > 0
        
        graph_data = {
            "nodes": [{"id": node, **node_features[node]} for node in G.nodes()],
            "edges": [{"source": u, "target": v, "relation": data.get("relation", "unknown")} 
                     for u, v, data in G.edges(data=True)],
            "predictions": predictions,
            "rgcn_predictions": rgcn_predictions,
            "stats": {
                "total_nodes": G.number_of_nodes(),
                "total_edges": G.number_of_edges(),
                "source": "sample",
                "rgcn_available": rgcn_predictions is not None,
                "ttl_enhanced": ttl_enhanced,
                "ontology_source": "TTL + Default" if ttl_enhanced else "Default only"
            }
        }
        
        # query_processor에 그래프 데이터 전달 (메모리 저장)
        if query_processor:
            query_processor._last_graph_data = graph_data
        
        print(f"✅ Sample data created: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
        
        message = dbc.Alert([
            html.H5("✅ Sample data loaded!", className="alert-heading"),
            html.P(f"Created {G.number_of_nodes()} nodes and {G.number_of_edges()} edges"),
            html.P(f"🧠 Ontology: {'TTL Enhanced' if ttl_enhanced else 'Basic Mode'}"),
            html.P(f"🤖 R-GCN: {'✅ Available' if rgcn_predictions else '❌ Not Available'}"),
            html.P("Click 'Update Charts' to see visualizations!")
        ], color="success")
        
        return message, graph_data
    
    elif trigger_id == "upload-pdf" and pdf_contents:
        print(f"📄 PDF uploaded: {pdf_filename}")
        
        try:
            if not pdf_filename or not pdf_filename.lower().endswith('.pdf'):
                return dbc.Alert("❌ Please select a PDF file", color="warning"), {}
            
            content_type, content_string = pdf_contents.split(',')
            decoded = base64.b64decode(content_string)
            
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(decoded)
                tmp_file_path = tmp_file.name
            
            try:
                # OCR 지원 PDF 처리
                pdf_content = extract_pdf_content(tmp_file_path)
                pdf_text = pdf_content['text']
                pdf_info = pdf_content['pdf_info']
                ocr_results = pdf_content['ocr_results']
                processing_method = pdf_content['processing_method']
                
                print(f"📝 PDF 처리 완료: {len(pdf_text)} characters, 방법: {processing_method}")
                print(f"📊 PDF 정보: 타입={pdf_info['pdf_type']}, 이미지={pdf_info.get('total_images', 0)}개")
                
                if len(pdf_text.strip()) < 10:
                    return dbc.Alert("❌ No meaningful text found in PDF", color="warning"), {}
                
                # 그래프 생성 (TTL 온톨로지 + OCR 데이터 포함)
                import time
                start_time = time.time()
                
                # OCR 결과를 그래프 생성에 전달
                extended_content = {
                    'text': pdf_text,
                    'ocr_results': ocr_results,
                    'pdf_info': pdf_info
                }
                
                # TTL 온톨로지 활용 그래프 생성
                G, node_features, predictions, learning_results = process_pdf_to_graph(extended_content, pdf_filename)
                
                # TTL 온톨로지 정보 확인
                ttl_enhanced = hasattr(ontology_learner, 'ttl_classes') and len(ontology_learner.ttl_classes) > 0
                ttl_classifications = learning_results.get('ttl_classifications', 0) if ttl_enhanced else 0
                
                # R-GCN 예측 수행
                rgcn_predictions = None
                if rgcn_manager:
                    try:
                        graph_data_for_rgcn = {
                            "nodes": [{"id": node, **node_features[node]} for node in G.nodes()],
                            "edges": [{"source": u, "target": v, "relation": data.get("relation", "unknown")} 
                                     for u, v, data in G.edges(data=True)]
                        }
                        rgcn_result = rgcn_manager.predict(graph_data_for_rgcn)
                        if rgcn_result:
                            rgcn_predictions = rgcn_result["predictions"]
                            print(f"✅ R-GCN predictions: {len(rgcn_predictions)} nodes")
                    except Exception as e:
                        print(f"❌ R-GCN prediction error: {e}")
                
                # 그래프 데이터 구성 (TTL + OCR 정보 포함)
                graph_data = {
                    "nodes": [{"id": node, **node_features[node]} for node in G.nodes()],
                    "edges": [{"source": u, "target": v, "relation": data.get("relation", "unknown")} 
                             for u, v, data in G.edges(data=True)],
                    "predictions": predictions,
                    "rgcn_predictions": rgcn_predictions,
                    "learning_results": learning_results,
                    "pdf_info": pdf_info,
                    "ocr_results": ocr_results,
                    "stats": {
                        "total_nodes": G.number_of_nodes(),
                        "total_edges": G.number_of_edges(),
                        "source": "pdf",
                        "filename": pdf_filename,
                        "text_length": len(pdf_text),
                        "processing_method": processing_method,
                        "pdf_type": pdf_info.get('pdf_type', 'unknown'),
                        "ocr_processed": processing_method in ['image', 'hybrid'],
                        "total_images": pdf_info.get('total_images', 0),
                        "patterns_found": len(learning_results.get("patterns", {})),
                        "entities_found": sum(len(v) for v in learning_results.get("new_entities", {}).values()),
                        "relations_found": len(learning_results.get("new_relations", [])),
                        "avg_confidence": sum(learning_results.get("confidence_scores", {}).values()) / 
                                        max(len(learning_results.get("confidence_scores", {})), 1),
                        "domain_insights": learning_results.get("domain_insights", {}),
                        "rgcn_available": rgcn_predictions is not None,
                        "ocr_elements": len(ocr_results),
                        "ttl_enhanced": ttl_enhanced,
                        "ttl_classifications": ttl_classifications,
                        "ontology_source": "TTL + Default" if ttl_enhanced else "Default only"
                    }
                }
                
                # query_processor에 그래프 데이터 전달 (메모리 저장)
                if query_processor:
                    query_processor._last_graph_data = graph_data
                
                # 데이터베이스에 처리 결과 저장 (TTL + OCR 정보 포함)
                try:
                    # PDF 정보를 DB 저장용으로 변환
                    pdf_info_for_db = {
                        'pdf_type': pdf_info.get('pdf_type', 'unknown'),
                        'has_images': pdf_info.get('has_images', False),
                        'ocr_processed': processing_method in ['image', 'hybrid'],
                        'image_count': pdf_info.get('total_images', 0),
                        'ttl_enhanced': ttl_enhanced,
                        'ttl_classifications': ttl_classifications
                    }
                    
                    doc_id = db_manager.save_processing_results(
                        pdf_filename, pdf_text, graph_data, learning_results, 
                        processing_time=time.time() - start_time,
                        pdf_info=pdf_info_for_db
                    )
                    if doc_id:
                        print(f"✅ 데이터베이스 저장 완료 - Document ID: {doc_id}")
                    else:
                        print("⚠️ 데이터베이스 저장 실패")
                        doc_id = f"temp_{pdf_filename}_{int(time.time())}"
                        
                except Exception as e:
                    print(f"❌ DB 저장 오류: {e}")
                    import traceback
                    traceback.print_exc()
                    doc_id = f"temp_{pdf_filename}_{int(time.time())}"
                
                print(f"✅ PDF processed and saved: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
                
                # 상세한 성공 메시지 (TTL + OCR 정보 포함)
                stats = graph_data["stats"]
                domain_insights = stats.get("domain_insights", {})
                
                # 처리 방법에 따른 메시지 customization
                processing_emoji = {
                    'text': '📝',
                    'image': '🖼️', 
                    'hybrid': '📝🖼️',
                    'error': '❌'
                }
                
                message = dbc.Alert([
                    html.H5(f"✅ PDF '{pdf_filename}' processed with TTL + OCR + Ontology Learning!", className="alert-heading"),
                    html.P(f"{processing_emoji.get(processing_method, '📄')} Processing: {processing_method.upper()} | PDF Type: {stats['pdf_type'].upper()}", className="mb-2"),
                    dbc.Row([
                        dbc.Col([
                            html.P(f"📊 Text: {stats['text_length']} characters", className="mb-1"),
                            html.P(f"🎯 Patterns: {stats['patterns_found']} types", className="mb-1"),
                            html.P(f"🏷️ Entities: {stats['entities_found']} found", className="mb-1"),
                            html.P(f"🖼️ Images: {stats['total_images']} processed", className="mb-1"),
                        ], width=6),
                        dbc.Col([
                            html.P(f"🔗 Relations: {stats['relations_found']} identified", className="mb-1"),
                            html.P(f"🕸️ Graph: {stats['total_nodes']} nodes, {stats['total_edges']} edges", className="mb-1"),
                            html.P(f"🎖️ Confidence: {stats['avg_confidence']:.2f}", className="mb-1"),
                            html.P(f"🔍 OCR Elements: {stats['ocr_elements']} extracted", className="mb-1"),
                        ], width=6),
                    ]),
                    html.Hr(),
                    html.P([
                        "🧠 Domain: ", 
                        html.Strong(domain_insights.get("industry_domain", "Unknown")),
                        f" | Complexity: {domain_insights.get('complexity_score', 0):.1f}/5.0",
                        f" | TTL: {'✅' if ttl_enhanced else '❌'} ({ttl_classifications} classifications)",
                        f" | R-GCN: {'✅' if stats.get('rgcn_available') else '❌'}",
                        f" | OCR: {'✅' if stats.get('ocr_processed') else '❌'}",
                        f" | DB ID: {doc_id}" if doc_id else ""
                    ], className="mb-2"),
                    html.P("💾 Results saved to database! Try querying: 'database statistics' or 'show recent documents'", className="mb-0")
                ], color="success")
                
                return message, graph_data
                
            finally:
                if os.path.exists(tmp_file_path):
                    os.unlink(tmp_file_path)
        
        except Exception as e:
            print(f"❌ PDF processing error: {e}")
            import traceback
            traceback.print_exc()
            return dbc.Alert(f"❌ Error processing PDF: {str(e)}", color="danger"), {}
    
    return "", {}

@app.callback(
    Output("pie-chart", "figure"),
    [Input("update-btn", "n_clicks"), Input("rgcn-compare-btn", "n_clicks")],
    [State("data-store", "data")]
)
def update_pie_chart(update_clicks, compare_clicks, stored_data):
    """파이 차트 업데이트 (R-GCN 비교 기능 추가)"""
    from dash import ctx
    
    print(f"🎯 Pie chart callback - update: {update_clicks}, compare: {compare_clicks}, data: {bool(stored_data)}")
    
    if not stored_data:
        return go.Figure().add_annotation(
            text="Load sample data or upload PDF, then click 'Update Charts'",
            showarrow=False, x=0.5, y=0.5,
            font=dict(size=16)
        )
    
    trigger_id = ctx.triggered[0]['prop_id'].split('.')[0] if ctx.triggered else None
    
    # R-GCN 비교 모드
    if trigger_id == "rgcn-compare-btn" and stored_data.get("rgcn_predictions"):
        rule_predictions = stored_data.get("predictions", [])
        rgcn_predictions = stored_data.get("rgcn_predictions", [])
        
        print("=== 디버깅 정보 ===")
        print(f"rule_predictions: {rule_predictions}")
        print(f"rgcn_predictions: {rgcn_predictions}")
        print(f"rgcn_predictions 타입: {type(rgcn_predictions)}")
        if rgcn_predictions:
            print(f"첫 번째 항목: {rgcn_predictions[0] if rgcn_predictions else 'None'}")
        print("==================")
        
        if len(rule_predictions) > 0 and len(rgcn_predictions) > 0:
            # 크기 맞추기
            min_size = min(len(rule_predictions), len(rgcn_predictions))
            
            # Rule-based 결과 카운트 (크기 맞춤)
            rule_counts = Counter(rule_predictions[:min_size])
            
            # R-GCN 결과에서 predicted_cluster만 추출해서 카운트 (크기 맞춤)
            rgcn_clusters = []
            for pred in rgcn_predictions[:min_size]:
                if isinstance(pred, dict):
                    rgcn_clusters.append(pred.get('predicted_cluster', 'unknown'))
                else:
                    # 만약 pred가 단순 값이라면 그대로 사용
                    rgcn_clusters.append(pred)
            
            rgcn_counts = Counter(rgcn_clusters)
            
            print(f"Size adjustment: rule={len(rule_predictions)} -> {min_size}, rgcn={len(rgcn_predictions)} -> {min_size}")
            print(f"rule_counts: {rule_counts}")
            print(f"rgcn_counts: {rgcn_counts}")
            
            # 라벨 설정
            labels = ["Class 0", "Entity", "Literal"]
            rule_values = [rule_counts.get(i, 0) for i in range(3)]
            
            # R-GCN 값들을 매핑 (importance -> 숫자 인덱스)
            importance_to_class = {
                'high_importance': 0,      # Class 0에 매핑
                'medium_importance': 1,    # Entity에 매핑
                'low_importance': 2,       # Literal에 매핑
                'unknown': 2,              # 알 수 없는 것은 Literal로
                0: 0,                      # 숫자 인덱스도 지원
                1: 1,
                2: 2
            }
            
            rgcn_values = [0, 0, 0]  # [Class 0, Entity, Literal]
            for cluster, count in rgcn_counts.items():
                class_idx = importance_to_class.get(cluster, 2)  # 기본값은 2 (Literal)
                rgcn_values[class_idx] += count
            
            print(f"최종 rule_values: {rule_values}")
            print(f"최종 rgcn_values: {rgcn_values}")
            
            fig = go.Figure()
            
            # Rule-based 결과
            fig.add_trace(go.Bar(
                name="Rule-based",
                x=labels,
                y=rule_values,
                marker_color="#ff9999",
                text=rule_values,
                textposition="auto",
                hovertemplate="<b>%{x}</b><br>Rule-based: %{y}<extra></extra>"
            ))
            
            # R-GCN 결과
            fig.add_trace(go.Bar(
                name="R-GCN",
                x=labels,
                y=rgcn_values,
                marker_color="#66b3ff",
                text=rgcn_values,
                textposition="auto",
                hovertemplate="<b>%{x}</b><br>R-GCN: %{y}<extra></extra>"
            ))
            
            # 정확도 계산 (옵션)
            total_nodes = min_size  # 실제 비교한 노드 수
            accuracy_info = ""
            if total_nodes > 0:
                # 클래스별 일치도 계산
                matches = sum(1 for i in range(3) if rule_values[i] > 0 and rgcn_values[i] > 0)
                total_classes = sum(1 for i in range(3) if rule_values[i] > 0)
                if total_classes > 0:
                    class_agreement = matches / total_classes * 100
                    accuracy_info = f" | Class Agreement: {class_agreement:.1f}%"
            
            fig.update_layout(
                title=f"Rule-based vs R-GCN Predictions Comparison<br><sub>Compared nodes: {total_nodes} (Rule: {len(rule_predictions)}, R-GCN: {len(rgcn_predictions)}){accuracy_info}</sub>",
                barmode='group',
                height=400,
                showlegend=True,
                xaxis_title="Node Classes",
                yaxis_title="Count",
                hovermode='x unified'
            )
            
            print(f"✅ R-GCN comparison chart created with {total_nodes} nodes")
            return fig
        else:
            print(f"⚠️ No data available: rule={len(rule_predictions)}, rgcn={len(rgcn_predictions)}")
            return go.Figure().add_annotation(
                text=f"No data available for comparison<br>Rule: {len(rule_predictions)}, R-GCN: {len(rgcn_predictions)}",
                showarrow=False, x=0.5, y=0.5,
                font=dict(size=14, color="orange")
            )
    
    # R-GCN 비교 요청했지만 데이터가 없는 경우
    elif trigger_id == "rgcn-compare-btn":
        return go.Figure().add_annotation(
            text="R-GCN predictions not available<br>Upload PDF or load sample data with R-GCN support",
            showarrow=False, x=0.5, y=0.5,
            font=dict(size=14, color="orange")
        )
    
    # 기본 파이 차트 (기존 로직 + R-GCN 정보)
    if (update_clicks or compare_clicks) and "predictions" in stored_data:
        predictions = stored_data["predictions"]
        pred_counts = Counter(predictions)
        
        labels = ["Class", "Entity", "Literal"]
        values = [pred_counts.get(i, 0) for i in range(3)]
        colors = ["#ff9999", "#66b3ff", "#99ff99"]
        
        fig = go.Figure()
        fig.add_trace(go.Pie(
            labels=labels,
            values=values,
            hole=0.3,
            textinfo="label+percent+value",
            marker=dict(colors=colors),
            hovertemplate="<b>%{label}</b><br>Count: %{value}<br>Percentage: %{percent}<extra></extra>"
        ))
        
        # R-GCN 정보 및 추가 통계
        rgcn_status = "✅ Available" if stored_data.get("rgcn_predictions") else "❌ Not Available"
        
        # 추가 통계 정보
        stats = stored_data.get("stats", {})
        additional_info = ""
        if stats:
            source = stats.get("source", "unknown")
            if source == "pdf":
                filename = stats.get("filename", "unknown")
                additional_info = f" | Source: {filename}"
            elif source == "sample":
                additional_info = " | Source: Sample Data"
        
        title = f"Node Classification Results (Total: {sum(values)})<br><sub>R-GCN: {rgcn_status}{additional_info}</sub>"
        
        fig.update_layout(
            title=title,
            height=400,
            showlegend=True,
            hovermode='closest'
        )
        
        print(f"✅ Pie chart created: {values}")
        return fig
    
    return go.Figure().add_annotation(
        text="Load sample data or upload PDF, then click 'Update Charts'",
        showarrow=False, x=0.5, y=0.5,
        font=dict(size=16)
    )


@app.callback(
    Output("network-graph", "figure"),
    [Input("update-btn", "n_clicks")],
    [State("data-store", "data")]
)
def update_network_graph(n_clicks, stored_data):
    print(f"🕸️ Network graph callback - clicks: {n_clicks}, data: {bool(stored_data)}")
    
    if n_clicks and stored_data and "nodes" in stored_data:
        nodes = stored_data["nodes"]
        edges = stored_data["edges"]
        
        # 노드 수 제한 (성능을 위해)
        max_nodes = 50
        if len(nodes) > max_nodes:
            # 신뢰도가 높은 노드들을 우선적으로 선택
            sorted_nodes = sorted(nodes, key=lambda x: x.get("confidence", 0), reverse=True)
            nodes = sorted_nodes[:max_nodes]
            node_ids = {node["id"] for node in nodes}
            # 선택된 노드들 사이의 엣지만 필터링
            edges = [edge for edge in edges 
                    if edge["source"] in node_ids and edge["target"] in node_ids]
            print(f"⚠️ Limited to top {max_nodes} nodes by confidence")
        
        G = nx.DiGraph()
        
        # 노드 추가
        for node in nodes:
            G.add_node(node["id"], **node)
        
        # 엣지 추가 (관계 정보 포함)
        for edge in edges:
            if edge["source"] in G.nodes() and edge["target"] in G.nodes():
                G.add_edge(edge["source"], edge["target"], 
                          relation=edge.get("relation", "unknown"))
        
        # 레이아웃 계산 (더 나은 시각화를 위해)
        try:
            if G.number_of_nodes() > 20:
                pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
            else:
                pos = nx.spring_layout(G, k=5, iterations=100, seed=42)
        except:
            # fallback
            pos = {node: (i % 5, i // 5) for i, node in enumerate(G.nodes())}
        
        # 엣지 그리기
        edge_x, edge_y = [], []
        edge_info = []
        
        for edge in G.edges(data=True):
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
            
            # 엣지 정보 저장
            relation = edge[2].get("relation", "unknown")
            edge_info.append(f"{edge[0]} → {edge[1]} ({relation})")
        
        # 노드 위치 및 정보
        node_x = [pos[node][0] for node in G.nodes()]
        node_y = [pos[node][1] for node in G.nodes()]
        node_text = []
        
        # 노드 색상, 크기, 호버 텍스트
        node_colors = []
        node_sizes = []
        hover_text = []
        
        for node in G.nodes():
            node_info = next((n for n in nodes if n["id"] == node), {})
            confidence = node_info.get("confidence", 0.5)
            node_type = node_info.get("type", "unknown")
            ontology_class = node_info.get("ontology_class", "Unknown")
            
            # 노드 텍스트 (길이 제한)
            display_text = node[:12] + "..." if len(node) > 12 else node
            node_text.append(display_text)
            
            # 신뢰도에 따른 크기 (더 명확한 차이)
            size = 10 + confidence * 30
            node_sizes.append(size)
            
            # 온톨로지 클래스에 따른 색상 (더 세분화)
            color_map = {
                "Equipment": [70, 130, 180],      # 스틸블루
                "ProcessRequirement": [255, 165, 0],  # 오렌지
                "Field": [50, 205, 50],           # 라임그린
                "Note": [218, 112, 214],          # 오키드
                "Project": [255, 69, 0],          # 레드오렌지
                "Person": [138, 43, 226],         # 블루바이올렛
                "Literal": [60, 179, 113],        # 미디엄씨그린
                "Entity": [70, 130, 180],         # 스틸블루 (기본 엔티티)
                "Unknown": [169, 169, 169]        # 다크그레이
            }
            
            # 타입 우선, 없으면 온톨로지 클래스 사용
            color_key = ontology_class if ontology_class != "Unknown" else node_type
            base_color = color_map.get(color_key, color_map["Unknown"])
            
            # 신뢰도에 따른 투명도 및 채도
            alpha = 0.5 + confidence * 0.4
            color = f"rgba({base_color[0]}, {base_color[1]}, {base_color[2]}, {alpha})"
            node_colors.append(color)
            
            # 연결된 노드 수 계산
            degree = G.degree(node)
            
            # 호버 텍스트 (더 상세한 정보)
            hover_text.append(
                f"<b>{node}</b><br>" +
                f"Type: {node_type}<br>" +
                f"Class: {ontology_class}<br>" +
                f"Confidence: {confidence:.3f}<br>" +
                f"Connections: {degree}<br>" +
                f"Size: {size:.1f}"
            )
        
        fig = go.Figure()
        
        # 엣지 추가 (관계별 색상 구분)
        fig.add_trace(go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=1.5, color='rgba(125,125,125,0.3)'),
            mode='lines',
            showlegend=False,
            hoverinfo='none',
            name='Relationships'
        ))
        
        # 노드 추가 (향상된 시각화)
        fig.add_trace(go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            text=node_text,
            textposition="middle center",
            textfont=dict(size=7, color="white", family="Arial Black"),
            marker=dict(
                size=node_sizes,
                color=node_colors,
                line=dict(width=2, color='rgba(50,50,50,0.8)'),
                sizemode='diameter'
            ),
            showlegend=False,
            hoverinfo='text',
            hovertext=hover_text,
            name='Nodes'
        ))
        
        # 통계 정보
        stats = stored_data.get("stats", {})
        source_info = ""
        if stats:
            source = stats.get("source", "unknown")
            if source == "pdf":
                filename = stats.get("filename", "unknown")
                source_info = f"Source: {filename[:20]}..."
            elif source == "sample":
                source_info = "Source: Sample Data"
        
        # R-GCN 정보
        rgcn_info = ""
        if stored_data.get("rgcn_predictions"):
            rgcn_info = " | R-GCN: ✅"
        
        # 범례 정보
        legend_info = "Color by Ontology Class | Size by Confidence"
        
        title_text = (
            f"Knowledge Graph Network ({G.number_of_nodes()} nodes, {G.number_of_edges()} edges)<br>"
            f"<sub>{legend_info}{rgcn_info}<br>{source_info}</sub>"
        )
        
        fig.update_layout(
            title=title_text,
            height=500,
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            showlegend=False,
            plot_bgcolor='rgba(248,249,250,0.8)',
            paper_bgcolor='white',
            margin=dict(l=20, r=20, t=100, b=20),
            hovermode='closest',
            font=dict(size=12)
        )
        
        print(f"✅ Enhanced network graph created: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
        return fig
    
    return go.Figure().add_annotation(
        text="Load sample data or upload PDF, then click 'Update Charts'<br><sub>Network visualization will show node relationships</sub>",
        showarrow=False, x=0.5, y=0.5,
        font=dict(size=16)
    )

@app.callback(
    Output("learning-stats", "children"),
    [Input("data-store", "data")]
)
def update_learning_stats(stored_data):
    """온톨로지 학습 통계 표시 (향상된 버전)"""
    if not stored_data or "learning_results" not in stored_data:
        return ""
    
    learning_results = stored_data["learning_results"]
    stats = stored_data.get("stats", {})
    
    confidence_scores = learning_results.get("confidence_scores", {})
    domain_insights = learning_results.get("domain_insights", {})
    patterns = learning_results.get("patterns", {})
    new_entities = learning_results.get("new_entities", {})
    new_relations = learning_results.get("new_relations", [])
    
    if not confidence_scores and not domain_insights:
        return ""
    
    # 신뢰도 분포 계산
    high_conf = sum(1 for c in confidence_scores.values() if c > 0.8)
    med_conf = sum(1 for c in confidence_scores.values() if 0.5 < c <= 0.8)
    low_conf = sum(1 for c in confidence_scores.values() if c <= 0.5)
    total_items = len(confidence_scores)
    
    # 평균 신뢰도
    avg_confidence = sum(confidence_scores.values()) / max(len(confidence_scores), 1)
    
    # 도메인 통찰 정보
    industry = domain_insights.get('industry_domain', 'Unknown')
    complexity = domain_insights.get('complexity_score', 0)
    technical_density = domain_insights.get('technical_density', 0)
    document_type = domain_insights.get('document_type', 'Unknown')
    
    # 학습된 패턴 수
    pattern_count = len(patterns)
    entity_count = sum(len(v) for v in new_entities.values()) if new_entities else 0
    relation_count = len(new_relations)
    
    # 처리 정보
    source = stats.get("source", "unknown")
    processing_method = stats.get("processing_method", "unknown")
    
    # 색상 및 진행률 계산
    if total_items > 0:
        high_pct = (high_conf / total_items) * 100
        med_pct = (med_conf / total_items) * 100
        low_pct = (low_conf / total_items) * 100
    else:
        high_pct = med_pct = low_pct = 0
    
    # 복잡도에 따른 색상
    complexity_color = "success" if complexity >= 3.5 else "warning" if complexity >= 2.0 else "info"
    
    # 신뢰도에 따른 전체 평가
    if avg_confidence > 0.8:
        overall_badge = dbc.Badge("Excellent", color="success", className="me-1")
    elif avg_confidence > 0.6:
        overall_badge = dbc.Badge("Good", color="primary", className="me-1")
    elif avg_confidence > 0.4:
        overall_badge = dbc.Badge("Fair", color="warning", className="me-1")
    else:
        overall_badge = dbc.Badge("Needs Review", color="danger", className="me-1")
    
    return dbc.Card([
        dbc.CardBody([
            # 헤더
            html.Div([
                html.H6("🧠 Learning Analytics", className="card-title d-inline"),
                overall_badge,
                dbc.Badge(f"Avg: {avg_confidence:.3f}", color="secondary", className="float-end")
            ], className="mb-3"),
            
            # 신뢰도 분포 (프로그레스 바 포함)
            html.Div([
                html.P("📊 Confidence Distribution:", className="mb-2 fw-bold"),
                html.Div([
                    html.Small(f"🎖️ High ({high_conf})", className="text-success"),
                    dbc.Progress(value=high_pct, color="success", className="mb-1", style={"height": "8px"})
                ]),
                html.Div([
                    html.Small(f"🏃 Medium ({med_conf})", className="text-warning"),
                    dbc.Progress(value=med_pct, color="warning", className="mb-1", style={"height": "8px"})
                ]),
                html.Div([
                    html.Small(f"🤔 Low ({low_conf})", className="text-danger"),
                    dbc.Progress(value=low_pct, color="danger", className="mb-2", style={"height": "8px"})
                ]),
            ], className="mb-3"),
            
            # 학습 성과
            html.Div([
                html.P("🎯 Learning Results:", className="mb-2 fw-bold"),
                dbc.Row([
                    dbc.Col([
                        html.P(f"📋 Patterns: {pattern_count}", className="mb-1 small"),
                        html.P(f"🏷️ Entities: {entity_count}", className="mb-1 small"),
                    ], width=6),
                    dbc.Col([
                        html.P(f"🔗 Relations: {relation_count}", className="mb-1 small"),
                        html.P(f"📄 Total Items: {total_items}", className="mb-1 small"),
                    ], width=6),
                ])
            ], className="mb-3"),
            
            # 도메인 분석
            html.Div([
                html.P("🔬 Domain Analysis:", className="mb-2 fw-bold"),
                html.P([
                    "🏭 Industry: ", 
                    dbc.Badge(industry, color="info", className="me-2"),
                    "📈 Complexity: ",
                    dbc.Badge(f"{complexity:.1f}/5.0", color=complexity_color)
                ], className="mb-1 small"),
                html.P(f"📊 Technical Density: {technical_density:.3f}", className="mb-1 small"),
                html.P(f"📋 Document Type: {document_type}", className="mb-1 small"),
            ], className="mb-3"),
            
            # 처리 정보
            html.Div([
                html.P("⚙️ Processing Info:", className="mb-2 fw-bold"),
                html.P([
                    f"📂 Source: {source.upper()}"
                ], className="mb-1 small"),
                html.P([
                    f"🔧 Method: {processing_method.upper()}" if processing_method != "unknown" else "🔧 Method: Standard"
                ], className="mb-1 small"),
                
                # R-GCN 상태
                html.P([
                    "🤖 R-GCN: ",
                    dbc.Badge("Available", color="success") if stored_data.get("rgcn_predictions") 
                    else dbc.Badge("Not Available", color="secondary")
                ], className="mb-0 small"),
            ])
        ])
    ], className="border-info")


@app.callback(
    Output("query-output", "children"),
    [Input("query-btn", "n_clicks")],
    [State("nl-query-input", "value"), State("data-store", "data")]
)
def handle_natural_language_query(n_clicks, query_text, stored_data):
    """자연어 쿼리 처리 (데이터베이스 통합 + 향상된 기능)"""
    if not n_clicks or not query_text:
        return html.Div([
            html.P("💡 Enhanced Query Examples:", className="mb-2 fw-bold"),
            dbc.Row([
                dbc.Col([
                    html.H6("📊 Database Queries:", className="text-primary"),
                    html.Ul([
                        html.Li("database statistics"),
                        html.Li("show recent documents"),
                        html.Li("search for temperature"),
                        html.Li("search for npsha")
                    ], className="small")
                ], width=6),
                dbc.Col([
                    html.H6("🕸️ Graph Queries:", className="text-success"),
                    html.Ul([
                        html.Li("show all entities with high confidence"),
                        html.Li("count all notes"),
                        html.Li("show relationships"),
                        html.Li("field pumping temperature")
                    ], className="small")
                ], width=6)
            ]),
            dbc.Row([
                dbc.Col([
                    html.H6("🤖 Advanced Queries:", className="text-warning"),
                    html.Ul([
                        html.Li("r-gcn predict"),
                        html.Li("show all equipment"),
                        html.Li("note 1"),
                        html.Li("startup conditions")
                    ], className="small")
                ], width=6),
                dbc.Col([
                    html.H6("🔍 Search Tips:", className="text-info"),
                    html.Ul([
                        html.Li("Use specific terms like 'pump', 'pressure'"),
                        html.Li("Try 'high confidence' for quality filters"),
                        html.Li("Search by ontology classes"),
                        html.Li("Combine keywords for better results")
                    ], className="small")
                ], width=6)
            ])
        ])
    
    print(f"🗣️ Natural language query: '{query_text}'")
    
    try:
        # 쿼리 처리 시간 측정
        import time
        start_time = time.time()
        
        # 쿼리 처리 (데이터베이스 우선)
        result = query_processor.process_query(query_text, stored_data)
        
        processing_time = time.time() - start_time
        result["processing_time"] = processing_time
        
        # 결과 렌더링
        return render_query_result(result, query_text)
        
    except Exception as e:
        print(f"❌ Query processing error: {e}")
        import traceback
        traceback.print_exc()
        return dbc.Alert([
            html.H5("❌ Query Processing Error", className="alert-heading"),
            html.P(f"Error: {str(e)}"),
            html.Hr(),
            html.P("Try a simpler query or check the examples above.", className="mb-0")
        ], color="danger")

def render_query_result(result, original_query=""):
    """쿼리 결과를 렌더링 (향상된 버전)"""
    result_type = result.get("type", "error")
    message = result.get("message", "")
    data = result.get("data", [])
    processing_time = result.get("processing_time", 0)
    
    # 헤더 컴포넌트
    header_components = []
    
    if result_type != "error":
        header_components.append(
            dbc.Alert([
                html.Div([
                    html.Strong("✅ Query Result: "),
                    message,
                    html.Span(f" ({processing_time:.3f}s)", className="text-muted float-end")
                ])
            ], color="success", className="mb-3")
        )
    
    components = header_components.copy()
    
    if result_type == "table" and data:
        # 테이블 형태로 결과 표시 (향상된 버전)
        columns = result.get("columns", [])
        if not columns and data:
            columns = list(data[0].keys()) if data else []
        
        # 데이터 정리 및 포맷팅
        clean_data = []
        for i, item in enumerate(data[:50]):  # 최대 50개 항목으로 제한
            clean_item = {"#": i + 1}  # 행 번호 추가
            for key, value in item.items():
                if isinstance(value, float):
                    clean_item[key] = round(value, 3)
                elif isinstance(value, str) and len(value) > 60:
                    clean_item[key] = value[:57] + "..."
                else:
                    clean_item[key] = str(value) if value is not None else "N/A"
            clean_data.append(clean_item)
        
        if clean_data:
            df = pd.DataFrame(clean_data)
            
            # 테이블 스타일링
            table = dbc.Table.from_dataframe(
                df, 
                striped=True, 
                bordered=True, 
                hover=True, 
                responsive=True,
                size="sm",
                className="table-hover"
            )
            
            # 테이블을 카드로 감싸기
            components.append(
                dbc.Card([
                    dbc.CardHeader([
                        html.H6(f"📋 Results Table ({len(clean_data)} of {len(data)} items)", className="mb-0")
                    ]),
                    dbc.CardBody([
                        table
                    ], className="p-2")
                ], className="mb-3")
            )
            
            # 더 많은 항목이 있을 때 알림
            if len(data) > 50:
                components.append(
                    dbc.Alert([
                        html.I(className="fas fa-info-circle me-2"),
                        f"Showing first 50 of {len(data)} results. Refine your query for more specific results."
                    ], color="info", className="mb-3")
                )
    
    elif result_type == "stat" and data:
        # 통계 형태로 결과 표시 (향상된 버전)
        stat_cards = []
        for key, value in data.items():
            # 값 포맷팅
            if isinstance(value, float):
                display_value = f"{value:.3f}" if value < 100 else f"{value:.1f}"
            else:
                display_value = str(value)
            
            # 아이콘 매핑
            icon_map = {
                "total": "📊",
                "count": "🔢",
                "confidence": "🎖️",
                "average": "📈",
                "high": "⬆️",
                "low": "⬇️",
                "nodes": "🕸️",
                "edges": "🔗",
                "documents": "📄",
                "patterns": "🎯"
            }
            
            icon = "📊"  # 기본 아이콘
            for keyword, emoji in icon_map.items():
                if keyword.lower() in key.lower():
                    icon = emoji
                    break
            
            stat_cards.append(
                dbc.Col([
                    dbc.Card([
                        dbc.CardBody([
                            html.H4([icon, " ", display_value], className="text-primary mb-1"),
                            html.P(key, className="mb-0 small text-muted")
                        ], className="text-center py-2")
                    ], className="h-100 shadow-sm")
                ], width=12, md=6, lg=3, className="mb-2")
            )
        
        # 통계 카드들을 여러 행으로 배치
        rows = []
        for i in range(0, len(stat_cards), 4):
            rows.append(dbc.Row(stat_cards[i:i+4], className="mb-2"))
        
        components.extend(rows)
    
    elif result_type == "suggestions" and data:
        # 제안사항 표시 (향상된 버전)
        components.append(
            dbc.Card([
                dbc.CardHeader([
                    html.H6("💡 Query Suggestions", className="mb-0 text-primary")
                ]),
                dbc.CardBody([
                    dbc.ListGroup([
                        dbc.ListGroupItem([
                            html.I(className="fas fa-lightbulb me-2 text-warning"),
                            suggestion
                        ], action=True, className="border-0") 
                        for suggestion in data
                    ], flush=True)
                ])
            ], className="mb-3")
        )
    
    elif result_type == "error":
        components = [
            dbc.Alert([
                html.H5("❌ Query Error", className="alert-heading"),
                html.P(message),
                html.Hr(),
                html.P([
                    "Try rephrasing your query or check the examples. ",
                    html.A("View examples", href="#", className="alert-link")
                ], className="mb-0")
            ], color="danger")
        ]
    
    else:
        components.append(
            dbc.Alert([
                html.I(className="fas fa-search me-2"),
                "No results found for your query. Try different keywords or check the examples above."
            ], color="warning")
        )
    
    # 쿼리 기록 (옵션)
    if original_query and result_type != "error":
        components.append(
            html.Div([
                html.Hr(),
                html.Small([
                    "Query: ",
                    html.Code(original_query, className="bg-light px-2 py-1 rounded"),
                    f" | Processed in {processing_time:.3f}s"
                ], className="text-muted")
            ])
        )
    
    return html.Div(components)

@app.callback(
    Output("rgcn-status", "children"),
    [Input("rgcn-predict-btn", "n_clicks")],
    [State("data-store", "data")]
)
def update_rgcn_status(n_clicks, stored_data):
    """R-GCN 상태 업데이트 및 예측 수행 (향상된 버전)"""
    if not rgcn_manager:
        return dbc.Alert([
            html.I(className="fas fa-exclamation-triangle me-2"),
            "R-GCN not available - Check system configuration"
        ], color="warning", className="p-2 small")
    
    # 기본 상태 표시 (버튼을 누르지 않았을 때)
    if not n_clicks:
        try:
            status = rgcn_manager.get_status()
            
            # 상태에 따른 색상 및 아이콘
            ready_status = status.get('initialized', False)
            status_color = "success" if ready_status else "warning"
            status_icon = "✅" if ready_status else "⚠️"
            
            device_info = status.get('device', 'unknown')
            device_icon = "🚀" if "cuda" in device_info.lower() else "💻"
            
            return dbc.Card([
                dbc.CardBody([
                    html.Div([
                        html.P([
                            "🤖 R-GCN Status: ",
                            dbc.Badge(f"{status_icon} {'Ready' if ready_status else 'Not Ready'}", 
                                    color=status_color, className="ms-1")
                        ], className="mb-2 small"),
                        
                        html.P(f"🔗 Relations: {status.get('num_relations', 0)}", className="mb-2 small"),
                        
                        html.P([
                            f"{device_icon} Device: ",
                            dbc.Badge(device_info.upper(), color="info", className="ms-1")
                        ], className="mb-0 small")
                    ])
                ], className="p-2")
            ], className="border-primary", style={"fontSize": "0.85rem"})
            
        except Exception as e:
            return dbc.Alert([
                html.I(className="fas fa-exclamation-circle me-2"),
                f"Status check failed: {str(e)}"
            ], color="danger", className="p-2 small")
    
    # R-GCN 예측 수행 (버튼을 눌렀을 때)
    if n_clicks and stored_data:
        # 데이터 유효성 검사
        if not stored_data.get("nodes") or not stored_data.get("edges"):
            return dbc.Alert([
                html.I(className="fas fa-database me-2"),
                "No graph data available - Upload PDF or load sample data first"
            ], color="info", className="p-2 small")
        
        try:
            # 예측 시간 측정
            import time
            start_time = time.time()
            
            # R-GCN 예측 실행
            rgcn_result = rgcn_manager.predict(stored_data)
            
            prediction_time = time.time() - start_time
            
            if rgcn_result:
                predictions = rgcn_result["predictions"]
                avg_confidence = rgcn_result["average_confidence"]
                class_dist = rgcn_result["class_distribution"]
                
                # 클래스 분포 정리
                class_names = {0: "Class 0", 1: "Entity", 2: "Literal", 3: "Type"}
                formatted_dist = {}
                for class_id, count in class_dist.items():
                    if count > 0:
                        class_name = class_names.get(class_id, f"Class {class_id}")
                        formatted_dist[class_name] = count
                
                # 성능 평가
                confidence_level = "High" if avg_confidence > 0.8 else "Medium" if avg_confidence > 0.6 else "Low"
                confidence_color = "success" if avg_confidence > 0.8 else "warning" if avg_confidence > 0.6 else "danger"
                
                return dbc.Alert([
                    html.Div([
                        html.H6([
                            html.I(className="fas fa-check-circle me-2"),
                            "R-GCN Prediction Complete"
                        ], className="alert-heading mb-3"),
                        
                        dbc.Row([
                            dbc.Col([
                                html.P([
                                    "📊 Nodes Processed: ",
                                    html.Strong(len(predictions))
                                ], className="mb-1 small"),
                                html.P([
                                    "⏱️ Processing Time: ",
                                    html.Strong(f"{prediction_time:.3f}s")
                                ], className="mb-1 small"),
                            ], width=6),
                            dbc.Col([
                                html.P([
                                    "🎖️ Avg Confidence: ",
                                    dbc.Badge(f"{avg_confidence:.3f} ({confidence_level})", 
                                            color=confidence_color)
                                ], className="mb-1 small"),
                                html.P([
                                    "📈 Classes Found: ",
                                    html.Strong(len(formatted_dist))
                                ], className="mb-1 small"),
                            ], width=6)
                        ]),
                        
                        # 클래스 분포 표시
                        html.Div([
                            html.P("📊 Class Distribution:", className="mb-2 small fw-bold"),
                            html.Div([
                                dbc.Badge(f"{class_name}: {count}", color="secondary", className="me-1 mb-1")
                                for class_name, count in formatted_dist.items()
                            ])
                        ], className="mt-2"),
                        
                        html.Hr(className="my-2"),
                        html.P([
                            html.I(className="fas fa-info-circle me-1"),
                            "Use 'R-GCN vs Rule' button to compare with rule-based results"
                        ], className="mb-0 small text-muted")
                    ])
                ], color="success", className="p-3")
                
            else:
                return dbc.Alert([
                    html.I(className="fas fa-times-circle me-2"),
                    "R-GCN prediction failed - Check model configuration"
                ], color="danger", className="p-2 small")
                
        except Exception as e:
            print(f"❌ R-GCN prediction error: {e}")
            import traceback
            traceback.print_exc()
            
            return dbc.Alert([
                html.Div([
                    html.H6([
                        html.I(className="fas fa-exclamation-triangle me-2"),
                        "R-GCN Prediction Error"
                    ], className="alert-heading mb-2"),
                    html.P(f"Error: {str(e)}", className="mb-1 small"),
                    html.P("Check console for detailed error information", className="mb-0 small text-muted")
                ])
            ], color="danger", className="p-3")
    
    # 데이터 없이 버튼만 눌렀을 때
    elif n_clicks:
        return dbc.Alert([
            html.I(className="fas fa-upload me-2"),
            "No data available - Please upload a PDF or load sample data first"
        ], color="info", className="p-2 small")
    
    # 기본 상태 (예상치 못한 경우)
    return dbc.Alert([
        html.I(className="fas fa-question-circle me-2"),
        "R-GCN ready - Click 'R-GCN Predict' to run prediction"
    ], color="secondary", className="p-2 small")


if __name__ == "__main__":
    print("=" * 60)
    print("🚀 Starting Advanced Industrial Knowledge Graph Analyzer...")
    print("=" * 60)
    
    # 시스템 상태 체크 및 출력
    print("📋 System Status Check:")
    print("   🧠 Enhanced ontology learning: ✅ Enabled")
    print("   💾 Database system: ✅ Integrated")
    print("   🔍 Natural language queries: ✅ Supported")
    print("   🎯 Advanced pattern recognition: ✅ Active")
    
    # R-GCN 상태 확인
    if rgcn_manager:
        try:
            status = rgcn_manager.get_status()
            print(f"   🤖 R-GCN Model: ✅ Ready ({status.get('device', 'unknown')})")
            print(f"   🔗 Relation Types: {status.get('num_relations', 0)} configured")
        except:
            print("   🤖 R-GCN Model: ⚠️ Available but status check failed")
    else:
        print("   🤖 R-GCN Model: ❌ Not available")
    
    # 데이터베이스 상태 확인
    try:
        db_stats = db_manager.get_database_stats()
        print(f"   📊 Database: ✅ Connected ({db_stats.get('total_documents', 0)} documents)")
    except:
        print("   📊 Database: ⚠️ Connected but status check failed")
    
    print("=" * 60)
    print("🎉 Features Available:")
    print("   📄 PDF Upload & OCR Processing")
    print("   🕸️ Knowledge Graph Visualization")
    print("   🗣️ Natural Language Queries")
    print("   🤖 R-GCN Neural Network Predictions")
    print("   📊 Rule-based vs Neural Comparison")
    print("   💾 Persistent Database Storage")
    
    print("=" * 60)
    print("💡 Quick Start Guide:")
    print("   1. Upload a PDF or click 'Load Sample Data'")
    print("   2. Click 'Update Charts' to see visualizations")
    print("   3. Try natural language queries like:")
    print("      • 'database statistics'")
    print("      • 'show all equipment with high confidence'")
    print("      • 'search for temperature'")
    print("   4. Use 'R-GCN Predict' for neural network analysis")
    
    print("=" * 60)
    print("🌐 Server Starting...")
    print("🔗 URL: http://127.0.0.1:8050")
    print("📱 Mobile friendly interface available")
    print("🔄 Hot reload enabled (debug mode)")
    print("=" * 60)
    
    try:
        app.run(debug=True, port=8050, host='127.0.0.1')
    except KeyboardInterrupt:
        print("\n" + "=" * 60)
        print("🛑 Server stopped by user")
        print("💾 All data has been saved to database")
        print("👋 Thank you for using the Knowledge Graph Analyzer!")
        print("=" * 60)
    except Exception as e:
        print(f"\n❌ Server error: {e}")
        print("🔧 Please check the configuration and try again")