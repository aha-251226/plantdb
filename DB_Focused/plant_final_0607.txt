# ì™„ì „í•œ í•„ë“œ ì¶”ì¶œê¸° - ëª¨ë“  í•„ë“œ ë¹„êµ ì‹œìŠ¤í…œ
# í•µì‹¬: ëª¨ë“  í•„ë“œ ê°•ì œ ì¶”ì¶œ, í•„í„°ë§ ì—†ìŒ, ì™„ì „ ë¹„êµ

import streamlit as st
import duckdb
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import json
import os
import io
import re
import hashlib
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Set
import numpy as np
from dataclasses import dataclass, asdict
import traceback
import warnings
import logging
from collections import defaultdict, Counter

# ê²½ê³  í•„í„°ë§
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import pdfplumber
    import fitz  # PyMuPDF
    PDF_LIBRARIES_AVAILABLE = True
    logger.info("âœ… PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")
except ImportError:
    PDF_LIBRARIES_AVAILABLE = False
    logger.warning("âŒ PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")

@dataclass
class FieldContext:
    """í•„ë“œì˜ ì»¨í…ìŠ¤íŠ¸ ì •ë³´"""
    field_name: str
    value: Any
    bbox: Tuple[float, float, float, float]
    page: int
    confidence: float
    context_type: str
    extraction_method: str
    parent_field: Optional[str] = None
    conditions: Optional[Dict] = None
    hierarchy_level: int = 0
    font_info: Optional[Dict] = None
    
    def to_dict(self):
        """ì•ˆì „í•œ ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        try:
            result = asdict(self)
            if result.get('conditions') is None:
                result['conditions'] = {}
            if result.get('font_info') is None:
                result['font_info'] = {}
            if result.get('parent_field') is None:
                result['parent_field'] = ""
            
            result['confidence'] = float(result['confidence'])
            result['page'] = int(result['page'])
            result['hierarchy_level'] = int(result['hierarchy_level'])
            
            return result
        except Exception as e:
            logger.error(f"FieldContext.to_dict() ì˜¤ë¥˜: {e}")
            return {
                'field_name': str(self.field_name),
                'value': str(self.value),
                'bbox': (0, 0, 0, 0),
                'page': 0,
                'confidence': 0.50,  # ë‚®ì€ ê¸°ë³¸ê°’
                'context_type': 'simple',
                'extraction_method': 'error_recovery',
                'parent_field': "",
                'conditions': {},
                'hierarchy_level': 0,
                'font_info': {}
            }

class ComprehensiveOntologyManager:
    """í¬ê´„ì  ì˜¨í†¨ë¡œì§€ ê´€ë¦¬ì - ëª¨ë“  í•„ë“œ íƒ€ì… ì§€ì›"""
    
    def __init__(self):
        # ğŸ”§ ëª¨ë“  ê°€ëŠ¥í•œ í•„ë“œ íŒ¨í„´ ì •ì˜ (í™•ì¥)
        self.field_patterns = {
            # ê¸°ë³¸ ë¬¸ì„œ ì •ë³´
            'job_no': ['job', 'job no', 'job number', 'job #', 'project no', 'jobno'],
            'project': ['project', 'project name', 'project title', 'proj'],
            'doc_no': ['doc', 'doc no', 'document', 'document no', 'document number', 'docno'],
            'item_no': ['item', 'item no', 'item number', 'equipment id', 'tag', 'tag no'],
            'client': ['client', 'customer', 'company', 'owner'],
            'service': ['service', 'description', 'work', 'application', 'purpose'],
            'page': ['page', 'page no', 'sheet'],
            'rev': ['rev', 'revision', 'rev no', 'revision no'],
            'location': ['location', 'site', 'plant', 'area'],
            
            # ì¥ë¹„ ì •ë³´
            'pump_type': ['pump type', 'pump', 'type of pump', 'pump typ'],
            'driver_type': ['driver type', 'motor type', 'drive type', 'driver', 'motor'],
            'required_type': ['required type', 'type required', 'configuration'],
            'no_required': ['no required', 'number required', 'qty', 'quantity'],
            'duty': ['duty', 'operation', 'service type'],
            'operating': ['operating', 'operation', 'normal operation'],
            'standby': ['standby', 'stand-by', 'spare'],
            
            # í”„ë¡œì„¸ìŠ¤ ì¡°ê±´
            'liquid_name': ['liquid name', 'fluid', 'fluid name', 'liquid', 'medium'],
            'fluid_type': ['fluid type', 'liquid type', 'medium type'],
            'pumping_temperature': ['pumping temperature', 'temperature', 'temp', 'operating temp'],
            'vapor_pressure': ['vapor pressure', 'vapour pressure', 'vp'],
            'specific_gravity': ['specific gravity', 'sg', 's.g.', 'density', 'sp gr'],
            'viscosity': ['viscosity', 'visc', 'dynamic viscosity'],
            'capacity': ['capacity', 'flow', 'flow rate', 'flowrate', 'rate'],
            'suction_pressure': ['suction pressure', 'suction press', 'inlet pressure'],
            'discharge_pressure': ['discharge pressure', 'discharge press', 'outlet pressure'],
            'differential_pressure': ['differential pressure', 'diff pressure', 'head'],
            'differential_head': ['differential head', 'total head', 'head'],
            'npsh': ['npsh', 'npsha', 'npsh available', 'net positive suction head'],
            
            # ì¡°ê±´ íƒ€ì…
            'normal': ['normal', 'norm', 'operating', 'design'],
            'maximum': ['maximum', 'max', 'peak'],
            'minimum': ['minimum', 'min', 'lowest'],
            'rated': ['rated', 'design', 'nominal'],
            
            # ì¬ì§ˆ ì •ë³´
            'material': ['material', 'mat', 'construction', 'material selection'],
            'casing': ['casing', 'case', 'housing'],
            'impeller': ['impeller', 'rotor'],
            'shaft': ['shaft', 'spindle'],
            'corrosion_allowance': ['corrosion allowance', 'corr allowance', 'ca'],
            
            # ìš´ì „ ì¡°ê±´
            'method_of_starting': ['method of starting', 'starting method', 'start method'],
            'manual': ['manual', 'hand'],
            'automatic': ['automatic', 'auto'],
            'characteristics': ['characteristics', 'properties', 'property'],
            'flammable': ['flammable', 'combustible'],
            'toxic': ['toxic', 'poisonous'],
            'insulation': ['insulation', 'insul'],
            'steam_tracing': ['steam tracing', 'tracing'],
            'indoor': ['indoor', 'inside'],
            'outdoor': ['outdoor', 'outside'],
            'leakage': ['leakage', 'leak'],
            
            # ê¸°íƒ€
            'notes': ['notes', 'note', 'remarks', 'comment'],
            'continuous': ['continuous', 'cont'],
            'intermittent': ['intermittent', 'int'],
            'horizontal': ['horizontal', 'horiz'],
            'vertical': ['vertical', 'vert'],
            'centrifugal': ['centrifugal', 'centri'],
            
            # Feed íƒ€ì…
            'am_feed': ['am feed', 'am_feed', 'amfeed'],
            'ah_feed': ['ah feed', 'ah_feed', 'ahfeed'],
            'overflash': ['overflash', 'over flash'],
            
            # ë‹¨ìœ„ ë° ì¸¡ì •ê°’
            'temperature_c': ['Â°c', 'deg c', 'celsius'],
            'pressure_kgcm2': ['kg/cm2', 'kg/cm2a', 'kg/cm2g'],
            'flow_m3h': ['m3/h', 'm3/hr', 'cubic meter per hour'],
            'viscosity_cp': ['cp', 'centipoise'],
            'meter': ['m', 'meter', 'metre'],
            
            # API í´ë˜ìŠ¤
            'api_class': ['api class', 'api', 'class'],
            'a8': ['a-8', 'a8', 'class a-8'],
            
            # ê¸°ë³¸ ë¶„ë¥˜
            'field': ['field', 'parameter', 'item', 'property'],
            'value': ['value', 'data', 'measurement', 'reading'],
            'specification': ['specification', 'spec', 'requirement']
        }
        
        logger.info(f"âœ… í¬ê´„ì  ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì™„ë£Œ: {len(self.field_patterns)} í•„ë“œ íƒ€ì…")
    
    def map_field_to_ontology(self, field_candidate: str) -> str:
        """í¬ê´„ì  í•„ë“œ ë§¤í•‘ - ëª¨ë“  íŒ¨í„´ ì‹œë„"""
        try:
            if not field_candidate:
                return "Unknown_Field"
            
            field_lower = str(field_candidate).lower().strip()
            
            # ğŸ”§ ëª¨ë“  íŒ¨í„´ ì²´í¬ (ìˆœì„œ ì¤‘ìš”)
            best_matches = []
            
            for ontology_field, patterns in self.field_patterns.items():
                for pattern in patterns:
                    # ì™„ì „ ì¼ì¹˜
                    if pattern == field_lower:
                        return ontology_field.upper()
                    
                    # í¬í•¨ ê´€ê³„
                    if pattern in field_lower or field_lower in pattern:
                        score = len(pattern) / max(len(pattern), len(field_lower))
                        best_matches.append((ontology_field.upper(), score))
            
            # ìµœê³  ì ìˆ˜ ë°˜í™˜
            if best_matches:
                best_matches.sort(key=lambda x: x[1], reverse=True)
                return best_matches[0][0]
            
            # ğŸ”§ ë§¤í•‘ ì‹¤íŒ¨ì‹œì—ë„ ì •ê·œí™”ëœ ì›ë³¸ ì‚¬ìš© (ì ˆëŒ€ ë²„ë¦¬ì§€ ì•ŠìŒ)
            normalized = re.sub(r'[^\w\s]', '_', field_candidate)
            normalized = re.sub(r'\s+', '_', normalized.strip())
            normalized = re.sub(r'^\d+\s*', '', normalized)  # ì•ì˜ ìˆ«ì ì œê±°
            
            return normalized.upper() if normalized else "UNNAMED_FIELD"
            
        except Exception as e:
            logger.error(f"í•„ë“œ ë§¤í•‘ ì˜¤ë¥˜: {e}")
            return f"ERROR_FIELD_{abs(hash(str(field_candidate))) % 1000}"

class MaximumExtractorProcessor:
    """ìµœëŒ€ ì¶”ì¶œ í”„ë¡œì„¸ì„œ - ëª¨ë“  ê°€ëŠ¥í•œ í•„ë“œ ì¶”ì¶œ"""
    
    def __init__(self, ontology_manager):
        self.ontology = ontology_manager
        
        # ğŸ”§ ìµœëŒ€ ì¶”ì¶œ ì„¤ì •
        self.confidence_threshold = 0.15  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’
        self.min_confidence = 0.20  # ìµœì†Œ ë³´ì¥ ì‹ ë¢°ë„
        
        # ğŸ”§ í•„í„°ë§ ì™„ì „ ì œê±°
        self.enable_filtering = False  # í•„í„°ë§ ë¹„í™œì„±í™”
        
        # ğŸ”§ ëª¨ë“  íŒ¨í„´ í¬í•¨ (í™•ì¥ëœ íŒ¨í„´)
        self.all_patterns = [
            # ê¸°ë³¸ ì½œë¡ /ë“±í˜¸ íŒ¨í„´
            r'([A-Za-z][A-Za-z\s\(\)]{1,40})\s*[:=]\s*([^\n\r:=]{1,100})',
            # ë²ˆí˜¸. íŒ¨í„´
            r'(\d+\.?\s*[A-Za-z][A-Za-z\s\(\)]{1,40})\s+([^\n\r\d]{1,100})',
            # ëŒ€ì‹œ íŒ¨í„´
            r'([A-Z][A-Z\s]{2,30})\s*[-â€“â€”]\s*([^\n\r-]{1,100})',
            # ê´„í˜¸ íŒ¨í„´
            r'([A-Za-z][A-Za-z\s]{1,30})\s*\(\s*([^)]{1,50})\s*\)',
            # íƒ­ êµ¬ë¶„
            r'([A-Za-z][A-Za-z\s]{1,30})\t+([^\n\r\t]{1,100})',
            # ê³µë°± êµ¬ë¶„ (ëŒ€ë¬¸ì)
            r'([A-Z]{2,})\s+([A-Za-z0-9][A-Za-z0-9\s\.,/-]{1,80})',
            # ë‹¨ìˆœ íŒ¨í„´
            r'([A-Za-z]{3,20})\s*:\s*([A-Za-z0-9\s\.,/-]{1,50})',
            # ì²´í¬ë°•ìŠ¤ íŒ¨í„´
            r'([â– â–¡â˜‘â˜â–£])\s*([A-Za-z][A-Za-z\s]{1,30})',
            # API í´ë˜ìŠ¤ íŒ¨í„´
            r'(API\s+CLASS)\s+([A-Z0-9\-]+)',
            # ì˜¨ë„ íŒ¨í„´
            r'(\d+\.?\d*)\s*(Â°?[CF]|DEG)',
            # ì••ë ¥ íŒ¨í„´
            r'(\d+\.?\d*)\s*(kg/cm2[AG]?|PSI|BAR)',
            # ìœ ëŸ‰ íŒ¨í„´
            r'(\d+\.?\d*)\s*(m3/h|M3/HR|GPM)',
            # ì ë„ íŒ¨í„´
            r'(\d+\.?\d*)\s*(cP|CP|cSt)',
            # ë¹„ì¤‘ íŒ¨í„´
            r'(\d+\.?\d*)\s*(SG|S\.G\.)',
            # í¼ì„¼íŠ¸ íŒ¨í„´
            r'(\d+\.?\d*)\s*(%|WT%|wt%)',
            # ë¯¸í„° íŒ¨í„´
            r'(\d+\.?\d*)\s*(m|M|meter)',
            # ë‚ ì§œ íŒ¨í„´
            r'(\d{4}[-/]\d{2}[-/]\d{2})',
            # ì‹œê°„ íŒ¨í„´
            r'(\d{1,2}:\d{2})',
            # ì½”ë“œ íŒ¨í„´
            r'([A-Z]{1,5}\d{2,6}[A-Z]*)',
            # ì‹ë³„ì íŒ¨í„´
            r'([A-Z]+-\d+[A-Z]*)',
            # ë…¸íŠ¸ ë²ˆí˜¸ íŒ¨í„´
            r'(NOTE\s+\d+)',
            # í˜ì´ì§€ íŒ¨í„´
            r'(\d+\s+OF\s+\d+)',
            # ë¦¬ë¹„ì „ íŒ¨í„´
            r'(REV\.?\s*\d+[A-Z]*)',
        ]
        
        logger.info(f"âœ… ìµœëŒ€ ì¶”ì¶œ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” - íŒ¨í„´: {len(self.all_patterns)}ê°œ")
    
    def extract_all_possible_fields(self, file_content: bytes, filename: str) -> Dict[str, Any]:
        """ëª¨ë“  ê°€ëŠ¥í•œ í•„ë“œ ì¶”ì¶œ - í•„í„°ë§ ì—†ìŒ"""
        debug_info = {
            'filename': filename,
            'total_patterns_used': len(self.all_patterns),
            'filtering_disabled': True,
            'fields_at_each_step': {},
            'extraction_stats': {}
        }
        
        try:
            logger.info(f"ğŸš€ ìµœëŒ€ ì¶”ì¶œ ì‹œì‘: {filename}")
            
            # 1. ë©”íƒ€ë°ì´í„°
            metadata = self._extract_metadata(file_content)
            
            # 2. ğŸ”§ ëª¨ë“  í˜ì´ì§€ ì „ì²´ ìŠ¤ìº” (ì œí•œ ì—†ìŒ)
            all_fields = {}
            
            # ê³µê°„ì  ì¶”ì¶œ (ëª¨ë“  í˜ì´ì§€)
            spatial_results = self._extract_all_spatial_fields(file_content)
            all_fields.update(spatial_results)
            debug_info['fields_at_each_step']['spatial'] = len(spatial_results)
            logger.info(f"âœ… ê³µê°„ì  ì¶”ì¶œ: {len(spatial_results)} í•„ë“œ")
            
            # í…Œì´ë¸” ì¶”ì¶œ (ëª¨ë“  í…Œì´ë¸”)
            table_results = self._extract_all_table_fields(file_content)
            all_fields.update(table_results)
            debug_info['fields_at_each_step']['table'] = len(table_results)
            logger.info(f"âœ… í…Œì´ë¸” ì¶”ì¶œ: {len(table_results)} í•„ë“œ")
            
            # í…ìŠ¤íŠ¸ ë¼ì¸ ì¶”ì¶œ
            text_results = self._extract_all_text_lines(file_content)
            all_fields.update(text_results)
            debug_info['fields_at_each_step']['text'] = len(text_results)
            logger.info(f"âœ… í…ìŠ¤íŠ¸ ë¼ì¸ ì¶”ì¶œ: {len(text_results)} í•„ë“œ")
            
            # íŒ¨í„´ ì¶”ì¶œ (ëª¨ë“  íŒ¨í„´)
            pattern_results = self._extract_all_patterns(file_content)
            all_fields.update(pattern_results)
            debug_info['fields_at_each_step']['pattern'] = len(pattern_results)
            logger.info(f"âœ… íŒ¨í„´ ì¶”ì¶œ: {len(pattern_results)} í•„ë“œ")
            
            # ë‹¨ì–´ ì¶”ì¶œ (ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë“¤)
            word_results = self._extract_meaningful_words(file_content)
            all_fields.update(word_results)
            debug_info['fields_at_each_step']['word'] = len(word_results)
            logger.info(f"âœ… ë‹¨ì–´ ì¶”ì¶œ: {len(word_results)} í•„ë“œ")
            
            # 3. ğŸ”§ ì˜¨í†¨ë¡œì§€ ë§¤í•‘ (ëª¨ë“  í•„ë“œ)
            mapped_fields = {}
            for field_id, context in all_fields.items():
                try:
                    if isinstance(context, FieldContext):
                        # ì˜¨í†¨ë¡œì§€ ë§¤í•‘
                        mapped_name = self.ontology.map_field_to_ontology(context.field_name)
                        context.field_name = mapped_name
                        
                        # ğŸ”§ ì‹ ë¢°ë„ ë³´ì¥ (ìµœì†Œê°’ ë³´ì¥)
                        context.confidence = max(self.min_confidence, context.confidence)
                        
                        mapped_fields[field_id] = context
                except Exception as e:
                    logger.debug(f"ë§¤í•‘ ì˜¤ë¥˜ {field_id}: {e}")
                    mapped_fields[field_id] = context
            
            debug_info['fields_at_each_step']['mapped'] = len(mapped_fields)
            logger.info(f"âœ… ì˜¨í†¨ë¡œì§€ ë§¤í•‘: {len(mapped_fields)} í•„ë“œ")
            
            # 4. ğŸ”§ ìµœì¢… ê²°ê³¼ ìƒì„± (í•„í„°ë§ ì—†ìŒ)
            doc_id = f"doc_{hashlib.md5(filename.encode()).hexdigest()[:8]}"
            processed_fields = []
            
            for field_id, context in mapped_fields.items():
                try:
                    field_data = context.to_dict()
                    field_data['doc_id'] = doc_id
                    processed_fields.append(field_data)
                except Exception as e:
                    logger.debug(f"í•„ë“œ ì²˜ë¦¬ ì˜¤ë¥˜ {field_id}: {e}")
            
            confidences = [f.get('confidence', self.min_confidence) for f in processed_fields]
            avg_confidence = float(np.mean(confidences)) if confidences else self.min_confidence
            
            debug_info['extraction_stats'] = {
                'total_extracted': len(all_fields),
                'total_mapped': len(mapped_fields),
                'total_final': len(processed_fields),
                'filtering_applied': False,
                'avg_confidence': avg_confidence
            }
            
            logger.info(f"âœ… ìµœëŒ€ ì¶”ì¶œ ì™„ë£Œ: {len(processed_fields)} í•„ë“œ")
            
            return {
                'doc_id': doc_id,
                'extracted_fields': processed_fields,
                'metadata': metadata,
                'total_confidence': avg_confidence,
                'extraction_summary': {
                    'total_fields': len(processed_fields),
                    'high_confidence_fields': sum(1 for f in processed_fields if f.get('confidence', 0) > 0.70),
                    'extraction_methods_used': ['spatial', 'table', 'text', 'pattern', 'word'],
                    'context_types': dict(Counter(f.get('context_type', 'unknown') for f in processed_fields)),
                    'debug_info': debug_info
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ìµœëŒ€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return self._create_minimal_error_result(filename, str(e))
    
    def _extract_metadata(self, file_content: bytes) -> Dict:
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            if PDF_LIBRARIES_AVAILABLE:
                doc = fitz.open(stream=file_content, filetype="pdf")
                return {'page_count': len(doc)}
            return {'page_count': 1}
        except Exception:
            return {'page_count': 1}
    
    def _extract_all_spatial_fields(self, file_content: bytes) -> Dict[str, FieldContext]:
        """ëª¨ë“  ê³µê°„ì  í•„ë“œ ì¶”ì¶œ - ì œí•œ ì—†ìŒ"""
        results = {}
        
        if not PDF_LIBRARIES_AVAILABLE:
            return results
        
        try:
            with pdfplumber.open(io.BytesIO(file_content)) as pdf:
                # ğŸ”§ ëª¨ë“  í˜ì´ì§€ ì²˜ë¦¬ (ì œí•œ ì—†ìŒ)
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    if not page_text:
                        continue
                    
                    # ë‹¨ì–´ë³„ ìœ„ì¹˜ ì •ë³´
                    words = page.extract_words()
                    
                    # ğŸ”§ ëª¨ë“  íŒ¨í„´ ì‹œë„
                    for pattern_idx, pattern in enumerate(self.all_patterns):
                        try:
                            matches = re.finditer(pattern, page_text, re.IGNORECASE | re.MULTILINE)
                            
                            for match_idx, match in enumerate(matches):
                                groups = match.groups()
                                if len(groups) >= 1:
                                    if len(groups) >= 2:
                                        key, value = groups[0], groups[1]
                                    else:
                                        key, value = f"PATTERN_{pattern_idx}", groups[0]
                                    
                                    # ğŸ”§ ëª¨ë“  ìŒ í—ˆìš© (ìµœì†Œ ê²€ì¦ë§Œ)
                                    if self._is_minimal_valid(key, value):
                                        field_id = f"spatial_{page_num}_{pattern_idx}_{match_idx}"
                                        
                                        results[field_id] = FieldContext(
                                            field_name=str(key).strip(),
                                            value=str(value).strip(),
                                            bbox=(0, 0, 100, 100),
                                            page=page_num,
                                            confidence=self._calculate_base_confidence('spatial'),
                                            context_type='spatial',
                                            extraction_method='spatial_maximum'
                                        )
                        
                        except Exception:
                            continue
                    
                    # ë‹¨ì–´ ê¸°ë°˜ í‚¤-ê°’ ìŒ
                    if words:
                        word_pairs = self._extract_word_pairs(words, page_num)
                        results.update(word_pairs)
        
        except Exception as e:
            logger.error(f"ê³µê°„ì  ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return results
    
    def _extract_all_table_fields(self, file_content: bytes) -> Dict[str, FieldContext]:
        """ëª¨ë“  í…Œì´ë¸” í•„ë“œ ì¶”ì¶œ - ì œí•œ ì—†ìŒ"""
        results = {}
        
        if not PDF_LIBRARIES_AVAILABLE:
            return results
        
        try:
            with pdfplumber.open(io.BytesIO(file_content)) as pdf:
                # ğŸ”§ ëª¨ë“  í˜ì´ì§€ì˜ ëª¨ë“  í…Œì´ë¸”
                for page_num, page in enumerate(pdf.pages):
                    tables = page.extract_tables()
                    if not tables:
                        continue
                    
                    # ğŸ”§ ëª¨ë“  í…Œì´ë¸” ì²˜ë¦¬
                    for table_idx, table in enumerate(tables):
                        if not table or len(table) < 1:
                            continue
                        
                        # ğŸ”§ ëª¨ë“  í–‰ì„ í—¤ë”ë¡œ ì‹œë„
                        for header_idx in range(min(len(table), 3)):
                            header_row = table[header_idx]
                            if not header_row:
                                continue
                            
                            # ğŸ”§ ëª¨ë“  ë°ì´í„° í–‰ ì²˜ë¦¬
                            for row_idx in range(len(table)):
                                if row_idx == header_idx:
                                    continue
                                
                                row = table[row_idx]
                                if not row:
                                    continue
                                
                                # ğŸ”§ ëª¨ë“  ì…€ ì¡°í•©
                                for col_idx in range(min(len(header_row), len(row))):
                                    header_cell = header_row[col_idx]
                                    data_cell = row[col_idx]
                                    
                                    if header_cell and data_cell:
                                        header_str = str(header_cell).strip()
                                        data_str = str(data_cell).strip()
                                        
                                        if self._is_minimal_valid(header_str, data_str):
                                            field_id = f"table_{page_num}_{table_idx}_{header_idx}_{row_idx}_{col_idx}"
                                            
                                            results[field_id] = FieldContext(
                                                field_name=header_str,
                                                value=data_str,
                                                bbox=(0, 0, 0, 0),
                                                page=page_num,
                                                confidence=self._calculate_base_confidence('table'),
                                                context_type='table',
                                                extraction_method='table_maximum'
                                            )
        
        except Exception as e:
            logger.error(f"í…Œì´ë¸” ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return results
    
    def _extract_all_text_lines(self, file_content: bytes) -> Dict[str, FieldContext]:
        """ëª¨ë“  í…ìŠ¤íŠ¸ ë¼ì¸ ì¶”ì¶œ"""
        results = {}
        
        if not PDF_LIBRARIES_AVAILABLE:
            return results
        
        try:
            with pdfplumber.open(io.BytesIO(file_content)) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    if not page_text:
                        continue
                    
                    # ğŸ”§ ëª¨ë“  ë¼ì¸ ì²˜ë¦¬
                    lines = page_text.split('\n')
                    for line_idx, line in enumerate(lines):
                        line = line.strip()
                        if len(line) > 2:  # ìµœì†Œ ê¸¸ì´ë§Œ ì²´í¬
                            # ë¼ì¸ì„ í•„ë“œë¡œ ì¶”ì¶œ
                            field_id = f"text_line_{page_num}_{line_idx}"
                            
                            # ì½œë¡ ì´ ìˆìœ¼ë©´ ë¶„ë¦¬ ì‹œë„
                            if ':' in line:
                                parts = line.split(':', 1)
                                if len(parts) == 2:
                                    key, value = parts[0].strip(), parts[1].strip()
                                    if self._is_minimal_valid(key, value):
                                        results[field_id] = FieldContext(
                                            field_name=key,
                                            value=value,
                                            bbox=(0, 0, 0, 0),
                                            page=page_num,
                                            confidence=self._calculate_base_confidence('text'),
                                            context_type='text_line',
                                            extraction_method='text_line'
                                        )
                            else:
                                # ì „ì²´ ë¼ì¸ì„ ê°’ìœ¼ë¡œ ì‚¬ìš©
                                results[field_id] = FieldContext(
                                    field_name=f"TEXT_LINE_{line_idx}",
                                    value=line,
                                    bbox=(0, 0, 0, 0),
                                    page=page_num,
                                    confidence=self._calculate_base_confidence('text'),
                                    context_type='text_content',
                                    extraction_method='text_line'
                                )
        
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ë¼ì¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return results
    
    def _extract_all_patterns(self, file_content: bytes) -> Dict[str, FieldContext]:
        """ëª¨ë“  íŒ¨í„´ ì¶”ì¶œ"""
        results = {}
        
        if not PDF_LIBRARIES_AVAILABLE:
            return results
        
        try:
            with pdfplumber.open(io.BytesIO(file_content)) as pdf:
                all_text = ""
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        all_text += page_text + "\n"
                
                if not all_text:
                    return results
                
                # ğŸ”§ íŠ¹ìˆ˜ íŒ¨í„´ë“¤ ì¶”ê°€ ê²€ìƒ‰
                special_patterns = {
                    'feed_types': r'(AM\s+FEED|AH\s+FEED|AM_FEED|AH_FEED)',
                    'api_classes': r'(API\s+CLASS\s+[A-Z0-9\-]+)',
                    'materials': r'(CASING|IMPELLER|SHAFT)\s*([A-Z0-9\s\-]+)',
                    'checkboxes': r'([â– â–¡â˜‘â˜])\s*([A-Z][A-Z\s]+)',
                    'units': r'(\d+\.?\d*)\s*([A-Za-zÂ°/%]+)',
                    'codes': r'([A-Z]{2,4}-[0-9A-Z\-]+)',
                    'dates': r'(\d{4}-\d{2}-\d{2})',
                    'revisions': r'(REV\.?\s*\d+[A-Z]*)',
                    'pages': r'(\d+\s+OF\s+\d+)',
                    'notes': r'(NOTE\s+\d+)',
                    'percentages': r'(\d+\.?\d*)\s*(%|WT%)',
                    'temperatures': r'(\d+\.?\d*)\s*(Â°?[CF]|DEG)',
                    'pressures': r'(\d+\.?\d*)\s*(PSI|BAR|kg/cm2)',
                    'flows': r'(\d+\.?\d*)\s*(m3/h|GPM|L/S)',
                }
                
                for category, pattern in special_patterns.items():
                    matches = re.finditer(pattern, all_text, re.IGNORECASE | re.MULTILINE)
                    
                    for match_idx, match in enumerate(matches):
                        groups = match.groups()
                        if groups:
                            if len(groups) >= 2:
                                key, value = groups[0], groups[1]
                            else:
                                key, value = category.upper(), groups[0]
                            
                            field_id = f"pattern_{category}_{match_idx}"
                            
                            results[field_id] = FieldContext(
                                field_name=str(key).strip(),
                                value=str(value).strip(),
                                bbox=(0, 0, 0, 0),
                                page=0,
                                confidence=self._calculate_base_confidence('pattern'),
                                context_type='pattern',
                                extraction_method=f'pattern_{category}'
                            )
        
        except Exception as e:
            logger.error(f"íŒ¨í„´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return results
    
    def _extract_meaningful_words(self, file_content: bytes) -> Dict[str, FieldContext]:
        """ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ"""
        results = {}
        
        if not PDF_LIBRARIES_AVAILABLE:
            return results
        
        try:
            with pdfplumber.open(io.BytesIO(file_content)) as pdf:
                all_text = ""
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        all_text += page_text + " "
                
                # ğŸ”§ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ
                words = re.findall(r'\b[A-Z]{2,}[A-Z0-9\-]*\b', all_text)
                word_counts = Counter(words)
                
                # ğŸ”§ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ì„ í•„ë“œë¡œ ì‚¬ìš©
                for word_idx, (word, count) in enumerate(word_counts.most_common(100)):
                    if len(word) >= 2 and count >= 1:
                        field_id = f"word_{word_idx}"
                        
                        results[field_id] = FieldContext(
                            field_name=word,
                            value=f"Found {count} times",
                            bbox=(0, 0, 0, 0),
                            page=0,
                            confidence=self._calculate_base_confidence('word'),
                            context_type='word',
                            extraction_method='word_extraction'
                        )
        
        except Exception as e:
            logger.error(f"ë‹¨ì–´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return results
    
    def _extract_word_pairs(self, words: List[Dict], page_num: int) -> Dict[str, FieldContext]:
        """ë‹¨ì–´ ê¸°ë°˜ í‚¤-ê°’ ìŒ ì¶”ì¶œ"""
        results = {}
        
        try:
            for i, word in enumerate(words):
                text = word['text']
                
                # ğŸ”§ ëª¨ë“  ê°€ëŠ¥í•œ í‚¤ í›„ë³´
                if len(text) >= 2:
                    # ë‹¤ìŒ ë‹¨ì–´ë“¤ì„ ê°’ìœ¼ë¡œ ì‹œë„
                    for j in range(i + 1, min(i + 5, len(words))):
                        next_word = words[j]
                        next_text = next_word['text']
                        
                        if len(next_text) >= 1:
                            field_id = f"word_pair_{page_num}_{i}_{j}"
                            
                            results[field_id] = FieldContext(
                                field_name=text,
                                value=next_text,
                                bbox=(word['x0'], word['top'], next_word['x1'], next_word['bottom']),
                                page=page_num,
                                confidence=self._calculate_base_confidence('word_pair'),
                                context_type='word_pair',
                                extraction_method='word_pair'
                            )
        
        except Exception as e:
            logger.debug(f"ë‹¨ì–´ ìŒ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return results
    
    def _is_minimal_valid(self, key: str, value: str) -> bool:
        """ìµœì†Œí•œì˜ ìœ íš¨ì„± ê²€ì¦ë§Œ"""
        try:
            key_str = str(key).strip()
            value_str = str(value).strip()
            
            # ğŸ”§ ë§¤ìš° ê´€ëŒ€í•œ ì¡°ê±´
            return (len(key_str) >= 1 and len(value_str) >= 1 and
                   len(key_str) <= 200 and len(value_str) <= 500 and
                   key_str != value_str)
        except Exception:
            return False
    
    def _calculate_base_confidence(self, method: str) -> float:
        """ê¸°ë³¸ ì‹ ë¢°ë„ ê³„ì‚°"""
        base_confidences = {
            'spatial': 0.60,
            'table': 0.70,
            'text': 0.50,
            'pattern': 0.65,
            'word': 0.40,
            'word_pair': 0.45
        }
        
        return base_confidences.get(method, self.min_confidence)
    
    def _create_minimal_error_result(self, filename: str, error_msg: str) -> Dict:
        """ìµœì†Œ ì˜¤ë¥˜ ê²°ê³¼"""
        return {
            'doc_id': f"error_{hashlib.md5(filename.encode()).hexdigest()[:8]}",
            'extracted_fields': [],
            'metadata': {'error': error_msg, 'page_count': 1},
            'total_confidence': 0.0,
            'extraction_summary': {
                'total_fields': 0,
                'high_confidence_fields': 0,
                'extraction_methods_used': [],
                'context_types': {},
                'error': error_msg
            }
        }

# ğŸ”§ ì™„ì „ ë¹„êµ ì‹œìŠ¤í…œ
class CompleteComparisonSystem:
    """ì™„ì „í•œ ë¬¸ì„œ ë¹„êµ ì‹œìŠ¤í…œ"""
    
    def __init__(self, db_manager):
        self.db = db_manager
        logger.info("âœ… ì™„ì „ ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    
    def generate_complete_comparison_matrix(self) -> pd.DataFrame:
        """ì™„ì „í•œ ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        try:
            # ğŸ”§ ëª¨ë“  ë¬¸ì„œì˜ ëª¨ë“  í•„ë“œ ê°€ì ¸ì˜¤ê¸°
            all_fields_query = """
                SELECT 
                    d.filename,
                    ef.field_name,
                    ef.field_value,
                    ef.confidence,
                    ef.extraction_method,
                    ef.context_type
                FROM extracted_fields ef
                JOIN documents d ON ef.doc_id = d.doc_id
                ORDER BY ef.field_name, d.filename
            """
            
            all_fields_df = self.db.execute_query(all_fields_query)
            
            if all_fields_df.empty:
                return pd.DataFrame()
            
            # ğŸ”§ ëª¨ë“  ê³ ìœ  í•„ë“œëª… ìˆ˜ì§‘
            all_unique_fields = set(all_fields_df['field_name'].unique())
            all_documents = list(all_fields_df['filename'].unique())
            
            logger.info(f"ğŸ“Š ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤: {len(all_unique_fields)} í•„ë“œ x {len(all_documents)} ë¬¸ì„œ")
            
            # ğŸ”§ ì™„ì „í•œ ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
            comparison_matrix = []
            
            for field_name in sorted(all_unique_fields):
                row_data = {'Field_Name': field_name}
                
                # ê° ë¬¸ì„œë³„ë¡œ í•´ë‹¹ í•„ë“œ ê°’ ì°¾ê¸°
                for doc_name in all_documents:
                    field_data = all_fields_df[
                        (all_fields_df['field_name'] == field_name) & 
                        (all_fields_df['filename'] == doc_name)
                    ]
                    
                    if not field_data.empty:
                        # ê°’ì´ ìˆëŠ” ê²½ìš°
                        value = field_data.iloc[0]['field_value']
                        confidence = field_data.iloc[0]['confidence']
                        row_data[f"{doc_name}_Value"] = value
                        row_data[f"{doc_name}_Confidence"] = confidence
                        row_data[f"{doc_name}_Status"] = "âœ… ìˆìŒ"
                    else:
                        # ê°’ì´ ì—†ëŠ” ê²½ìš°
                        row_data[f"{doc_name}_Value"] = "âŒ ê°’ ì—†ìŒ"
                        row_data[f"{doc_name}_Confidence"] = 0.0
                        row_data[f"{doc_name}_Status"] = "âŒ ì—†ìŒ"
                
                # ğŸ”§ í•„ë“œë³„ ë¹„êµ í†µê³„ ê³„ì‚°
                values = [row_data.get(f"{doc}_Value", "") for doc in all_documents]
                non_empty_values = [v for v in values if v != "âŒ ê°’ ì—†ìŒ"]
                
                if len(non_empty_values) > 1:
                    # ê°’ë“¤ì´ ëª¨ë‘ ê°™ì€ì§€ í™•ì¸
                    unique_values = set(non_empty_values)
                    if len(unique_values) == 1:
                        row_data['Comparison_Result'] = "ğŸŸ¢ ëª¨ë“  ê°’ ë™ì¼"
                    else:
                        row_data['Comparison_Result'] = f"ğŸ”´ ê°’ ì°¨ì´ ({len(unique_values)}ê°œ ë‹¤ë¥¸ ê°’)"
                elif len(non_empty_values) == 1:
                    row_data['Comparison_Result'] = "ğŸŸ¡ 1ê°œ ë¬¸ì„œì—ë§Œ ìˆìŒ"
                else:
                    row_data['Comparison_Result'] = "âš« ëª¨ë“  ë¬¸ì„œì—ì„œ ì—†ìŒ"
                
                row_data['Documents_With_Field'] = len(non_empty_values)
                row_data['Total_Documents'] = len(all_documents)
                
                comparison_matrix.append(row_data)
            
            return pd.DataFrame(comparison_matrix)
            
        except Exception as e:
            logger.error(f"ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì˜¤ë¥˜: {e}")
            return pd.DataFrame()
    
    def save_comparison_results(self, comparison_df: pd.DataFrame):
        """ë¹„êµ ê²°ê³¼ ì €ì¥"""
        try:
            # ğŸ”§ ë¹„êµ ê²°ê³¼ í…Œì´ë¸” ìƒì„±
            self.db.conn.execute("""
                CREATE TABLE IF NOT EXISTS field_comparisons (
                    id INTEGER PRIMARY KEY,
                    field_name VARCHAR NOT NULL,
                    comparison_result VARCHAR,
                    documents_with_field INTEGER,
                    total_documents INTEGER,
                    comparison_data JSON,
                    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            self.db.conn.execute("DELETE FROM field_comparisons")
            
            # ìƒˆë¡œìš´ ë¹„êµ ê²°ê³¼ ì €ì¥
            for _, row in comparison_df.iterrows():
                comparison_data = row.to_dict()
                
                self.db.conn.execute("""
                    INSERT INTO field_comparisons (
                        field_name, comparison_result, documents_with_field, 
                        total_documents, comparison_data
                    ) VALUES (?, ?, ?, ?, ?)
                """, [
                    row['Field_Name'],
                    row.get('Comparison_Result', ''),
                    row.get('Documents_With_Field', 0),
                    row.get('Total_Documents', 0),
                    json.dumps(comparison_data)
                ])
            
            logger.info(f"âœ… ë¹„êµ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(comparison_df)} í•„ë“œ")
            
        except Exception as e:
            logger.error(f"ë¹„êµ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")

# DuckDB ë§¤ë‹ˆì € (í™•ì¥ëœ ìŠ¤í‚¤ë§ˆ)
class AdvancedDuckDBManager:
    """ê³ ê¸‰ DuckDB ë§¤ë‹ˆì € - ë¹„êµ ê¸°ëŠ¥ í¬í•¨"""
    
    def __init__(self, db_path: str = "plant_documents.db"):
        self.db_path = db_path
        try:
            self.conn = duckdb.connect(db_path)
            self.init_database()
            logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ: {db_path}")
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            self.conn = duckdb.connect(":memory:")
            self.db_path = ":memory:"
            self.init_database()
    
    def init_database(self):
        """í™•ì¥ëœ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ"""
        try:
            # ë¬¸ì„œ í…Œì´ë¸”
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS documents (
                    doc_id VARCHAR PRIMARY KEY,
                    filename VARCHAR NOT NULL,
                    upload_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    file_size INTEGER,
                    page_count INTEGER,
                    total_confidence DECIMAL(4,3),
                    total_fields INTEGER,
                    metadata JSON
                )
            """)
            
            # í•„ë“œ í…Œì´ë¸”
            self.conn.execute("""
                CREATE SEQUENCE IF NOT EXISTS extracted_fields_seq START 1
            """)
            
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS extracted_fields (
                    id INTEGER PRIMARY KEY DEFAULT nextval('extracted_fields_seq'),
                    doc_id VARCHAR,
                    field_name VARCHAR NOT NULL,
                    field_value TEXT,
                    confidence DECIMAL(5,4),
                    extraction_method VARCHAR,
                    page_number INTEGER,
                    bbox_x1 DECIMAL(10,3),
                    bbox_y1 DECIMAL(10,3),
                    bbox_x2 DECIMAL(10,3),
                    bbox_y2 DECIMAL(10,3),
                    context_type VARCHAR,
                    hierarchy_level INTEGER DEFAULT 0,
                    parent_field VARCHAR,
                    conditions JSON,
                    font_info JSON,
                    extraction_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (doc_id) REFERENCES documents(doc_id)
                )
            """)
            
            # ğŸ”§ ë¹„êµ ê²°ê³¼ í…Œì´ë¸” ì¶”ê°€
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS field_comparisons (
                    id INTEGER PRIMARY KEY,
                    field_name VARCHAR NOT NULL,
                    comparison_result VARCHAR,
                    documents_with_field INTEGER,
                    total_documents INTEGER,
                    comparison_data JSON,
                    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # ì¸ë±ìŠ¤ ìƒì„±
            self.conn.execute("CREATE INDEX IF NOT EXISTS idx_extracted_fields_doc_id ON extracted_fields(doc_id)")
            self.conn.execute("CREATE INDEX IF NOT EXISTS idx_extracted_fields_name ON extracted_fields(field_name)")
            self.conn.execute("CREATE INDEX IF NOT EXISTS idx_field_comparisons_name ON field_comparisons(field_name)")
            
            logger.info("í™•ì¥ëœ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def insert_document_with_metadata(self, doc_id: str, filename: str, file_size: int, extraction_result: Dict):
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            metadata_json = json.dumps(extraction_result.get('metadata', {}))
            total_fields = len(extraction_result.get('extracted_fields', []))
            
            self.conn.execute("""
                INSERT OR REPLACE INTO documents (
                    doc_id, filename, file_size, page_count, total_confidence, total_fields, metadata
                ) VALUES (?, ?, ?, ?, ?, ?, ?)
            """, [
                doc_id, filename, file_size,
                extraction_result.get('metadata', {}).get('page_count', 1),
                extraction_result.get('total_confidence', 0.0),
                total_fields,
                metadata_json
            ])
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def insert_extracted_fields_batch(self, fields_data: List[Dict]):
        """í•„ë“œ ë°ì´í„° ì¼ê´„ ì €ì¥"""
        try:
            batch_data = []
            for field in fields_data:
                try:
                    conditions_json = json.dumps(field.get('conditions', {}))
                    font_info_json = json.dumps(field.get('font_info', {}))
                    
                    bbox = field.get('bbox', (0, 0, 0, 0))
                    if not bbox or len(bbox) != 4:
                        bbox = (0, 0, 0, 0)
                    
                    batch_data.append([
                        field.get('doc_id', ''),
                        field.get('field_name', 'Unknown'),
                        field.get('value', ''),
                        float(field.get('confidence', 0.50)),
                        field.get('extraction_method', 'unknown'),
                        int(field.get('page', 0)),
                        float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3]),
                        field.get('context_type', 'unknown'),
                        int(field.get('hierarchy_level', 0)),
                        field.get('parent_field', ''),
                        conditions_json,
                        font_info_json
                    ])
                except Exception as e:
                    logger.error(f"í•„ë“œ ë°ì´í„° ì¤€ë¹„ ì˜¤ë¥˜: {e}")
                    continue
            
            if batch_data:
                self.conn.executemany("""
                    INSERT INTO extracted_fields (
                        doc_id, field_name, field_value, confidence, extraction_method,
                        page_number, bbox_x1, bbox_y1, bbox_x2, bbox_y2,
                        context_type, hierarchy_level, parent_field, conditions, font_info
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, batch_data)
                
                logger.info(f"í•„ë“œ ì¼ê´„ ì €ì¥ ì™„ë£Œ: {len(batch_data)}ê°œ")
        
        except Exception as e:
            logger.error(f"í•„ë“œ ì¼ê´„ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def execute_query(self, query: str) -> pd.DataFrame:
        """ì¿¼ë¦¬ ì‹¤í–‰"""
        try:
            return self.conn.execute(query).df()
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return pd.DataFrame()

def process_files_maximum_extraction(uploaded_files, db, pdf_processor, comparison_system):
    """ìµœëŒ€ ì¶”ì¶œ íŒŒì¼ ì²˜ë¦¬"""
    st.header("ğŸš€ ìµœëŒ€ í•„ë“œ ì¶”ì¶œ ë° ì™„ì „ ë¹„êµ ì‹œìŠ¤í…œ")
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    results_container = st.container()
    
    total_files = len(uploaded_files)
    processed_results = []
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            status_text.text(f"ìµœëŒ€ ì¶”ì¶œ ì¤‘: {uploaded_file.name} ({i+1}/{total_files})")
            
            file_size = len(uploaded_file.getvalue())
            
            # ğŸ”§ ìµœëŒ€ ì¶”ì¶œ ì‹¤í–‰
            with st.spinner(f"'{uploaded_file.name}' ëª¨ë“  í•„ë“œ ì¶”ì¶œ ì¤‘..."):
                extraction_result = pdf_processor.extract_all_possible_fields(
                    uploaded_file.getvalue(), 
                    uploaded_file.name
                )
            
            # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
            debug_info = extraction_result.get('extraction_summary', {}).get('debug_info', {})
            
            with results_container:
                if debug_info.get('fields_at_each_step'):
                    st.write(f"**{uploaded_file.name} ìµœëŒ€ ì¶”ì¶œ ê²°ê³¼:**")
                    total_extracted = 0
                    for step, count in debug_info['fields_at_each_step'].items():
                        st.write(f"- {step}: {count} í•„ë“œ")
                        total_extracted += count
                    st.write(f"- **ì´ ì¶”ì¶œ: {total_extracted} í•„ë“œ**")
                
                if debug_info.get('extraction_stats'):
                    stats = debug_info['extraction_stats']
                    st.write(f"- ìµœì¢… ì²˜ë¦¬: {stats.get('total_final', 0)} í•„ë“œ")
                    st.write(f"- í•„í„°ë§: {'ë¹„í™œì„±í™”' if not stats.get('filtering_applied', True) else 'í™œì„±í™”'}")
            
            # ğŸ”§ ëª¨ë“  í•„ë“œ ì €ì¥ (í•„í„°ë§ ì—†ìŒ)
            if extraction_result['extracted_fields']:
                # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                db.insert_document_with_metadata(
                    extraction_result['doc_id'],
                    uploaded_file.name,
                    file_size,
                    extraction_result
                )
                
                db.insert_extracted_fields_batch(extraction_result['extracted_fields'])
                
                # ì„±ê³µ ê¸°ë¡
                processed_results.append({
                    'filename': uploaded_file.name,
                    'status': 'success',
                    'total_fields': len(extraction_result['extracted_fields']),
                    'confidence': extraction_result['total_confidence'],
                    'extraction_methods': len(extraction_result['extraction_summary']['extraction_methods_used'])
                })
                
                st.success(f"âœ… {uploaded_file.name}: {len(extraction_result['extracted_fields'])} í•„ë“œ ì¶”ì¶œ ì„±ê³µ!")
            else:
                st.error(f"âŒ {uploaded_file.name}: í•„ë“œ ì¶”ì¶œ ì‹¤íŒ¨")
        
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {uploaded_file.name}: {e}")
            st.error(f"âŒ {uploaded_file.name}: ì²˜ë¦¬ ì˜¤ë¥˜ - {str(e)}")
        
        finally:
            progress_bar.progress((i + 1) / total_files)
    
    # ğŸ”§ ì™„ì „ ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
    if len(processed_results) >= 2:
        with st.spinner("ì™„ì „ ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì¤‘..."):
            comparison_matrix = comparison_system.generate_complete_comparison_matrix()
            if not comparison_matrix.empty:
                comparison_system.save_comparison_results(comparison_matrix)
                st.success("âœ… ì™„ì „ ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì™„ë£Œ!")
    
    # ìµœì¢… ìš”ì•½
    status_text.text("âœ… ìµœëŒ€ ì¶”ì¶œ ë° ë¹„êµ ì™„ë£Œ!")
    
    success_count = sum(1 for r in processed_results if r['status'] == 'success')
    total_fields = sum(r.get('total_fields', 0) for r in processed_results if r['status'] == 'success')
    
    st.success(f"""
    ğŸ“Š **ìµœëŒ€ ì¶”ì¶œ ì™„ë£Œ ìš”ì•½**
    - ì„±ê³µ: {success_count}/{total_files} ë¬¸ì„œ
    - ì´ ì¶”ì¶œ í•„ë“œ: {total_fields}ê°œ
    - í•„í„°ë§: ì™„ì „ ë¹„í™œì„±í™”
    - ë¹„êµ ì‹œìŠ¤í…œ: í™œì„±í™”
    """)
    
    if processed_results:
        results_df = pd.DataFrame(processed_results)
        st.dataframe(results_df, use_container_width=True)

# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    
    st.set_page_config(
        page_title="Complete Field Extraction & Comparison System",
        page_icon="",
        layout="wide"
    )
    
    st.markdown("""
        <div style="padding: 1.5rem; text-align: center; margin-bottom: 2rem;">                   
            <h1>Complete Field Extraction & Comparison System</h1>
            <p><strong>ëª¨ë“  í•„ë“œ ì¶”ì¶œ â†’ ì™„ì „ ë¹„êµ â†’ DuckDB ì €ì¥</strong></p>
        </div>
    """, unsafe_allow_html=True)
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    try:
        if 'db_manager' not in st.session_state:
            st.session_state.db_manager = AdvancedDuckDBManager()
        
        if 'ontology_manager' not in st.session_state:
            st.session_state.ontology_manager = ComprehensiveOntologyManager()
        
        if 'pdf_processor' not in st.session_state:
            st.session_state.pdf_processor = MaximumExtractorProcessor(st.session_state.ontology_manager)
        
        if 'comparison_system' not in st.session_state:
            st.session_state.comparison_system = CompleteComparisonSystem(st.session_state.db_manager)
        
        db = st.session_state.db_manager
        pdf_processor = st.session_state.pdf_processor
        comparison_system = st.session_state.comparison_system
        
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.stop()
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ")
        uploaded_files = st.file_uploader(
            "PDF íŒŒì¼ ì„ íƒ", 
            type=['pdf'], 
            accept_multiple_files=True,
            help="ëª¨ë“  í•„ë“œ ì¶”ì¶œ ë° ì™„ì „ ë¹„êµ"
        )
        
        st.header("ğŸš€ ìµœëŒ€ ì¶”ì¶œ ì„¤ì •")
        st.success("ì‹ ë¢°ë„ ì„ê³„ê°’: 0.15 (ìµœì €)")
        st.success("í•„í„°ë§: ì™„ì „ ë¹„í™œì„±í™”")
        st.success("ëª¨ë“  íŒ¨í„´: í™œì„±í™”")
        st.success("ëª¨ë“  í˜ì´ì§€: ì²˜ë¦¬")
        st.success("ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤: ìë™ ìƒì„±")
        
        # í†µê³„
        try:
            doc_count = db.execute_query("SELECT COUNT(*) as count FROM documents").iloc[0]['count']
            field_count = db.execute_query("SELECT COUNT(*) as count FROM extracted_fields").iloc[0]['count']
            unique_fields = db.execute_query("SELECT COUNT(DISTINCT field_name) as count FROM extracted_fields").iloc[0]['count']
            comparison_count = db.execute_query("SELECT COUNT(*) as count FROM field_comparisons").iloc[0]['count']
            
            st.metric("ë¬¸ì„œ ìˆ˜", doc_count)
            st.metric("ì´ í•„ë“œ", field_count)
            st.metric("ê³ ìœ  í•„ë“œ", unique_fields)
            st.metric("ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤", comparison_count)
        except:
            st.metric("ë¬¸ì„œ ìˆ˜", 0)
            st.metric("ì´ í•„ë“œ", 0)
            st.metric("ê³ ìœ  í•„ë“œ", 0)
            st.metric("ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤", 0)
    
    # íƒ­ êµ¬ì„±
    tab1, tab2, tab3 = st.tabs(["ğŸ“„ ìµœëŒ€ ì¶”ì¶œ", "ğŸ” ì™„ì „ ë¹„êµ", "ğŸ“Š ë°ì´í„° ë¶„ì„"])
    
    with tab1:
        # íŒŒì¼ ì²˜ë¦¬
        if uploaded_files:
            process_files_maximum_extraction(uploaded_files, db, pdf_processor, comparison_system)
        
        # ìµœê·¼ ì¶”ì¶œ ê²°ê³¼
        try:
            recent_extractions = db.execute_query("""
                SELECT d.filename, ef.field_name, ef.field_value, ef.confidence, ef.extraction_method
                FROM extracted_fields ef
                JOIN documents d ON ef.doc_id = d.doc_id
                ORDER BY ef.extraction_time DESC
                LIMIT 100
            """)
            
            if not recent_extractions.empty:
                st.header("ğŸ“‹ ìµœê·¼ ì¶”ì¶œ ê²°ê³¼ (ìƒìœ„ 100ê°œ)")
                st.dataframe(recent_extractions, use_container_width=True)
            else:
                st.info("ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        
        except Exception as e:
            st.warning(f"ë°ì´í„° ë¡œë“œ ì¤‘ ë¬¸ì œ: {e}")
    
    with tab2:
        st.header("ğŸ” ì™„ì „ ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤")
        
        try:
            # ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤ í‘œì‹œ
            comparison_matrix = comparison_system.generate_complete_comparison_matrix()
            
            if not comparison_matrix.empty:
                st.success(f"âœ… ì™„ì „ ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤: {len(comparison_matrix)} í•„ë“œ")
                
                # í•„í„°ë§ ì˜µì…˜
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    show_differences_only = st.checkbox("ì°¨ì´ë‚˜ëŠ” í•„ë“œë§Œ í‘œì‹œ", value=False)
                
                with col2:
                    show_missing_only = st.checkbox("ëˆ„ë½ í•„ë“œë§Œ í‘œì‹œ", value=False)
                
                with col3:
                    min_documents = st.slider("ìµœì†Œ ë¬¸ì„œ ìˆ˜", 1, 10, 1)
                
                # í•„í„°ë§ ì ìš©
                filtered_matrix = comparison_matrix.copy()
                
                if show_differences_only:
                    filtered_matrix = filtered_matrix[filtered_matrix['Comparison_Result'].str.contains('ğŸ”´ ê°’ ì°¨ì´', na=False)]
                
                if show_missing_only:
                    filtered_matrix = filtered_matrix[filtered_matrix['Comparison_Result'].str.contains('ğŸŸ¡ 1ê°œ ë¬¸ì„œì—ë§Œ', na=False)]
                
                if min_documents > 1:
                    filtered_matrix = filtered_matrix[filtered_matrix['Documents_With_Field'] >= min_documents]
                
                # ë¹„êµ ê²°ê³¼ í‘œì‹œ
                if not filtered_matrix.empty:
                    st.dataframe(
                        filtered_matrix,
                        use_container_width=True,
                        column_config={
                            'Comparison_Result': st.column_config.TextColumn(
                                "ë¹„êµ ê²°ê³¼",
                                width="medium"
                            ),
                            'Documents_With_Field': st.column_config.NumberColumn(
                                "í•„ë“œ ë³´ìœ  ë¬¸ì„œ ìˆ˜",
                                format="%d"
                            ),
                            'Total_Documents': st.column_config.NumberColumn(
                                "ì „ì²´ ë¬¸ì„œ ìˆ˜",
                                format="%d"
                            )
                        }
                    )
                    
                    # ë¹„êµ í†µê³„
                    st.subheader("ğŸ“Š ë¹„êµ í†µê³„")
                    
                    total_fields = len(filtered_matrix)
                    identical_fields = len(filtered_matrix[filtered_matrix['Comparison_Result'].str.contains('ğŸŸ¢ ëª¨ë“  ê°’ ë™ì¼', na=False)])
                    different_fields = len(filtered_matrix[filtered_matrix['Comparison_Result'].str.contains('ğŸ”´ ê°’ ì°¨ì´', na=False)])
                    partial_fields = len(filtered_matrix[filtered_matrix['Comparison_Result'].str.contains('ğŸŸ¡ 1ê°œ ë¬¸ì„œì—ë§Œ', na=False)])
                    missing_fields = len(filtered_matrix[filtered_matrix['Comparison_Result'].str.contains('âš« ëª¨ë“  ë¬¸ì„œì—ì„œ ì—†ìŒ', na=False)])
                    
                    stat_col1, stat_col2, stat_col3, stat_col4, stat_col5 = st.columns(5)
                    
                    with stat_col1:
                        st.metric("ì´ í•„ë“œ", total_fields)
                    
                    with stat_col2:
                        st.metric("ë™ì¼ í•„ë“œ", identical_fields, f"{identical_fields/total_fields*100:.1f}%" if total_fields > 0 else "0%")
                    
                    with stat_col3:
                        st.metric("ì°¨ì´ í•„ë“œ", different_fields, f"{different_fields/total_fields*100:.1f}%" if total_fields > 0 else "0%")
                    
                    with stat_col4:
                        st.metric("ë¶€ë¶„ í•„ë“œ", partial_fields, f"{partial_fields/total_fields*100:.1f}%" if total_fields > 0 else "0%")
                    
                    with stat_col5:
                        st.metric("ëˆ„ë½ í•„ë“œ", missing_fields, f"{missing_fields/total_fields*100:.1f}%" if total_fields > 0 else "0%")
                    
                    # ë¹„êµ ê²°ê³¼ ì°¨íŠ¸
                    if total_fields > 0:
                        comparison_stats = {
                            'ë™ì¼': identical_fields,
                            'ì°¨ì´': different_fields, 
                            'ë¶€ë¶„': partial_fields,
                            'ëˆ„ë½': missing_fields
                        }
                        
                        fig = px.pie(
                            values=list(comparison_stats.values()),
                            names=list(comparison_stats.keys()),
                            title="í•„ë“œ ë¹„êµ ê²°ê³¼ ë¶„í¬",
                            color_discrete_map={
                                'ë™ì¼': '#2ecc71',
                                'ì°¨ì´': '#e74c3c',
                                'ë¶€ë¶„': '#f39c12',
                                'ëˆ„ë½': '#34495e'
                            }
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    
                    # CSV ë‹¤ìš´ë¡œë“œ
                    csv = filtered_matrix.to_csv(index=False)
                    st.download_button(
                        label="ğŸ“¥ ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤ CSV ë‹¤ìš´ë¡œë“œ",
                        data=csv,
                        file_name=f"field_comparison_matrix_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                
                else:
                    st.warning("í•„í„° ì¡°ê±´ì— ë§ëŠ” í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            else:
                st.info("ë¹„êµí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ 2ê°œ ì´ìƒì˜ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        
        except Exception as e:
            st.error(f"ë¹„êµ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì˜¤ë¥˜: {e}")
    
    with tab3:
        st.header("ğŸ“Š ì¶”ì¶œ ë°ì´í„° ë¶„ì„")
        
        try:
            # ë¬¸ì„œë³„ í•„ë“œ ìˆ˜ ë¶„ì„
            doc_field_stats = db.execute_query("""
                SELECT 
                    d.filename,
                    d.total_fields,
                    d.total_confidence,
                    COUNT(DISTINCT ef.field_name) as unique_fields,
                    COUNT(ef.id) as total_extracted_fields,
                    AVG(ef.confidence) as avg_field_confidence,
                    COUNT(CASE WHEN ef.confidence >= 0.70 THEN 1 END) as high_confidence_fields
                FROM documents d
                LEFT JOIN extracted_fields ef ON d.doc_id = ef.doc_id
                GROUP BY d.doc_id, d.filename, d.total_fields, d.total_confidence
                ORDER BY total_extracted_fields DESC
            """)
            
            if not doc_field_stats.empty:
                st.subheader("ğŸ“„ ë¬¸ì„œë³„ í•„ë“œ í†µê³„")
                
                # ë¬¸ì„œë³„ í•„ë“œ ìˆ˜ ì°¨íŠ¸
                fig1 = px.bar(
                    doc_field_stats,
                    x='filename',
                    y='total_extracted_fields',
                    title="ë¬¸ì„œë³„ ì¶”ì¶œëœ í•„ë“œ ìˆ˜",
                    color='avg_field_confidence',
                    color_continuous_scale='Viridis'
                )
                fig1.update_xaxes(tickangle=45)
                st.plotly_chart(fig1, use_container_width=True)
                
                # ì‹ ë¢°ë„ ë¶„í¬
                fig2 = px.histogram(
                    doc_field_stats,
                    x='avg_field_confidence',
                    nbins=20,
                    title="ë¬¸ì„œë³„ í‰ê·  ì‹ ë¢°ë„ ë¶„í¬"
                )
                st.plotly_chart(fig2, use_container_width=True)
                
                # ìƒì„¸ í†µê³„ í…Œì´ë¸”
                st.dataframe(
                    doc_field_stats,
                    use_container_width=True,
                    column_config={
                        'avg_field_confidence': st.column_config.ProgressColumn(
                            "í‰ê·  ì‹ ë¢°ë„",
                            format="%.3f",
                            min_value=0.0,
                            max_value=1.0
                        ),
                        'total_confidence': st.column_config.ProgressColumn(
                            "ë¬¸ì„œ ì‹ ë¢°ë„",
                            format="%.3f",
                            min_value=0.0,
                            max_value=1.0
                        )
                    }
                )
            
            # í•„ë“œ íƒ€ì…ë³„ ë¶„ì„
            field_type_stats = db.execute_query("""
                SELECT 
                    field_name,
                    COUNT(*) as occurrence_count,
                    COUNT(DISTINCT doc_id) as document_count,
                    AVG(confidence) as avg_confidence,
                    MIN(confidence) as min_confidence,
                    MAX(confidence) as max_confidence,
                    STRING_AGG(DISTINCT extraction_method, ', ') as extraction_methods
                FROM extracted_fields
                GROUP BY field_name
                HAVING COUNT(*) > 1
                ORDER BY occurrence_count DESC
                LIMIT 50
            """)
            
            if not field_type_stats.empty:
                st.subheader("ğŸ·ï¸ í•„ë“œ íƒ€ì…ë³„ í†µê³„ (ìƒìœ„ 50ê°œ)")
                
                # í•„ë“œë³„ ì¶œí˜„ ë¹ˆë„
                fig3 = px.bar(
                    field_type_stats.head(20),
                    x='field_name',
                    y='occurrence_count',
                    title="í•„ë“œë³„ ì¶œí˜„ ë¹ˆë„ (ìƒìœ„ 20ê°œ)",
                    color='avg_confidence',
                    color_continuous_scale='RdYlGn'
                )
                fig3.update_xaxes(tickangle=45)
                st.plotly_chart(fig3, use_container_width=True)
                
                # í•„ë“œë³„ ë¬¸ì„œ ì»¤ë²„ë¦¬ì§€
                fig4 = px.scatter(
                    field_type_stats,
                    x='document_count',
                    y='avg_confidence',
                    size='occurrence_count',
                    hover_data=['field_name'],
                    title="í•„ë“œë³„ ë¬¸ì„œ ì»¤ë²„ë¦¬ì§€ vs í‰ê·  ì‹ ë¢°ë„"
                )
                st.plotly_chart(fig4, use_container_width=True)
                
                # ìƒì„¸ í…Œì´ë¸”
                st.dataframe(
                    field_type_stats,
                    use_container_width=True,
                    column_config={
                        'avg_confidence': st.column_config.ProgressColumn(
                            "í‰ê·  ì‹ ë¢°ë„",
                            format="%.3f",
                            min_value=0.0,
                            max_value=1.0
                        ),
                        'min_confidence': st.column_config.ProgressColumn(
                            "ìµœì†Œ ì‹ ë¢°ë„",
                            format="%.3f",
                            min_value=0.0,
                            max_value=1.0
                        ),
                        'max_confidence': st.column_config.ProgressColumn(
                            "ìµœëŒ€ ì‹ ë¢°ë„",
                            format="%.3f",
                            min_value=0.0,
                            max_value=1.0
                        )
                    }
                )
            
            # ì¶”ì¶œ ë°©ë²•ë³„ ì„±ëŠ¥ ë¶„ì„
            method_stats = db.execute_query("""
                SELECT 
                    extraction_method,
                    context_type,
                    COUNT(*) as field_count,
                    AVG(confidence) as avg_confidence,
                    COUNT(DISTINCT field_name) as unique_fields,
                    COUNT(DISTINCT doc_id) as document_coverage
                FROM extracted_fields
                GROUP BY extraction_method, context_type
                ORDER BY field_count DESC
            """)
            
            if not method_stats.empty:
                st.subheader("ğŸ”§ ì¶”ì¶œ ë°©ë²•ë³„ ì„±ëŠ¥")
                
                # ë°©ë²•ë³„ í•„ë“œ ìˆ˜
                fig5 = px.treemap(
                    method_stats,
                    path=['extraction_method', 'context_type'],
                    values='field_count',
                    color='avg_confidence',
                    title="ì¶”ì¶œ ë°©ë²•ë³„ í•„ë“œ ìˆ˜ ë° ì‹ ë¢°ë„"
                )
                st.plotly_chart(fig5, use_container_width=True)
                
                # ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”
                st.dataframe(
                    method_stats,
                    use_container_width=True,
                    column_config={
                        'avg_confidence': st.column_config.ProgressColumn(
                            "í‰ê·  ì‹ ë¢°ë„",
                            format="%.3f",
                            min_value=0.0,
                            max_value=1.0
                        )
                    }
                )
        
        except Exception as e:
            st.error(f"ë°ì´í„° ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    # í•˜ë‹¨ ì •ë³´
    st.markdown("---")
    st.markdown("""
        <div style="text-align: center; color: #7f8c8d;">
            <p><strong>Complete Field Extraction & Comparison System</strong></p>
            <p>Yonsei University</p>
        </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()