import os
import base64
import json
import pandas as pd
import networkx as nx
import plotly.graph_objects as go
from dash import Dash, dcc, html, Input, Output, State
import dash_bootstrap_components as dbc
from collections import Counter
import tempfile
import re
import sqlite3
import time  # ‚Üê Ïù¥ Ï§Ñ Ï∂îÍ∞Ä
from datetime import datetime

# PDF Ï≤òÎ¶¨ ÎùºÏù¥Î∏åÎü¨Î¶¨ (ÏÑ†ÌÉùÏ†Å)
try:
    from pdfminer.high_level import extract_text
    PDF_AVAILABLE = True
except ImportError:
    print("PDFMiner not available - using mock text extraction")
    PDF_AVAILABLE = False

# DuckDB ÏßÄÏõê (ÏÑ†ÌÉùÏ†Å)
try:
    import duckdb
    DUCKDB_AVAILABLE = True
except ImportError:
    print("DuckDB not available - using SQLite instead")
    DUCKDB_AVAILABLE = False

# R-GCN Í¥ÄÎ†® import Î∞è ÏÑ§Ï†ï (Í∏∞Ï°¥ Ïú†ÏßÄ)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch_geometric.nn import RGCNConv
    from torch_geometric.data import Data
    import numpy as np
    print("‚úÖ R-GCN module loaded successfully")
    
    class RGCNModel(nn.Module):
        def __init__(self, num_nodes, num_relations, num_classes, hidden_dim=64):
            super(RGCNModel, self).__init__()
            self.num_nodes = num_nodes
            self.num_relations = num_relations
            self.num_classes = num_classes
            self.hidden_dim = hidden_dim
            
            self.embedding = nn.Embedding(num_nodes, hidden_dim)
            self.conv1 = RGCNConv(hidden_dim, hidden_dim, num_relations)
            self.conv2 = RGCNConv(hidden_dim, num_classes, num_relations)
            self.dropout = nn.Dropout(0.2)
            
        def forward(self, data):
            x, edge_index, edge_type = data.x, data.edge_index, data.edge_type
            
            x = self.embedding(torch.arange(self.num_nodes).to(x.device))
            x = F.relu(self.conv1(x, edge_index, edge_type))
            x = self.dropout(x)
            x = self.conv2(x, edge_index, edge_type)
            
            return F.log_softmax(x, dim=1)
    
    class RGCNManager:
        def __init__(self):
            self.model = None
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.node_mapping = {}
            self.relation_mapping = {}
            self.is_trained = False
            
        def prepare_data(self, graph_data):
            """Í∑∏ÎûòÌîÑ Îç∞Ïù¥ÌÑ∞Î•º PyTorch Geometric ÌòïÌÉúÎ°ú Î≥ÄÌôò"""
            try:
                nodes = graph_data.get("nodes", [])
                edges = graph_data.get("edges", [])
                
                if not nodes or not edges:
                    print("‚ö†Ô∏è Insufficient data for R-GCN")
                    return None
                
                # ÎÖ∏Îìú Îß§Ìïë
                node_ids = [node["id"] for node in nodes]
                self.node_mapping = {node_id: idx for idx, node_id in enumerate(node_ids)}
                
                # Í¥ÄÍ≥Ñ Îß§Ìïë
                relations = list(set(edge.get("relation", "unknown") for edge in edges))
                self.relation_mapping = {rel: idx for idx, rel in enumerate(relations)}
                
                # Ïó£ÏßÄ Ïù∏Îç±Ïä§ÏôÄ ÌÉÄÏûÖ ÏÉùÏÑ±
                edge_index = []
                edge_type = []
                
                for edge in edges:
                    src = edge.get("source", "")
                    tgt = edge.get("target", "")
                    rel = edge.get("relation", "unknown")
                    
                    if src in self.node_mapping and tgt in self.node_mapping:
                        edge_index.append([self.node_mapping[src], self.node_mapping[tgt]])
                        edge_type.append(self.relation_mapping[rel])
                
                if not edge_index:
                    print("‚ö†Ô∏è No valid edges for R-GCN")
                    return None
                
                # ÎÖ∏Îìú ÌäπÏÑ± (ÎçîÎØ∏ ÌäπÏÑ±)
                num_nodes = len(nodes)
                x = torch.randn(num_nodes, 16)  # 16Ï∞®Ïõê ÌäπÏÑ±
                
                edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
                edge_type = torch.tensor(edge_type, dtype=torch.long)
                
                # ÌÉÄÍ≤ü Î†àÏù¥Î∏î ÏÉùÏÑ± (ÏòàÏ∏°Í∞í Í∏∞Î∞ò)
                predictions = graph_data.get("predictions", [])
                if len(predictions) == num_nodes:
                    y = torch.tensor(predictions, dtype=torch.long)
                else:
                    # Í∏∞Î≥∏Í∞íÏúºÎ°ú ÎÖ∏Îìú ÌÉÄÏûÖ Í∏∞Î∞ò Î†àÏù¥Î∏î ÏÉùÏÑ±
                    y = torch.zeros(num_nodes, dtype=torch.long)
                    for i, node in enumerate(nodes):
                        if node.get("type") == "entity":
                            y[i] = 1
                        elif node.get("type") == "literal":
                            y[i] = 2
                        else:
                            y[i] = 0
                
                data = Data(x=x, edge_index=edge_index, edge_type=edge_type, y=y)
                print(f"‚úÖ R-GCN data prepared: {num_nodes} nodes, {len(edge_index[0])} edges, {len(relations)} relations")
                
                return data
                
            except Exception as e:
                print(f"‚ùå R-GCN data preparation error: {e}")
                return None
        
        def train_model(self, data):
            """R-GCN Î™®Îç∏ ÌõàÎ†®"""
            try:
                if data is None:
                    return False
                
                num_nodes = data.x.size(0)
                num_relations = len(self.relation_mapping)
                num_classes = 3  # entity, literal, unknown
                
                self.model = RGCNModel(num_nodes, num_relations, num_classes).to(self.device)
                optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)
                
                data = data.to(self.device)
                
                self.model.train()
                for epoch in range(100):
                    optimizer.zero_grad()
                    out = self.model(data)
                    loss = F.nll_loss(out, data.y)
                    loss.backward()
                    optimizer.step()
                    
                    if epoch % 20 == 0:
                        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")
                
                self.is_trained = True
                print("‚úÖ R-GCN model training completed")
                return True
                
            except Exception as e:
                print(f"‚ùå R-GCN training error: {e}")
                return False
        
        def predict(self, data):
            """R-GCN ÏòàÏ∏° ÏàòÌñâ"""
            try:
                if self.model is None or not self.is_trained:
                    print("‚ö†Ô∏è R-GCN model not trained")
                    return []
                
                self.model.eval()
                data = data.to(self.device)
                
                with torch.no_grad():
                    out = self.model(data)
                    pred = out.argmax(dim=1)
                
                predictions = pred.cpu().numpy().tolist()
                print(f"‚úÖ R-GCN predictions generated: {len(predictions)} nodes")
                
                return predictions
                
            except Exception as e:
                print(f"‚ùå R-GCN prediction error: {e}")
                return []
        
        def get_status(self):
            """R-GCN ÏÉÅÌÉú Î∞òÌôò"""
            return {
                "device": str(self.device),
                "num_relations": len(self.relation_mapping),
                "num_nodes": len(self.node_mapping),
                "is_trained": self.is_trained,
                "model_loaded": self.model is not None
            }
    
    # R-GCN Í¥ÄÎ¶¨Ïûê Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±
    rgcn_manager = RGCNManager()
    
except ImportError:
    print("‚ö†Ô∏è R-GCN dependencies not available - neural network features disabled")
    rgcn_manager = None

# Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í¥ÄÎ¶¨ ÌÅ¥ÎûòÏä§
class DatabaseManager:
    def __init__(self, db_path="knowledge_graph.db"):
        self.db_path = db_path
        self.use_duckdb = DUCKDB_AVAILABLE
        self.init_database()
    
    def get_connection(self):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Î∞òÌôò"""
        if self.use_duckdb:
            return duckdb.connect(self.db_path.replace('.db', '.duckdb'))
        else:
            return sqlite3.connect(self.db_path)
    
    def init_database(self):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌÖåÏù¥Î∏î Ï¥àÍ∏∞Ìôî"""
        try:
            conn = self.get_connection()
            
            # Î¨∏ÏÑú ÌÖåÏù¥Î∏î
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS documents (
                        id INTEGER PRIMARY KEY,
                        filename TEXT NOT NULL,
                        upload_time TEXT NOT NULL,
                        content_length INTEGER,
                        processing_time REAL,
                        text_preview TEXT
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS documents (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        filename TEXT NOT NULL,
                        upload_time TEXT NOT NULL,
                        content_length INTEGER,
                        processing_time REAL,
                        text_preview TEXT
                    );
                """)
            
            # ÎÖ∏Îìú ÌÖåÏù¥Î∏î
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS nodes (
                        id TEXT PRIMARY KEY,
                        document_id INTEGER,
                        node_type TEXT,
                        ontology_class TEXT,
                        confidence REAL,
                        predicted_class INTEGER,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS nodes (
                        id TEXT PRIMARY KEY,
                        document_id INTEGER,
                        node_type TEXT,
                        ontology_class TEXT,
                        confidence REAL,
                        predicted_class INTEGER,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            
            # Ïó£ÏßÄ ÌÖåÏù¥Î∏î
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS edges (
                        id INTEGER PRIMARY KEY,
                        document_id INTEGER,
                        source_node TEXT,
                        target_node TEXT,
                        relation_type TEXT,
                        confidence REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS edges (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        document_id INTEGER,
                        source_node TEXT,
                        target_node TEXT,
                        relation_type TEXT,
                        confidence REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            
            # Ïò®ÌÜ®Î°úÏßÄ Ìå®ÌÑ¥ ÌÖåÏù¥Î∏î
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS ontology_patterns (
                        id INTEGER PRIMARY KEY,
                        pattern_type TEXT,
                        pattern_value TEXT,
                        frequency INTEGER,
                        confidence_avg REAL,
                        last_seen TEXT,
                        document_count INTEGER
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS ontology_patterns (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        pattern_type TEXT,
                        pattern_value TEXT,
                        frequency INTEGER,
                        confidence_avg REAL,
                        last_seen TEXT,
                        document_count INTEGER
                    );
                """)
            
            # ÎèÑÎ©îÏù∏ Ïù∏ÏÇ¨Ïù¥Ìä∏ ÌÖåÏù¥Î∏î
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS domain_insights (
                        id INTEGER PRIMARY KEY,
                        document_id INTEGER,
                        document_type TEXT,
                        industry_domain TEXT,
                        complexity_score REAL,
                        technical_density REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS domain_insights (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        document_id INTEGER,
                        document_type TEXT,
                        industry_domain TEXT,
                        complexity_score REAL,
                        technical_density REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            
            if self.use_duckdb:
                conn.commit()
            else:
                conn.commit()
            
            conn.close()
            print(f"‚úÖ Database initialized: {'DuckDB' if self.use_duckdb else 'SQLite'}")
            
        except Exception as e:
            print(f"‚ùå Database initialization error: {e}")
    
    def save_processing_results(self, filename, text_content, graph_data, learning_results, processing_time=0):
        """Ï≤òÎ¶¨ Í≤∞Í≥ºÎ•º Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï†ÄÏû•"""
        try:
            conn = self.get_connection()
            current_time = datetime.now().isoformat()
            
            # 1. Î¨∏ÏÑú Ï†ïÎ≥¥ Ï†ÄÏû•
            text_preview = text_content[:500] if text_content else ""
            
            if self.use_duckdb:
                # DuckDB: Í∏∞Ï°¥ ÏµúÎåÄ ID ÌôïÏù∏ÌïòÏó¨ Ï§ëÎ≥µ Î∞©ÏßÄ
                try:
                    max_id_result = conn.execute("SELECT MAX(id) FROM documents").fetchone()
                    doc_id = (max_id_result[0] if max_id_result[0] is not None else 0) + 1
                except:
                    doc_id = 1
                
                conn.execute("""
                    INSERT INTO documents (id, filename, upload_time, content_length, processing_time, text_preview)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, [doc_id, filename, current_time, len(text_content), processing_time, text_preview])
            else:
                # SQLite: AUTO INCREMENT ÏÇ¨Ïö©
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO documents (filename, upload_time, content_length, processing_time, text_preview)
                    VALUES (?, ?, ?, ?, ?)
                """, (filename, current_time, len(text_content), processing_time, text_preview))
                doc_id = cursor.lastrowid
            
            # 2. ÎÖ∏Îìú Ï†ÄÏû• (Ï§ëÎ≥µ ÌôïÏù∏ + Í≥†Ïú† ID ÏÉùÏÑ±)
            nodes = graph_data.get("nodes", [])
            predictions = graph_data.get("predictions", [])
            
            for i, node in enumerate(nodes):
                predicted_class = predictions[i] if i < len(predictions) else 0
                # ÌååÏùºÎ™ÖÍ≥º ÎÖ∏Îìú IDÎ•º Ï°∞Ìï©ÌïòÏó¨ ÏôÑÏ†ÑÌûà Í≥†Ïú†Ìïú ID ÏÉùÏÑ±
                safe_filename = filename.replace(' ', '_').replace('-', '_').replace('.pdf', '').replace('(', '').replace(')', '')
                original_node_id = node['id'].replace(' ', '_').replace('-', '_').replace('/', '_').replace('(', '').replace(')', '')
                node_id = f"{doc_id}_{safe_filename}_{original_node_id}_{i}"  # Ïù∏Îç±Ïä§ÍπåÏßÄ Ï∂îÍ∞ÄÌïòÏó¨ ÏôÑÏ†Ñ Í≥†Ïú†Ìôî
                
                if self.use_duckdb:
                    try:
                        conn.execute("""
                            INSERT INTO nodes (id, document_id, node_type, ontology_class, confidence, predicted_class, creation_time)
                            VALUES (?, ?, ?, ?, ?, ?, ?)
                        """, [
                            node_id, doc_id, node.get("type", "unknown"), 
                            node.get("ontology_class", "Unknown"), node.get("confidence", 0.5),
                            predicted_class, current_time
                        ])
                    except Exception as e:
                        if "Duplicate key" not in str(e):
                            print(f"‚ö†Ô∏è Node insert warning: {e}")
                else:
                    try:
                        conn.execute("""
                            INSERT OR IGNORE INTO nodes (id, document_id, node_type, ontology_class, confidence, predicted_class, creation_time)
                            VALUES (?, ?, ?, ?, ?, ?, ?)
                        """, (
                            node_id, doc_id, node.get("type", "unknown"), 
                            node.get("ontology_class", "Unknown"), node.get("confidence", 0.5),
                            predicted_class, current_time
                        ))
                    except Exception as e:
                        print(f"‚ö†Ô∏è Node insert warning: {e}")
            
            # 3. Ïó£ÏßÄ Ï†ÄÏû• (Ï§ëÎ≥µ ÌôïÏù∏ + Í≥†Ïú† ID ÏÉùÏÑ±)
            edges = graph_data.get("edges", [])
            
            if self.use_duckdb:
                try:
                    max_edge_id_result = conn.execute("SELECT MAX(id) FROM edges").fetchone()
                    edge_id = (max_edge_id_result[0] if max_edge_id_result[0] is not None else 0) + 1
                except:
                    edge_id = 1
                    
                for j, edge in enumerate(edges):
                    try:
                        safe_filename = filename.replace(' ', '_').replace('-', '_').replace('.pdf', '').replace('(', '').replace(')', '')
                        source_id = f"{doc_id}_{safe_filename}_{edge.get('source', '').replace(' ', '_').replace('-', '_').replace('/', '_').replace('(', '').replace(')', '')}"
                        target_id = f"{doc_id}_{safe_filename}_{edge.get('target', '').replace(' ', '_').replace('-', '_').replace('/', '_').replace('(', '').replace(')', '')}"
                        
                        conn.execute("""
                            INSERT INTO edges (id, document_id, source_node, target_node, relation_type, confidence, creation_time)
                            VALUES (?, ?, ?, ?, ?, ?, ?)
                        """, [
                            edge_id, doc_id, source_id, target_id,
                            edge.get("relation", "unknown"), 0.8, current_time
                        ])
                        edge_id += 1
                    except Exception as e:
                        if "Duplicate key" not in str(e):
                            print(f"‚ö†Ô∏è Edge insert warning: {e}")
            else:
                for j, edge in enumerate(edges):
                    try:
                        safe_filename = filename.replace(' ', '_').replace('-', '_').replace('.pdf', '').replace('(', '').replace(')', '')
                        source_id = f"{doc_id}_{safe_filename}_{edge.get('source', '').replace(' ', '_').replace('-', '_').replace('/', '_').replace('(', '').replace(')', '')}"
                        target_id = f"{doc_id}_{safe_filename}_{edge.get('target', '').replace(' ', '_').replace('-', '_').replace('/', '_').replace('(', '').replace(')', '')}"
                        
                        conn.execute("""
                            INSERT INTO edges (document_id, source_node, target_node, relation_type, confidence, creation_time)
                            VALUES (?, ?, ?, ?, ?, ?)
                        """, (
                            doc_id, source_id, target_id,
                            edge.get("relation", "unknown"), 0.8, current_time
                        ))
                    except Exception as e:
                        print(f"‚ö†Ô∏è Edge insert warning: {e}")
            
            # 4. Ïò®ÌÜ®Î°úÏßÄ Ìå®ÌÑ¥ Ï†ÄÏû•/ÏóÖÎç∞Ïù¥Ìä∏ (Ï§ëÎ≥µ ÌôïÏù∏)
            patterns = learning_results.get("patterns", {})
            confidence_scores = learning_results.get("confidence_scores", {})
            
            if self.use_duckdb:
                try:
                    max_pattern_id_result = conn.execute("SELECT MAX(id) FROM ontology_patterns").fetchone()
                    pattern_id = (max_pattern_id_result[0] if max_pattern_id_result[0] is not None else 0) + 1
                except:
                    pattern_id = 1
                    
                for pattern_type, pattern_list in patterns.items():
                    for pattern_value in pattern_list:
                        pattern_str = str(pattern_value)[:200]  # Í∏∏Ïù¥ Ï†úÌïú
                        avg_confidence = sum(confidence_scores.values()) / max(len(confidence_scores), 1)
                        
                        try:
                            conn.execute("""
                                INSERT INTO ontology_patterns (id, pattern_type, pattern_value, frequency, confidence_avg, last_seen, document_count)
                                VALUES (?, ?, ?, ?, ?, ?, ?)
                            """, [pattern_id, pattern_type, pattern_str, 1, avg_confidence, current_time, 1])
                            pattern_id += 1
                        except Exception as e:
                            if "Duplicate key" not in str(e):
                                print(f"‚ö†Ô∏è Pattern insert warning: {e}")
            else:
                for pattern_type, pattern_list in patterns.items():
                    for pattern_value in pattern_list:
                        pattern_str = str(pattern_value)[:200]  # Í∏∏Ïù¥ Ï†úÌïú
                        avg_confidence = sum(confidence_scores.values()) / max(len(confidence_scores), 1)
                        
                        try:
                            conn.execute("""
                                INSERT INTO ontology_patterns (pattern_type, pattern_value, frequency, confidence_avg, last_seen, document_count)
                                VALUES (?, ?, ?, ?, ?, ?)
                            """, (pattern_type, pattern_str, 1, avg_confidence, current_time, 1))
                        except Exception as e:
                            print(f"‚ö†Ô∏è Pattern insert warning: {e}")
            
            # 5. ÎèÑÎ©îÏù∏ Ïù∏ÏÇ¨Ïù¥Ìä∏ Ï†ÄÏû• (Ï§ëÎ≥µ ÌôïÏù∏)
            domain_insights = learning_results.get("domain_insights", {})
            if domain_insights:
                if self.use_duckdb:
                    try:
                        max_insight_id_result = conn.execute("SELECT MAX(id) FROM domain_insights").fetchone()
                        insight_id = (max_insight_id_result[0] if max_insight_id_result[0] is not None else 0) + 1
                    except:
                        insight_id = 1
                        
                    try:
                        conn.execute("""
                            INSERT INTO domain_insights (id, document_id, document_type, industry_domain, complexity_score, technical_density, creation_time)
                            VALUES (?, ?, ?, ?, ?, ?, ?)
                        """, [
                            insight_id, doc_id, domain_insights.get("document_type", "unknown"),
                            domain_insights.get("industry_domain", "unknown"),
                            domain_insights.get("complexity_score", 0),
                            domain_insights.get("technical_density", 0),
                            current_time
                        ])
                    except Exception as e:
                        if "Duplicate key" not in str(e):
                            print(f"‚ö†Ô∏è Insight insert warning: {e}")
                else:
                    try:
                        conn.execute("""
                            INSERT INTO domain_insights (document_id, document_type, industry_domain, complexity_score, technical_density, creation_time)
                            VALUES (?, ?, ?, ?, ?, ?)
                        """, (
                            doc_id, domain_insights.get("document_type", "unknown"),
                            domain_insights.get("industry_domain", "unknown"),
                            domain_insights.get("complexity_score", 0),
                            domain_insights.get("technical_density", 0),
                            current_time
                        ))
                    except Exception as e:
                        print(f"‚ö†Ô∏è Insight insert warning: {e}")
            
            if self.use_duckdb:
                conn.commit()
            else:
                conn.commit()
            
            conn.close()
            
            print(f"‚úÖ Data saved to database - Document ID: {doc_id}")
            return doc_id
            
        except Exception as e:
            print(f"‚ùå Database save error: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def execute_query(self, query):
        """SQL ÏøºÎ¶¨ Ïã§Ìñâ"""
        try:
            conn = self.get_connection()
            
            if self.use_duckdb:
                result = conn.execute(query).fetch_df()
            else:
                result = pd.read_sql_query(query, conn)
            
            conn.close()
            return result
            
        except Exception as e:
            print(f"‚ùå Query execution error: {e}")
            return pd.DataFrame()
    
    def get_database_stats(self):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌÜµÍ≥Ñ Î∞òÌôò"""
        try:
            stats = {}
            
            # Í∏∞Î≥∏ ÌÜµÍ≥Ñ
            stats["total_documents"] = self.execute_query("SELECT COUNT(*) as count FROM documents")["count"].iloc[0]
            stats["total_nodes"] = self.execute_query("SELECT COUNT(*) as count FROM nodes")["count"].iloc[0]
            stats["total_edges"] = self.execute_query("SELECT COUNT(*) as count FROM edges")["count"].iloc[0]
            stats["total_patterns"] = self.execute_query("SELECT COUNT(*) as count FROM ontology_patterns")["count"].iloc[0]
            
            # ÏµúÍ∑º Î¨∏ÏÑú
            recent_docs = self.execute_query("""
                SELECT filename, upload_time, content_length 
                FROM documents 
                ORDER BY upload_time DESC 
                LIMIT 5
            """)
            stats["recent_documents"] = recent_docs.to_dict('records') if not recent_docs.empty else []
            
            # ÎÖ∏Îìú ÌÉÄÏûÖÎ≥Ñ Î∂ÑÌè¨
            node_distribution = self.execute_query("""
                SELECT node_type, COUNT(*) as count 
                FROM nodes 
                GROUP BY node_type
            """)
            stats["node_distribution"] = node_distribution.to_dict('records') if not node_distribution.empty else []
            
            # Ïò®ÌÜ®Î°úÏßÄ ÌÅ¥ÎûòÏä§Î≥Ñ Î∂ÑÌè¨  
            class_distribution = self.execute_query("""
                SELECT ontology_class, COUNT(*) as count, AVG(confidence) as avg_confidence
                FROM nodes 
                GROUP BY ontology_class 
                ORDER BY count DESC
            """)
            stats["class_distribution"] = class_distribution.to_dict('records') if not class_distribution.empty else []
            
            # Í≥†Ïã†Î¢∞ÎèÑ ÏóîÌã∞Ìã∞ Ïàò
            high_confidence_count = self.execute_query("""
                SELECT COUNT(*) as count 
                FROM nodes 
                WHERE confidence > 0.8
            """)["count"].iloc[0]
            stats["high_confidence_entities"] = high_confidence_count
            
            return stats
            
        except Exception as e:
            print(f"‚ùå Database stats error: {e}")
            return {}

# Î¨∏ÏÑú ÎπÑÍµê Í¥ÄÎ¶¨ ÌÅ¥ÎûòÏä§ Ï∂îÍ∞Ä
class DocumentComparisonManager:
    def __init__(self, db_manager):
        self.db_manager = db_manager
        self.document_cache = {}  # Î¨∏ÏÑúÎ≥Ñ Ï∫êÏãú
        
    def add_document(self, doc_id, filename, graph_data, learning_results):
        """Î¨∏ÏÑúÎ•º ÎπÑÍµê Ï∫êÏãúÏóê Ï∂îÍ∞Ä"""
        self.document_cache[doc_id] = {
            "filename": filename,
            "graph_data": graph_data,
            "learning_results": learning_results,
            "processed_time": datetime.now().isoformat()
        }
        print(f"üìÑ Document added to comparison cache: {filename} (ID: {doc_id})")
    
    def get_all_documents(self):
        """Î™®Îì† Î¨∏ÏÑú Î™©Î°ù Î∞òÌôò"""
        try:
            query = """
                SELECT id, filename, upload_time, content_length, processing_time
                FROM documents 
                ORDER BY upload_time DESC
            """
            return self.db_manager.execute_query(query)
        except Exception as e:
            print(f"‚ùå Error getting documents: {e}")
            return pd.DataFrame()
    
    def compare_documents(self, doc_ids=None):
        """ÏÑ†ÌÉùÎêú Î¨∏ÏÑúÎì§ÏùÑ ÎπÑÍµê"""
        if not doc_ids:
            # Î™®Îì† Î¨∏ÏÑú ÎπÑÍµê
            docs_df = self.get_all_documents()
            if docs_df.empty:
                return {"error": "No documents found for comparison"}
            doc_ids = docs_df['id'].tolist()
        
        comparison_results = {
            "document_overview": self._compare_document_overview(doc_ids),
            "field_comparison": self._compare_fields(doc_ids),
            "entity_comparison": self._compare_entities(doc_ids),
            "statistics_comparison": self._compare_statistics(doc_ids),
            "content_similarity": self._analyze_content_similarity(doc_ids)
        }
        
        return comparison_results
    
    def _compare_document_overview(self, doc_ids):
        """Î¨∏ÏÑú Í∞úÏöî ÎπÑÍµê (Ï§ëÎ≥µ Ï†úÍ±∞)"""
        try:
            placeholders = ','.join([str(doc_id) for doc_id in doc_ids])
            query = f"""
                SELECT DISTINCT d.id, d.filename, d.upload_time, d.content_length,
                       COUNT(DISTINCT n.id) as node_count,
                       COUNT(DISTINCT e.id) as edge_count,
                       AVG(n.confidence) as avg_confidence
                FROM documents d
                LEFT JOIN nodes n ON d.id = n.document_id
                LEFT JOIN edges e ON d.id = e.document_id
                WHERE d.id IN ({placeholders})
                GROUP BY d.id, d.filename, d.upload_time, d.content_length
                HAVING node_count > 0  -- ÎÖ∏ÎìúÍ∞Ä ÏûàÎäî Î¨∏ÏÑúÎßå
                ORDER BY d.upload_time DESC
            """
            
            result = self.db_manager.execute_query(query)
            
            # Ï§ëÎ≥µ ÌååÏùºÎ™Ö Ï†úÍ±∞ (ÏµúÏã† Í≤ÉÎßå Ïú†ÏßÄ)
            if not result.empty:
                # ÌååÏùºÎ™Ö Í∏∞Ï§ÄÏúºÎ°ú Ï§ëÎ≥µ Ï†úÍ±∞ (ÏµúÏã† upload_time Í∏∞Ï§Ä)
                result = result.sort_values('upload_time', ascending=False).drop_duplicates('filename', keep='first')
            
            return result.to_dict('records') if not result.empty else []
        except Exception as e:
            print(f"‚ùå Document overview comparison error: {e}")
            return []
    
    def _compare_fields(self, doc_ids):
        """ÌïÑÎìú ÎπÑÍµê"""
        try:
            placeholders = ','.join([str(doc_id) for doc_id in doc_ids])
            query = f"""
                SELECT d.filename, n.id as field_id, n.ontology_class, n.confidence
                FROM documents d
                JOIN nodes n ON d.id = n.document_id
                WHERE d.id IN ({placeholders}) AND n.ontology_class = 'Field'
                ORDER BY d.filename, n.confidence DESC
            """
            
            result = self.db_manager.execute_query(query)
            
            # ÌïÑÎìúÎ≥Ñ Î¨∏ÏÑú Îß§Ìä∏Î¶≠Ïä§ ÏÉùÏÑ±
            if not result.empty:
                field_matrix = {}
                for _, row in result.iterrows():
                    field_name = row['field_id'].split('_')[1] if '_' in row['field_id'] else row['field_id']
                    filename = row['filename']
                    
                    if field_name not in field_matrix:
                        field_matrix[field_name] = {}
                    field_matrix[field_name][filename] = row['confidence']
                
                return field_matrix
            
            return {}
        except Exception as e:
            print(f"‚ùå Field comparison error: {e}")
            return {}
    
    def _compare_entities(self, doc_ids):
        """ÏóîÌã∞Ìã∞ ÎπÑÍµê"""
        try:
            placeholders = ','.join([str(doc_id) for doc_id in doc_ids])
            query = f"""
                SELECT d.filename, n.ontology_class, COUNT(*) as count, AVG(n.confidence) as avg_confidence
                FROM documents d
                JOIN nodes n ON d.id = n.document_id
                WHERE d.id IN ({placeholders}) AND n.node_type = 'entity'
                GROUP BY d.filename, n.ontology_class
                ORDER BY d.filename, count DESC
            """
            
            result = self.db_manager.execute_query(query)
            return result.to_dict('records') if not result.empty else []
        except Exception as e:
            print(f"‚ùå Entity comparison error: {e}")
            return []
    
    def _compare_statistics(self, doc_ids):
        """ÌÜµÍ≥Ñ ÎπÑÍµê"""
        try:
            stats_comparison = []
            
            for doc_id in doc_ids:
                # Í∞Å Î¨∏ÏÑúÎ≥Ñ ÌÜµÍ≥Ñ
                query = f"""
                    SELECT d.filename,
                           COUNT(DISTINCT CASE WHEN n.node_type = 'entity' THEN n.id END) as entity_count,
                           COUNT(DISTINCT CASE WHEN n.node_type = 'literal' THEN n.id END) as literal_count,
                           COUNT(DISTINCT e.id) as edge_count,
                           AVG(n.confidence) as avg_confidence,
                           COUNT(DISTINCT CASE WHEN n.confidence > 0.8 THEN n.id END) as high_confidence_count
                    FROM documents d
                    LEFT JOIN nodes n ON d.id = n.document_id
                    LEFT JOIN edges e ON d.id = e.document_id
                    WHERE d.id = {doc_id}
                    GROUP BY d.filename
                """
                
                result = self.db_manager.execute_query(query)
                if not result.empty:
                    stats_comparison.extend(result.to_dict('records'))
            
            return stats_comparison
        except Exception as e:
            print(f"‚ùå Statistics comparison error: {e}")
            return []
    
    def _analyze_content_similarity(self, doc_ids):
        """ÎÇ¥Ïö© Ïú†ÏÇ¨ÎèÑ Î∂ÑÏÑù"""
        try:
            # Í≥µÌÜµ ÏóîÌã∞Ìã∞ Î∂ÑÏÑù
            placeholders = ','.join([str(doc_id) for doc_id in doc_ids])
            query = f"""
                SELECT n.id, COUNT(DISTINCT n.document_id) as doc_count, AVG(n.confidence) as avg_confidence
                FROM nodes n
                WHERE n.document_id IN ({placeholders})
                AND n.node_type = 'entity'
                GROUP BY n.id
                HAVING COUNT(DISTINCT n.document_id) > 1
                ORDER BY doc_count DESC, avg_confidence DESC
            """
            
            result = self.db_manager.execute_query(query)
            
            similarity_analysis = {
                "common_entities": result.to_dict('records') if not result.empty else [],
                "similarity_score": len(result) / max(len(doc_ids), 1) if not result.empty else 0
            }
            
            return similarity_analysis
        except Exception as e:
            print(f"‚ùå Content similarity analysis error: {e}")
            return {"common_entities": [], "similarity_score": 0}
        
def extract_pdf_text(pdf_path):
    """PDFÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú - Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ"""
    if PDF_AVAILABLE:
        try:
            # Í∏∞Î≥∏ Ï∂îÏ∂ú ÏãúÎèÑ
            text = extract_text(pdf_path)
            
            # ÌÖçÏä§Ìä∏ ÌíàÏßà ÌôïÏù∏
            if len(text.strip()) < 100 or text.count('+') > 10:
                print("‚ö†Ô∏è Low quality text extraction, trying alternative method...")
                # ÎåÄÏïà Î∞©Î≤ï: ÌéòÏù¥ÏßÄÎ≥Ñ Ï∂îÏ∂ú
                from pdfminer.high_level import extract_pages
                from pdfminer.layout import LTTextContainer
                
                alternative_text = ""
                try:
                    for page_layout in extract_pages(pdf_path):
                        for element in page_layout:
                            if isinstance(element, LTTextContainer):
                                alternative_text += element.get_text()
                    
                    if len(alternative_text) > len(text):
                        text = alternative_text
                        print("‚úÖ Alternative extraction method worked better")
                except:
                    print("‚ö†Ô∏è Alternative extraction failed, using original")
            
            # üîç ÎîîÎ≤ÑÍπÖ: Ïã§Ï†ú Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏ ÌôïÏù∏
            print("üîç PDF TEXT DEBUG:")
            print("="*50)
            print(text[:1000])  # Ï≤´ 1000ÏûêÎßå Ï∂úÎ†•
            print("="*50)
            
            # üîç ÌÖçÏä§Ìä∏ ÌíàÏßà Í≤ÄÏÇ¨
            quality_score = 0
            if len(text) > 500: quality_score += 1
            if '+' not in text[:200]: quality_score += 1  # Ï≤´ Î∂ÄÎ∂ÑÏóê +Í∞Ä ÏóÜÏúºÎ©¥ Ï¢ãÏùå
            if any(keyword in text.upper() for keyword in ["REVISION", "DATE", "BY"]): quality_score += 1
            
            print(f"üìä Text quality score: {quality_score}/3")
            
            # üîç ÌäπÏ†ï ÌÇ§ÏõåÎìú Í≤ÄÏÉâ
            keywords = ["14", "2012", "HJL", "SKL", "REVISION", "DATE", "BY"]
            print("üîç KEYWORD SEARCH:")
            for keyword in keywords:
                if keyword in text:
                    print(f"   ‚úÖ Found: {keyword}")
                    # Ìï¥Îãπ ÌÇ§ÏõåÎìú Ï£ºÎ≥Ä ÌÖçÏä§Ìä∏ Î≥¥Í∏∞
                    idx = text.find(keyword)
                    context = text[max(0, idx-30):idx+50]
                    print(f"      Context: {repr(context)}")
                else:
                    print(f"   ‚ùå Missing: {keyword}")
            print("="*50)
            
            return text
        except Exception as e:
            print(f"PDF extraction error: {e}")
            return "Error extracting PDF text"
    else:
        # Mock ÌÖçÏä§Ìä∏ Î∞òÌôò (Îçî ÌòÑÏã§Ï†ÅÏù∏ Îç∞Ïù¥ÌÑ∞)
        return """Industrial Equipment Specification
        Project: 7T04 - Centrifugal Pump Process Data
        Equipment ID: P-2105 A/B
        Pump Type: CENTRIFUGAL
        Driver Type: MOTOR / MOTOR
        Capacity: 71.1 m3/h (AM Feed)
        Temperature: 384 ‚ÑÉ
        Pressure: 17.1 kg/cm2A
        Viscosity: 1.0 cP
        Revision: 14
        Checked by: HJL / SKL
        Date: 2012-12-26
        
        REVISION    DATE        BY/CHECKED
        10          2008-06-27  WSJ / WGK
        11          2010-09-08  WJK / WGK
        11A         2010-06-18  WJK / WGK
        11B         2011-01-26  WJK / WGK
        12          2012-08-14  HJL / SKL
        13          2012-12-26  HJL / SKL
        14          2012-12-26  HJL / SKL
        
        NOTES:
        1. NPSHA required is 8.0 m minimum at rated flow and temperature.
        2. Foundation to be designed for continuous operation.
        3. Pump to be suitable for outdoor installation.
        
        PROCESS REQUIREMENT
        PUMPING TEMPERATURE (‚ÑÉ): 384
        CAPACITY (m3/hr): 71.1 @ NORMAL / 70.4 @ RATED
        SUCTION PRESSURE (kg/cm2g): 17.1 @ NORMAL / 16.1 @ MIN
        DISCHARGE PRESSURE (kg/cm2g): 207 @ NORMAL
        SPECIFIC GRAVITY: 1.0
        VISCOSITY (cP): 1.0
        """

# Ïò®ÌÜ®Î°úÏßÄ ÌÅ¥ÎûòÏä§ (ÎåÄÌè≠ Í∞ïÌôîÎêú Î≤ÑÏ†Ñ)
# Ïò®ÌÜ®Î°úÏßÄ ÌÅ¥ÎûòÏä§ (ÏôÑÏ†Ñ Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ)
class AdvancedOntologyLearner:
    def __init__(self):
        # Í∏∞Î≥∏ Ïò®ÌÜ®Î°úÏßÄ Ïä§ÌÇ§Îßà
        self.classes = {"Project", "Equipment", "ProcessRequirement", "Person", "Document", "Revision", "Note", "Field", "Date"}
        self.object_properties = {"hasEquipment", "hasProcessReq", "reviewedBy", "hasProject", "hasValue", "hasRevision", "hasNote", "hasField"}
        self.datatype_properties = {"jobNo", "itemNo", "value", "noteText", "revisionNumber", "revisionDate", "fieldName", "fieldValue"}
        
        # ÌïôÏäµÎêú Ìå®ÌÑ¥ Ï†ÄÏû•ÏÜå
        self.learned_patterns = {}
        self.entity_instances = {}
        self.confidence_scores = {}
        
        # ÌôïÏû•Îêú ÎèÑÎ©îÏù∏ ÌäπÌôî ÌÇ§ÏõåÎìú
        self.domain_keywords = {
            "Project": ["job", "project", "doc", "document", "specification", "drawing", "client", "location"],
            "Equipment": ["pump", "motor", "driver", "centrifugal", "equipment", "vessel", "tank", "compressor", "exchanger"],
            "ProcessRequirement": ["temperature", "pressure", "capacity", "flow", "viscosity", "density", "npsh", "head", "suction", "discharge"],
            "Person": ["checked", "reviewed", "approved", "by", "engineer", "manager"],
            "Revision": ["revision", "rev", "version", "updated", "modified"],
            "Note": ["note", "remark", "comment", "description", "notes"],
            "Field": ["type", "required", "operating", "duty", "liquid", "vapor", "specific", "differential", "material", "method"],
            "Date": ["date", "time", "year", "month", "day"]
        }
        
        # ÌôïÏû•Îêú Îã®ÏúÑ Î∞è Ï∏°Ï†ïÍ∞í Ìå®ÌÑ¥
        self.unit_patterns = {
            "temperature": ["‚ÑÉ", "¬∞C", "¬∞F", "K", "DEG C"],
            "pressure": ["kg/cm2", "kg/cm2g", "kg/cm2A", "bar", "psi", "Pa", "kPa", "MPa"],
            "flow": ["m3/h", "m3/hr", "M3/HR", "l/min", "gpm", "bph"],
            "viscosity": ["cP", "Pa¬∑s", "cSt"],
            "length": ["mm", "cm", "m", "in", "ft", "METER"],
            "percentage": ["%", "wt%", "WT%"],
            "density": ["KG/M3", "kg/m3"],
            "time": ["hr", "yr", "hour", "year"]
        }
        
        # ÌôîÌïôÍ≥µÏ†ï Ï†ÑÎ¨∏Ïö©Ïñ¥
        self.process_terms = {
            "fluids": ["OVERFLASH", "GAS OIL", "AH FEED", "AM FEED", "SULFUR"],
            "materials": ["API CLASS", "FLAMMABLE", "TOXIC", "H2S", "CHLORIDE"],
            "operations": ["CONTINUOUS", "INTERMITTENT", "MANUAL", "AUTOMATIC", "INDOOR", "OUTDOOR"],
            "equipment_types": ["HORIZONTAL", "CENTRIFUGAL", "STEAM TRACING", "STEAM JACKET", "INSULATION"]
        }
        
    def learn_from_text(self, text):
        print("üß† Advanced ontology learning from text...")
        
        # 1. Í∏∞Î≥∏ Ìå®ÌÑ¥ Ï∂îÏ∂ú (ÎåÄÌè≠ Í∞ïÌôî)
        basic_patterns = self._extract_basic_patterns(text)
        
        # 2. ÌïÑÎìú-Í∞í Ïåç Ï∂îÏ∂ú
        field_value_pairs = self._extract_field_value_pairs(text)
        
        # 3. NOTE ÏÑπÏÖò Ï∂îÏ∂ú  
        notes = self._extract_notes_section(text)
        
        # 4. Ïª®ÌÖçÏä§Ìä∏ Í∏∞Î∞ò ÏóîÌã∞Ìã∞ Ïù∏Ïãù
        contextual_entities = self._extract_contextual_entities(text)
        
        # 5. Í¥ÄÍ≥Ñ Ï∂îÏ∂ú
        relations = self._extract_relations(text, contextual_entities)
        
        # 6. Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
        confidence_scores = self._calculate_confidence(basic_patterns, contextual_entities)
        
        # 7. Ïò®ÌÜ®Î°úÏßÄ Îß§Ïπ≠
        ontology_matches = self._match_to_ontology(contextual_entities)
        
        # 8. ÌïôÏäµ Í≤∞Í≥º Ï†ÄÏû•
        self._update_learned_knowledge(basic_patterns, contextual_entities, relations)
        
        # ÌÜµÌï© Í≤∞Í≥º
        results = {
            "patterns": basic_patterns,
            "field_value_pairs": field_value_pairs,
            "notes": notes,
            "new_entities": contextual_entities,
            "new_relations": relations,
            "matched_entities": ontology_matches,
            "confidence_scores": confidence_scores,
            "domain_insights": self._generate_domain_insights(text)
        }
        
        # Ï¥ù ÏóîÌã∞Ìã∞ Ïàò Í≥ÑÏÇ∞
        total_entities = (
            sum(len(v) if isinstance(v, list) else 1 for v in basic_patterns.values()) +
            len(field_value_pairs) +
            len(notes) +
            sum(len(v) for v in contextual_entities.values())
        )
        
        print(f"üìä Enhanced learning completed:")
        print(f"   - Basic Patterns: {len(basic_patterns)} types")
        print(f"   - Field-Value Pairs: {len(field_value_pairs)}")
        print(f"   - Notes: {len(notes)}")
        print(f"   - Contextual Entities: {sum(len(v) for v in contextual_entities.values())}")
        print(f"   - Total Entities: {total_entities}")
        print(f"   - Relations: {len(relations)} identified")
        print(f"   - Avg Confidence: {sum(confidence_scores.values())/len(confidence_scores) if confidence_scores else 0:.2f}")
        
        return results
    
    def _extract_basic_patterns(self, text):
        """Í∏∞Î≥∏ Ìå®ÌÑ¥ Ï∂îÏ∂ú - ÏôÑÏ†Ñ Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ"""
        patterns = {}
        
        try:
            # 1. ÌîÑÎ°úÏ†ùÌä∏/Î¨∏ÏÑú Î≤àÌò∏ Ìå®ÌÑ¥ (ÌôïÏû•)
            project_patterns = [
                r'(?:JOB\s*NO\.?\s*:?\s*)([A-Z0-9]{2,}(?:-[A-Z0-9]+)*)',
                r'(?:PROJECT\s*:?\s*)([A-Z0-9]{2,}(?:\s+[A-Z0-9]+)*)',
                r'(?:DOC\.?\s*NO\.?\s*:?\s*)([A-Z0-9]{2,}(?:-[A-Z0-9]+)*)',
                r'(?:ITEM\s*NO\.?\s*:?\s*)([A-Z0-9]{2,}(?:-[A-Z0-9]+)*)',
                r'\b([A-Z]\d+[A-Z]?\d*(?:-[A-Z0-9]+)*)\b'
            ]
            
            project_ids = []
            for pattern in project_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                project_ids.extend(matches)
            
            if project_ids:
                patterns["project_ids"] = list(set(project_ids))
            
            # 2. Î™®Îì† Î¶¨ÎπÑÏ†Ñ Î≤àÌò∏ Ï∂îÏ∂ú (ÏùºÎ∞òÌôî)
            revision_patterns = [
                r'(?:REVISION|REV\.?|REV)\s*:?\s*(\d+[A-Z]?)',
                r'(?:revision|rev)\s+(\d+[A-Z]?)',
                r'\bR(\d+[A-Z]?)\b',
                r'\bRev\s*(\d+[A-Z]?)\b',
                # ÌÖåÏù¥Î∏îÏóêÏÑú Ïó∞ÏÜçÎêú Î¶¨ÎπÑÏ†Ñ Î≤àÌò∏Îì§
                r'(?:REVISION\s+DATE\s+)(?:\d+[A-Z]?\s+)*(\d+[A-Z]?)(?=\s+(?:20\d{2}|BY))',
                # Í∞úÎ≥Ñ Î¶¨ÎπÑÏ†Ñ Î≤àÌò∏
                r'\b(1[0-9][A-Z]?)\b',  # 10-19
                r'\b([2-9]\d[A-Z]?)\b'   # 20+
            ]
            
            revision_numbers = []
            for pattern in revision_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                revision_numbers.extend(matches)
            
            if revision_numbers:
                patterns["revision_numbers"] = list(set(revision_numbers))
            
            # 3. Î™®Îì† ÎÇ†Ïßú Ìå®ÌÑ¥ Ï∂îÏ∂ú (ÏôÑÏ†Ñ ÏùºÎ∞òÌôî)
            date_patterns = [
                r'(\d{4}-\d{1,2}-\d{1,2})',  # YYYY-MM-DD
                r'(\d{1,2}/\d{1,2}/\d{4})',  # MM/DD/YYYY
                r'(\d{1,2}-\d{1,2}-\d{4})',  # MM-DD-YYYY
                r'(20\d{2}-\d{1,2}-\d{1,2})', # 20XX-XX-XX ÌôïÏã§Ìïú ÌòïÌÉú
            ]
            
            dates = []
            for pattern in date_patterns:
                matches = re.findall(pattern, text)
                dates.extend(matches)
            
            if dates:
                patterns["dates"] = list(set(dates))
            
            # 4. Î™®Îì† ÏÇ¨Îûå Ïù¥Î¶Ñ/Ïù¥ÎãàÏÖú Ï∂îÏ∂ú (ÏôÑÏ†Ñ ÏùºÎ∞òÌôî)
            person_patterns = [
                r'(?:BY/CHECKED|BY CHECKED|Checked\s+by|Reviewed\s+by|Approved\s+by)\s*:?\s*([A-Z]{2,4}(?:\s*/\s*[A-Z]{2,4})*)',
                r'\b([A-Z]{2,4})\s*/\s*([A-Z]{2,4})\b',  # XXX / YYY ÌòïÌÉú
                r'\b([A-Z]{3})\s+/\s+([A-Z]{3})\b',     # XXX / YYY (Í≥µÎ∞± Ìè¨Ìï®)
            ]
            
            person_names = []
            for pattern in person_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    if isinstance(match, tuple):
                        person_names.extend([m for m in match if m and len(m) >= 2])
                    else:
                        names = re.split(r'\s*/\s*', match)
                        person_names.extend([n.strip() for n in names if n.strip() and len(n.strip()) >= 2])
            
            if person_names:
                patterns["person_names"] = list(set(person_names))
            
            # 5. Ïû•ÎπÑ ÏãùÎ≥ÑÏûê Ìå®ÌÑ¥ (ÌôïÏû•)
            equipment_patterns = [
                r'([A-Z]-\d+(?:\s*[A-Z](?:/[A-Z])?)?)',
                r'([A-Z]{2,3}-\d+)',
                r'([A-Z]\d{3,}[A-Z]?)',
                r'(P-\d+\s*[A-Z]?/?[A-Z]?)',  # P-2105 A/B ÌòïÌÉú
            ]
            
            equipment_ids = []
            for pattern in equipment_patterns:
                matches = re.findall(pattern, text)
                equipment_ids.extend(matches)
            
            if equipment_ids:
                patterns["equipment_ids"] = list(set(equipment_ids))
            
            # 6. Î™®Îì† ÏàòÏπò Í∞í Ìå®ÌÑ¥ (ÎåÄÌè≠ ÌôïÏû•)
            numerical_patterns = []
            for unit_type, units in self.unit_patterns.items():
                for unit in units:
                    pattern = rf'(\d+\.?\d*)\s*{re.escape(unit)}'
                    matches = re.findall(pattern, text, re.IGNORECASE)
                    for match in matches:
                        numerical_patterns.append((match, unit, unit_type))
            
            # Îã®ÏúÑ ÏóÜÎäî Ïà´ÏûêÎì§ÎèÑ Ï∂îÏ∂ú (Ïª®ÌÖçÏä§Ìä∏ Í∏∞Î∞ò)
            standalone_numbers = re.findall(r'\b(\d+\.?\d+)\b', text)
            for num in standalone_numbers:
                if float(num) > 1:  # ÏùòÎØ∏ÏûàÎäî Ïà´ÏûêÎßå
                    numerical_patterns.append((num, "", "number"))
            
            if numerical_patterns:
                patterns["numerical_values"] = numerical_patterns
            
            # 7. ÌôîÌïôÍ≥µÏ†ï Ï†ÑÎ¨∏Ïö©Ïñ¥ Ï∂îÏ∂ú
            process_entities = []
            for category, terms in self.process_terms.items():
                for term in terms:
                    if term in text:
                        process_entities.append((term, category))
            
            if process_entities:
                patterns["process_terms"] = process_entities
            
            # 8. API Î∂ÑÎ•ò Î∞è Ïû¨Î£å Îì±Í∏â
            material_patterns = [
                r'(API\s+CLASS\s+[A-Z]-\d+)',
                r'(API\s+[A-Z]-\d+)',
                r'([A-Z]\s*-\s*\d+\s*\([^)]+\))',  # A-8 (HOLD) ÌòïÌÉú
            ]
            
            materials = []
            for pattern in material_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                materials.extend(matches)
            
            if materials:
                patterns["materials"] = list(set(materials))
            
            print(f"üîç Extracted basic patterns: {list(patterns.keys())}")
            for pattern_type, values in patterns.items():
                if isinstance(values, list):
                    print(f"   {pattern_type}: {values[:5]}{'...' if len(values) > 5 else ''}")
                else:
                    print(f"   {pattern_type}: {values}")
            
        except Exception as e:
            print(f"‚ùå Pattern extraction error: {e}")
            patterns = {}
        
        return patterns if patterns else {}
    
    def _extract_field_value_pairs(self, text):
        """ÌïÑÎìú-Í∞í Ïåç Ï∂îÏ∂ú"""
        field_value_pairs = []
        
        try:
            # Î≤àÌò∏Í∞Ä ÏûàÎäî ÌïÑÎìú Ìå®ÌÑ¥ (01, 02, 03... ÌòïÌÉú)
            numbered_field_pattern = r'(\d{2})\s+([A-Z\s/()]+?)\s+([A-Z0-9\s:@.‚ÑÉ]+?)(?=\d{2}|\n\n|$)'
            matches = re.findall(numbered_field_pattern, text, re.MULTILINE)
            
            for match in matches:
                field_num, field_name, field_value = match
                field_value_pairs.append({
                    "field_number": field_num.strip(),
                    "field_name": field_name.strip(),
                    "field_value": field_value.strip(),
                    "type": "numbered_field"
                })
            
            # ÏΩúÎ°†ÏúºÎ°ú Íµ¨Î∂ÑÎêú ÌïÑÎìú Ìå®ÌÑ¥
            colon_field_pattern = r'([A-Z\s]+?)\s*:\s*([A-Z0-9\s.‚ÑÉ/-]+?)(?=\n|$)'
            matches = re.findall(colon_field_pattern, text)
            
            for match in matches:
                field_name, field_value = match
                if len(field_name.strip()) > 2 and len(field_value.strip()) > 0:
                    field_value_pairs.append({
                        "field_name": field_name.strip(),
                        "field_value": field_value.strip(),
                        "type": "colon_separated"
                    })
            
            print(f"üè∑Ô∏è Extracted {len(field_value_pairs)} field-value pairs")
            
        except Exception as e:
            print(f"‚ùå Field-value extraction error: {e}")
        
        return field_value_pairs
    
    def _extract_notes_section(self, text):
        """NOTES ÏÑπÏÖò Ï∂îÏ∂ú"""
        notes = []
        
        try:
            # NOTES: Ïù¥ÌõÑ Î™®Îì† ÌÖçÏä§Ìä∏ Ï∞æÍ∏∞
            notes_pattern = r'NOTES?\s*:?\s*(.*?)(?=\n\n|\nREVISION|\nDATE|$)'
            notes_match = re.search(notes_pattern, text, re.DOTALL | re.IGNORECASE)
            
            if notes_match:
                notes_text = notes_match.group(1).strip()
                
                # Í∞úÎ≥Ñ ÎÖ∏Ìä∏ Î∂ÑÎ¶¨ (1., 2., 3. Îì±)
                individual_notes = re.split(r'\n\s*(\d+)\.\s*', notes_text)
                
                if len(individual_notes) > 1:
                    for i in range(1, len(individual_notes), 2):
                        if i + 1 < len(individual_notes):
                            note_num = individual_notes[i]
                            note_text = individual_notes[i + 1].strip()
                            
                            notes.append({
                                "note_number": note_num,
                                "note_text": note_text,
                                "type": "numbered_note"
                            })
                else:
                    # Î≤àÌò∏Í∞Ä ÏóÜÎäî Ï†ÑÏ≤¥ ÎÖ∏Ìä∏
                    notes.append({
                        "note_text": notes_text,
                        "type": "general_note"
                    })
            
            print(f"üìù Extracted {len(notes)} notes")
            for note in notes:
                if "note_number" in note:
                    print(f"   Note {note['note_number']}: {note['note_text'][:50]}...")
                else:
                    print(f"   General note: {note['note_text'][:50]}...")
            
        except Exception as e:
            print(f"‚ùå Notes extraction error: {e}")
        
        return notes
    
    def _extract_contextual_entities(self, text):
        """Ïª®ÌÖçÏä§Ìä∏Î•º Í≥†Î†§Ìïú ÏóîÌã∞Ìã∞ Ï∂îÏ∂ú - Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ"""
        entities = {}
        
        try:
            sentences = re.split(r'[.!?\n]+', text)
            
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) < 5:
                    continue
                
                for class_name, keywords in self.domain_keywords.items():
                    if any(keyword.lower() in sentence.lower() for keyword in keywords):
                        if class_name not in entities:
                            entities[class_name] = []
                        
                        specific_entities = self._extract_specific_entities(sentence, class_name)
                        entities[class_name].extend(specific_entities)
            
            # Ï§ëÎ≥µ Ï†úÍ±∞
            for class_name in entities:
                entities[class_name] = list(set(entities[class_name]))
                
        except Exception as e:
            print(f"‚ùå Contextual entity extraction error: {e}")
            entities = {}
        
        return entities
    
    def _extract_specific_entities(self, sentence, class_name):
        """ÌäπÏ†ï ÌÅ¥ÎûòÏä§Ïóê ÎåÄÌïú Íµ¨Ï≤¥Ï†Å ÏóîÌã∞Ìã∞ Ï∂îÏ∂ú - ÌôïÏû•Îêú Î≤ÑÏ†Ñ"""
        entities = []
        
        try:
            if class_name == "Equipment":
                equipment_types = re.findall(r'\b(CENTRIFUGAL|PUMP|MOTOR|DRIVER|COMPRESSOR|VESSEL|TANK|HEAT\s+EXCHANGER|HORIZONTAL)\b', sentence.upper())
                entities.extend(equipment_types)
                
                equipment_ids = re.findall(r'\b([A-Z]-?\d+(?:\s*[A-Z]/?)*)\b', sentence)
                entities.extend(equipment_ids)
            
            elif class_name == "ProcessRequirement":
                process_vars = re.findall(r'\b(temperature|pressure|flow|capacity|viscosity|density|npsh|head|suction|discharge)\b', sentence.lower())
                entities.extend(process_vars)
                
                for unit_type, units in self.unit_patterns.items():
                    for unit in units:
                        pattern = rf'(\d+\.?\d*\s*{re.escape(unit)})'
                        matches = re.findall(pattern, sentence, re.IGNORECASE)
                        entities.extend(matches)
            
            elif class_name == "Project":
                project_info = re.findall(r'\b([A-Z0-9]{3,}(?:-[A-Z0-9]+)*)\b', sentence)
                entities.extend(project_info)
            
            elif class_name == "Person":
                if any(word in sentence.lower() for word in ['by', 'checked', 'reviewed', 'approved']):
                    names = re.findall(r'\b([A-Z]{2,4})\b', sentence)
                    entities.extend(names)
            
            elif class_name == "Field":
                # ÌïÑÎìúÎ™Ö Ï∂îÏ∂ú
                field_names = re.findall(r'\b([A-Z\s]{3,20})\b(?=\s*:|\s*\()', sentence)
                entities.extend(field_names)
                    
        except Exception as e:
            print(f"‚ùå Specific entity extraction error for {class_name}: {e}")
        
        return entities
    
    def _extract_relations(self, text, entities):
        """ÏóîÌã∞Ìã∞ Í∞Ñ Í¥ÄÍ≥Ñ Ï∂îÏ∂ú - ÌôïÏû•Îêú Î≤ÑÏ†Ñ"""
        relations = []
        
        try:
            relation_patterns = {
                "hasEquipment": (r'(project|job).*?(equipment|pump|motor)', "Project", "Equipment"),
                "hasProcessReq": (r'(equipment|pump).*?(temperature|pressure|flow)', "Equipment", "ProcessRequirement"),
                "reviewedBy": (r'(revision|document).*?(?:by|checked|reviewed).*?([A-Z]{2,4})', "Revision", "Person"),
                "hasValue": (r'(\w+).*?(\d+\.?\d*\s*[a-zA-Z/%]+)', "ProcessRequirement", "Literal"),
                "hasField": (r'(\d{2})\s+([A-Z\s]+)', "Document", "Field"),
                "hasNote": (r'(NOTE|NOTES)\s*:?\s*(\d+)', "Document", "Note")
            }
            
            for relation_name, (pattern, domain_class, range_class) in relation_patterns.items():
                matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    if isinstance(match, tuple) and len(match) >= 2:
                        relations.append({
                            "property": relation_name,
                            "domain": domain_class,
                            "range": range_class,
                            "instances": match,
                            "confidence": 0.7
                        })
                        
        except Exception as e:
            print(f"‚ùå Relation extraction error: {e}")
        
        return relations
    
    def _calculate_confidence(self, patterns, entities):
        """Ïã†Î¢∞ÎèÑ Ï†êÏàò Í≥ÑÏÇ∞ - Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ"""
        confidence_scores = {}
        
        try:
            for class_name, entity_list in entities.items():
                for entity in entity_list:
                    confidence = 0.5
                    
                    if class_name in self.domain_keywords:
                        if any(keyword in entity.lower() for keyword in self.domain_keywords[class_name]):
                            confidence += 0.2
                    
                    if re.match(r'^[A-Z]-?\d+', entity):
                        confidence += 0.2
                    
                    if re.match(r'^\d+\.?\d*\s*[a-zA-Z/%]+$', entity):
                        confidence += 0.3
                    
                    # Í∏∏Ïù¥ Í∏∞Î∞ò Î≥¥Ï†ï
                    if len(entity) > 10:
                        confidence += 0.1
                    
                    confidence_scores[entity] = min(confidence, 1.0)
                    
        except Exception as e:
            print(f"‚ùå Confidence calculation error: {e}")
        
        return confidence_scores
    
    def _match_to_ontology(self, entities):
        """Ïò®ÌÜ®Î°úÏßÄ Ïä§ÌÇ§ÎßàÏôÄ Îß§Ïπ≠ - Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ"""
        matches = {}
        
        try:
            for class_name, entity_list in entities.items():
                if class_name in self.classes:
                    matches[class_name] = []
                    for entity in entity_list:
                        match_info = {
                            "entity": entity,
                            "ontology_class": class_name,
                            "confidence": self.confidence_scores.get(entity, 0.5),
                            "properties": self._suggest_properties(entity, class_name)
                        }
                        matches[class_name].append(match_info)
                        
        except Exception as e:
            print(f"‚ùå Ontology matching error: {e}")
        
        return matches
    
    def _suggest_properties(self, entity, class_name):
        """ÏóîÌã∞Ìã∞Ïóê Ï†ÅÌï©Ìïú ÌîÑÎ°úÌçºÌã∞ Ï†úÏïà - ÌôïÏû•Îêú Î≤ÑÏ†Ñ"""
        suggestions = []
        
        try:
            if class_name == "Equipment":
                suggestions.extend(["itemNo", "equipmentType", "capacity", "material", "classification"])
            elif class_name == "Project":
                suggestions.extend(["jobNo", "projectName", "docNo", "client", "location"])
            elif class_name == "ProcessRequirement":
                suggestions.extend(["value", "unit", "condition", "operatingCase", "maximum", "minimum"])
            elif class_name == "Person":
                suggestions.extend(["name", "role", "initials"])
            elif class_name == "Field":
                suggestions.extend(["fieldNumber", "fieldName", "fieldValue", "fieldType"])
            elif class_name == "Note":
                suggestions.extend(["noteNumber", "noteText", "noteType"])
                
        except Exception as e:
            print(f"‚ùå Property suggestion error: {e}")
        
        return suggestions
    
    def _generate_domain_insights(self, text):
        """ÎèÑÎ©îÏù∏ ÌäπÌôî Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ± - ÌôïÏû•Îêú Î≤ÑÏ†Ñ"""
        insights = {
            "document_type": "technical_specification",
            "industry_domain": "process_engineering",
            "complexity_score": min(len(text) / 1000, 5.0),
            "technical_density": len(re.findall(r'\d+\.?\d*\s*[a-zA-Z/%]+', text)) / max(len(text.split()), 1)
        }
        
        # Î¨∏ÏÑú ÌÉÄÏûÖ ÏÑ∏Î∂Ä Î∂ÑÎ•ò
        if "PUMP" in text.upper():
            insights["equipment_type"] = "pump_specification"
        if "CENTRIFUGAL" in text.upper():
            insights["pump_type"] = "centrifugal"
        if "PROCESS DATA" in text.upper():
            insights["data_sheet_type"] = "process_data"
        
        # ÏúÑÌóòÎèÑ Î∂ÑÏÑù
        hazard_keywords = ["FLAMMABLE", "TOXIC", "H2S", "SULFUR", "CHLORIDE"]
        hazard_count = sum(1 for keyword in hazard_keywords if keyword in text.upper())
        insights["hazard_level"] = "high" if hazard_count >= 3 else "medium" if hazard_count >= 1 else "low"
        
        return insights
    
    def _update_learned_knowledge(self, patterns, entities, relations):
        """ÌïôÏäµÎêú ÏßÄÏãù ÏóÖÎç∞Ïù¥Ìä∏ - ÌôïÏû•Îêú Î≤ÑÏ†Ñ"""
        try:
            # Ìå®ÌÑ¥ ÌïôÏäµ ÏóÖÎç∞Ïù¥Ìä∏
            for pattern_type, values in patterns.items():
                if pattern_type not in self.learned_patterns:
                    self.learned_patterns[pattern_type] = {}
                
                if isinstance(values, list):
                    for value in values:
                        value_str = str(value) if not isinstance(value, (tuple, list)) else str(value)
                        self.learned_patterns[pattern_type][value_str] = \
                            self.learned_patterns[pattern_type].get(value_str, 0) + 1
            
            # ÏóîÌã∞Ìã∞ Ïù∏Ïä§ÌÑ¥Ïä§ ÏóÖÎç∞Ïù¥Ìä∏
            for class_name, entity_list in entities.items():
                if class_name not in self.entity_instances:
                    self.entity_instances[class_name] = set()
                
                for entity in entity_list:
                    self.entity_instances[class_name].add(str(entity))
            
            print(f"üìö Knowledge updated: {len(self.learned_patterns)} pattern types, "
                  f"{sum(len(instances) for instances in self.entity_instances.values())} entity instances")
            
        except Exception as e:
            print(f"‚ùå Knowledge update error: {e}")

# PDF Ï≤òÎ¶¨ Î∞è Í∑∏ÎûòÌîÑ ÏÉùÏÑ± (ÏôÑÏ†Ñ Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ)
def process_pdf_to_graph(pdf_content, filename):
    """PDF ÎÇ¥Ïö©ÏùÑ Ï≤òÎ¶¨ÌïòÏó¨ ÏôÑÏ†ÑÌïú ÏßÄÏãù Í∑∏ÎûòÌîÑ ÏÉùÏÑ±"""
    print(f"üìÑ Processing PDF with advanced ontology learning: {filename}")
    
    # Í∞ïÌôîÎêú Ïò®ÌÜ®Î°úÏßÄ ÌïôÏäµ
    learning_results = ontology_learner.learn_from_text(pdf_content)
    
    # Í∑∏ÎûòÌîÑ ÏÉùÏÑ±
    G = nx.DiGraph()
    node_counter = 0
    
    # Î¨∏ÏÑú Î£®Ìä∏ ÎÖ∏Îìú
    doc_node = f"Document_{filename.replace('.', '_')}_{node_counter}"
    G.add_node(doc_node, node_type="entity", ontology_class="Document")
    node_counter += 1
    
    # ========== 1. BASIC PATTERNS Ï≤òÎ¶¨ (ÌôïÏû•Îêú Ìå®ÌÑ¥Îì§) ==========
    patterns = learning_results.get("patterns", {})
    
    # 1-1. Revision Numbers Ï≤òÎ¶¨ (Î™®Îì† Î¶¨ÎπÑÏ†Ñ)
    if "revision_numbers" in patterns:
        for rev_num in patterns["revision_numbers"]:
            rev_node = f"Revision_{rev_num}_{node_counter}"
            G.add_node(rev_node, 
                      node_type="entity", 
                      ontology_class="Revision",
                      confidence=0.9)
            
            G.add_edge(doc_node, rev_node, relation="hasRevision")
            
            # Î¶¨ÎπÑÏ†Ñ Í∞í Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
            rev_value_node = f"RevisionValue_{rev_num}_{node_counter}"
            G.add_node(rev_value_node, 
                      node_type="literal", 
                      ontology_class="string",
                      confidence=0.9)
            
            G.add_edge(rev_node, rev_value_node, relation="hasValue")
            node_counter += 1
            print(f"‚úÖ Added Revision: {rev_num}")
    
    # 1-2. Dates Ï≤òÎ¶¨ (Î™®Îì† ÎÇ†Ïßú)
    if "dates" in patterns:
        for date_str in patterns["dates"]:
            date_node = f"Date_{date_str.replace('-', '_')}_{node_counter}"
            G.add_node(date_node, 
                      node_type="entity", 
                      ontology_class="Date",
                      confidence=0.9)
            
            G.add_edge(doc_node, date_node, relation="hasDate")
            
            # ÎÇ†Ïßú Í∞í Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
            date_value_node = f"DateValue_{date_str.replace('-', '_')}_{node_counter}"
            G.add_node(date_value_node, 
                      node_type="literal", 
                      ontology_class="date",
                      confidence=0.9)
            
            G.add_edge(date_node, date_value_node, relation="hasValue")
            node_counter += 1
            print(f"‚úÖ Added Date: {date_str}")
    
    # 1-3. Person Names Ï≤òÎ¶¨ (Î™®Îì† ÏÇ¨Îûå)
    if "person_names" in patterns:
        for person_name in patterns["person_names"]:
            person_node = f"Person_{person_name}_{node_counter}"
            G.add_node(person_node, 
                      node_type="entity", 
                      ontology_class="Person",
                      confidence=0.9)
            
            G.add_edge(doc_node, person_node, relation="reviewedBy")
            
            # ÏÇ¨Îûå Ïù¥Î¶Ñ Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
            name_value_node = f"PersonName_{person_name}_{node_counter}"
            G.add_node(name_value_node, 
                      node_type="literal", 
                      ontology_class="string",
                      confidence=0.9)
            
            G.add_edge(person_node, name_value_node, relation="hasValue")
            node_counter += 1
            print(f"‚úÖ Added Person: {person_name}")
    
    # 1-4. Equipment IDs Ï≤òÎ¶¨ (Î™®Îì† Ïû•ÎπÑ)
    if "equipment_ids" in patterns:
        for eq_id in patterns["equipment_ids"][:10]:  # ÏµúÎåÄ 10Í∞ú
            eq_node = f"Equipment_{eq_id.replace('-', '_').replace('/', '_').replace(' ', '_')}_{node_counter}"
            G.add_node(eq_node, 
                      node_type="entity", 
                      ontology_class="Equipment",
                      confidence=0.85)
            
            G.add_edge(doc_node, eq_node, relation="hasEquipment")
            
            # Ïû•ÎπÑ ID Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
            eq_value_node = f"EquipmentID_{eq_id.replace('-', '_').replace('/', '_').replace(' ', '_')}_{node_counter}"
            G.add_node(eq_value_node, 
                      node_type="literal", 
                      ontology_class="string",
                      confidence=0.85)
            
            G.add_edge(eq_node, eq_value_node, relation="hasValue")
            node_counter += 1
            print(f"‚úÖ Added Equipment: {eq_id}")
    
    # 1-5. Project IDs Ï≤òÎ¶¨
    if "project_ids" in patterns:
        for proj_id in patterns["project_ids"][:5]:  # ÏµúÎåÄ 5Í∞ú
            proj_node = f"Project_{proj_id.replace('-', '_').replace(' ', '_')}_{node_counter}"
            G.add_node(proj_node, 
                      node_type="entity", 
                      ontology_class="Project",
                      confidence=0.9)
            
            G.add_edge(doc_node, proj_node, relation="hasProject")
            
            # ÌîÑÎ°úÏ†ùÌä∏ ID Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
            proj_value_node = f"ProjectID_{proj_id.replace('-', '_').replace(' ', '_')}_{node_counter}"
            G.add_node(proj_value_node, 
                      node_type="literal", 
                      ontology_class="string",
                      confidence=0.9)
            
            G.add_edge(proj_node, proj_value_node, relation="hasValue")
            node_counter += 1
            print(f"‚úÖ Added Project: {proj_id}")
    
    # 1-6. Process Terms Ï≤òÎ¶¨ (ÌôîÌïôÍ≥µÏ†ï Ïö©Ïñ¥Îì§)
    if "process_terms" in patterns:
        for term, category in patterns["process_terms"][:15]:  # ÏµúÎåÄ 15Í∞ú
            term_node = f"ProcessTerm_{term.replace(' ', '_').replace('/', '_')}_{node_counter}"
            G.add_node(term_node, 
                      node_type="entity", 
                      ontology_class="ProcessTerm",
                      confidence=0.8)
            
            G.add_edge(doc_node, term_node, relation="hasProcessTerm")
            
            # Ïö©Ïñ¥ Í∞í Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
            term_value_node = f"TermValue_{term.replace(' ', '_').replace('/', '_')}_{node_counter}"
            G.add_node(term_value_node, 
                      node_type="literal", 
                      ontology_class="string",
                      confidence=0.8)
            
            G.add_edge(term_node, term_value_node, relation="hasValue")
            node_counter += 1
            print(f"‚úÖ Added Process Term: {term} ({category})")
    
    # 1-7. Materials Ï≤òÎ¶¨ (API Î∂ÑÎ•ò Îì±)
    if "materials" in patterns:
        for material in patterns["materials"][:5]:  # ÏµúÎåÄ 5Í∞ú
            mat_node = f"Material_{material.replace(' ', '_').replace('-', '_')}_{node_counter}"
            G.add_node(mat_node, 
                      node_type="entity", 
                      ontology_class="Material",
                      confidence=0.85)
            
            G.add_edge(doc_node, mat_node, relation="hasMaterial")
            
            # Ïû¨Î£å Í∞í Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
            mat_value_node = f"MaterialValue_{material.replace(' ', '_').replace('-', '_')}_{node_counter}"
            G.add_node(mat_value_node, 
                      node_type="literal", 
                      ontology_class="string",
                      confidence=0.85)
            
            G.add_edge(mat_node, mat_value_node, relation="hasValue")
            node_counter += 1
            print(f"‚úÖ Added Material: {material}")
    
    # ========== 2. FIELD-VALUE PAIRS Ï≤òÎ¶¨ ==========
    field_value_pairs = learning_results.get("field_value_pairs", [])
    
    for i, field_pair in enumerate(field_value_pairs[:20]):  # ÏµúÎåÄ 20Í∞ú
        field_name = field_pair.get("field_name", "Unknown")
        field_value = field_pair.get("field_value", "")
        field_number = field_pair.get("field_number", "")
        
        # ÌïÑÎìú ÏóîÌã∞Ìã∞ ÎÖ∏Îìú
        field_node = f"Field_{field_name.replace(' ', '_').replace('/', '_').replace('(', '').replace(')', '')}_{node_counter}"
        G.add_node(field_node, 
                  node_type="entity", 
                  ontology_class="Field",
                  confidence=0.85)
        
        G.add_edge(doc_node, field_node, relation="hasField")
        
        # ÌïÑÎìúÎ™Ö Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
        field_name_node = f"FieldName_{field_name.replace(' ', '_').replace('/', '_')}_{node_counter}"
        G.add_node(field_name_node, 
                  node_type="literal", 
                  ontology_class="string",
                  confidence=0.85)
        
        G.add_edge(field_node, field_name_node, relation="hasFieldName")
        
        # ÌïÑÎìúÍ∞í Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
        if field_value:
            field_value_node = f"FieldValue_{field_value.replace(' ', '_').replace('/', '_')}_{node_counter}"
            G.add_node(field_value_node, 
                      node_type="literal", 
                      ontology_class="string",
                      confidence=0.85)
            
            G.add_edge(field_node, field_value_node, relation="hasFieldValue")
        
        # ÌïÑÎìú Î≤àÌò∏Í∞Ä ÏûàÏúºÎ©¥ Ï∂îÍ∞Ä
        if field_number:
            field_num_node = f"FieldNumber_{field_number}_{node_counter}"
            G.add_node(field_num_node, 
                      node_type="literal", 
                      ontology_class="integer",
                      confidence=0.9)
            
            G.add_edge(field_node, field_num_node, relation="hasFieldNumber")
        
        node_counter += 1
        print(f"‚úÖ Added Field: {field_number} {field_name} = {field_value[:30]}...")
    
    # ========== 3. NOTES Ï≤òÎ¶¨ ==========
    notes = learning_results.get("notes", [])
    
    for note in notes[:10]:  # ÏµúÎåÄ 10Í∞ú
        note_text = note.get("note_text", "")
        note_number = note.get("note_number", "")
        note_type = note.get("type", "general_note")
        
        # NOTE ÏóîÌã∞Ìã∞ ÎÖ∏Îìú
        if note_number:
            note_node = f"Note_{note_number}_{node_counter}"
        else:
            note_node = f"Note_General_{node_counter}"
            
        G.add_node(note_node, 
                  node_type="entity", 
                  ontology_class="Note",
                  confidence=0.9)
        
        G.add_edge(doc_node, note_node, relation="hasNote")
        
        # NOTE Î≤àÌò∏ Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
        if note_number:
            note_num_node = f"NoteNumber_{note_number}_{node_counter}"
            G.add_node(note_num_node, 
                      node_type="literal", 
                      ontology_class="integer",
                      confidence=0.9)
            
            G.add_edge(note_node, note_num_node, relation="hasNoteNumber")
        
        # NOTE ÌÖçÏä§Ìä∏ Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
        if note_text:
            if note_number:
                note_text_node = f"NoteText_{note_number}_{node_counter}"
            else:
                note_text_node = f"NoteText_General_{node_counter}"
                
            G.add_node(note_text_node, 
                      node_type="literal", 
                      ontology_class="text",
                      confidence=0.9)
            
            G.add_edge(note_node, note_text_node, relation="hasNoteText")
        
        node_counter += 1
        print(f"‚úÖ Added Note: {note_number if note_number else 'General'}")

    # ========== 4. NUMERICAL VALUES Ï≤òÎ¶¨ (Îã®ÏúÑÎ≥Ñ Í∑∏Î£πÌôî) ==========
    if "numerical_values" in patterns:
        unit_groups = {}
        for value, unit, unit_type in patterns["numerical_values"]:
            if unit_type not in unit_groups:
                unit_groups[unit_type] = []
            unit_groups[unit_type].append((value, unit))
        
        for unit_type, values in unit_groups.items():
            # Îã®ÏúÑ ÌÉÄÏûÖÎ≥Ñ Í∑∏Î£π ÎÖ∏Îìú
            group_node = f"ProcessGroup_{unit_type}_{node_counter}"
            G.add_node(group_node, 
                      node_type="entity", 
                      ontology_class="ProcessRequirement",
                      confidence=0.8)
            
            G.add_edge(doc_node, group_node, relation="hasProcessReq")
            
            # Í∞úÎ≥Ñ Í∞íÎì§ Ï∂îÍ∞Ä (ÏµúÎåÄ 5Í∞ú)
            for value, unit in values[:5]:
                val_node = f"Value_{value}_{unit.replace('/', '_').replace(' ', '_')}_{node_counter}"
                G.add_node(val_node, 
                          node_type="literal", 
                          ontology_class="decimal",
                          confidence=0.9)
                
                G.add_edge(group_node, val_node, relation="hasValue")
                node_counter += 1
                print(f"‚úÖ Added {unit_type}: {value} {unit}")
    
    # ========== 5. Í∏∞Ï°¥ CONTEXTUAL ENTITIES Ï≤òÎ¶¨ (Î≥¥ÏôÑ) ==========
    matched_entities = learning_results.get("matched_entities", {})
    
    for class_name, entity_matches in matched_entities.items():
        for match_info in entity_matches[:3]:  # ÏµúÎåÄ 3Í∞úÎßå (Ï§ëÎ≥µ Î∞©ÏßÄ)
            entity = match_info["entity"]
            confidence = match_info["confidence"]
            
            # Ïù¥ÎØ∏ Ï∂îÍ∞ÄÎêú ÏóîÌã∞Ìã∞Ïù∏ÏßÄ ÌôïÏù∏ (Ï§ëÎ≥µ Î∞©ÏßÄ)
            entity_clean = entity.replace(' ', '_').replace('/', '_')
            existing_nodes = [n for n in G.nodes() if entity_clean in n]
            
            if not existing_nodes and confidence > 0.5:
                entity_node = f"{class_name}_{entity_clean}_{node_counter}"
                G.add_node(entity_node, 
                          node_type="entity", 
                          ontology_class=class_name,
                          confidence=confidence)
                
                G.add_edge(doc_node, entity_node, relation=f"has{class_name}")
                
                # ÏóîÌã∞Ìã∞ Í∞í Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
                value_node = f"EntityValue_{entity_clean}_{node_counter}"
                G.add_node(value_node, 
                          node_type="literal", 
                          ontology_class="string",
                          confidence=confidence)
                
                G.add_edge(entity_node, value_node, relation="hasValue")
                node_counter += 1
    
    # ========== 6. Í∑∏ÎûòÌîÑ ÏôÑÏÑ± Ï≤òÎ¶¨ ==========
    
    # ÎÖ∏Îìú ÌäπÏßï ÏÉùÏÑ±
    node_features = {}
    for node in G.nodes():
        node_data = G.nodes[node]
        node_features[node] = {
            "type": node_data.get("node_type", "entity"),
            "ontology_class": node_data.get("ontology_class", "Unknown"),
            "confidence": node_data.get("confidence", 0.5)
        }
    
    # Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò ÏòàÏ∏° ÏÉùÏÑ±
    predictions = []
    for node in G.nodes():
        node_info = node_features[node]
        confidence = node_info["confidence"]
        node_type = node_info["type"]
        
        if confidence > 0.8:
            if node_type == "entity":
                predictions.append(1)
            elif node_type == "literal":
                predictions.append(2)
            else:
                predictions.append(0)
        else:
            predictions.append(1 if node_type == "entity" else 2)
    
    print(f"‚úÖ Complete enhanced graph generated:")
    print(f"   - Total Nodes: {G.number_of_nodes()}")
    print(f"   - Total Edges: {G.number_of_edges()}")
    print(f"   - Entity Nodes: {sum(1 for n in node_features.values() if n['type'] == 'entity')}")
    print(f"   - Literal Nodes: {sum(1 for n in node_features.values() if n['type'] == 'literal')}")
    print(f"   - High Confidence: {sum(1 for n in node_features.values() if n['confidence'] > 0.8)}")
    
    return G, node_features, predictions, learning_results

# ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ìï®Ïàò
def create_sample_graph():
    G = nx.DiGraph()
    
    nodes = [
        ("Project_7T04", {"node_type": "entity", "ontology_class": "Project"}),
        ("Equipment_P2105", {"node_type": "entity", "ontology_class": "Equipment"}), 
        ("ProcessReq_Temp", {"node_type": "entity", "ontology_class": "ProcessRequirement"}),
        ("7T04", {"node_type": "literal", "ontology_class": "string"}),
        ("P-2105 A/B", {"node_type": "literal", "ontology_class": "string"}),
        ("384 ‚ÑÉ", {"node_type": "literal", "ontology_class": "decimal"})
    ]
    
    for node_id, attrs in nodes:
        G.add_node(node_id, **attrs)
    
    edges = [
        ("Project_7T04", "7T04", "hasValue"),
        ("Equipment_P2105", "P-2105 A/B", "hasValue"),
        ("ProcessReq_Temp", "384 ‚ÑÉ", "hasValue"),
        ("Project_7T04", "Equipment_P2105", "hasEquipment"),
        ("Equipment_P2105", "ProcessReq_Temp", "hasProcessReq")
    ]
    
    for src, dst, rel in edges:
        G.add_edge(src, dst, relation=rel)
    
    node_features = {}
    for node in G.nodes():
        node_data = G.nodes[node]
        node_features[node] = {
            "type": node_data.get("node_type", "entity"),
            "ontology_class": node_data.get("ontology_class", "Unknown"),
            "confidence": 0.8 if node_data.get("node_type") == "entity" else 0.9
        }
    
    predictions = []
    for node in G.nodes():
        node_type = node_features[node]["type"]
        if node_type == "entity":
            predictions.append(1)
        elif node_type == "literal":
            predictions.append(2)
        else:
            predictions.append(0)
    
    return G, node_features, predictions

class NaturalLanguageQueryProcessor:
    def __init__(self, ontology_learner, db_manager):
        self.ontology_learner = ontology_learner
        self.db_manager = db_manager
        
        # ÌôïÏû•Îêú ÏøºÎ¶¨ Ìå®ÌÑ¥ Îß§Ìïë (Î¨∏ÏÑú ÎπÑÍµê Ï∂îÍ∞Ä)
        self.query_patterns = {
            "show_all": [
                r"show\s+all\s+(\w+)",
                r"list\s+all\s+(\w+)",
                r"find\s+all\s+(\w+)",
                r"get\s+all\s+(\w+)"
            ],
            "filter_by_confidence": [
                r"(?:show|find|list)\s+(\w+)\s+with\s+(?:high|good)\s+confidence",
                r"(?:show|find|list)\s+high\s+confidence\s+(\w+)",
                r"(\w+)\s+with\s+confidence\s+>\s*(\d+\.?\d*)"
            ],
            "filter_by_class": [
                r"(?:show|find|list)\s+(\w+)\s+(?:of\s+type|class)\s+(\w+)",
                r"(\w+)\s+that\s+are\s+(\w+)",
                r"all\s+(\w+)\s+(\w+)"
            ],
            "count": [
                r"(?:how\s+many|count)\s+(\w+)",
                r"number\s+of\s+(\w+)",
                r"total\s+(\w+)"
            ],
            "stats": [
                r"(?:statistics|stats)\s+(?:for|of|about)?\s*(\w+)?",
                r"(?:summary|overview)\s+(?:of|about)?\s*(\w+)?",
                r"(?:analyze|analysis)\s+(\w+)?",
                r"database\s+(?:stats|statistics)"
            ],
            "relationships": [
                r"(?:show|find|list)\s+(?:relationships?|relations?|connections?)",
                r"what\s+is\s+connected\s+to\s+(\w+)",
                r"(\w+)\s+(?:connected|related|linked)\s+to\s+(\w+)"
            ],
            "recent": [
                r"(?:recent|latest|newest)\s+(\w+)",
                r"last\s+(\d+)\s+(\w+)",
                r"show\s+recent\s+(?:documents|files)"
            ],
            "search": [
                r"search\s+(?:for\s+)?(.+)",
                r"find\s+(.+)\s+in\s+database",
                r"lookup\s+(.+)"
            ],
            # ÏÉàÎ°úÏö¥ ÌäπÌôî Ìå®ÌÑ¥Îì§
            "field_search": [
                r"(?:show|find|list)\s+field\s+(.+)",
                r"field\s+(.+)",
                r"what\s+is\s+(.+)\s+field"
            ],
            "note_search": [
                r"(?:show|find|list)\s+note\s*(\d*)",
                r"note\s+(\d+)",
                r"notes?\s+about\s+(.+)"
            ],
            # Î¨∏ÏÑú ÎπÑÍµê Í¥ÄÎ†® Ìå®ÌÑ¥ Ï∂îÍ∞Ä
            "document_comparison": [
                r"compare\s+documents?",
                r"document\s+comparison",
                r"compare\s+all\s+documents?",
                r"comparison\s+between\s+documents?"
            ],
            "field_differences": [
                r"field\s+differences?",
                r"different\s+fields?",
                r"compare\s+fields?",
                r"field\s+comparison"
            ],
            "document_list": [
                r"list\s+(?:all\s+)?documents?",
                r"show\s+(?:all\s+)?documents?",
                r"what\s+documents?\s+do\s+you\s+have"
            ],
            "similarity": [
                r"similar(?:ity)?\s+(?:between\s+)?documents?",
                r"how\s+similar",
                r"common\s+(?:elements?|entities?|fields?)"
            ]
        }
        
        # ÎåÄÌè≠ ÌôïÏû•Îêú ÎèôÏùòÏñ¥ Îß§Ìïë (ÌôîÌïôÍ≥µÏ†ï ÌäπÌôî)
        self.synonyms = {
            # Í∏∞Î≥∏ Ïû•ÎπÑ Í¥ÄÎ†®
            "equipment": ["equipment", "machine", "device", "pump", "motor", "driver", "vessel", "tank", "compressor"],
            "project": ["project", "job", "document", "doc", "specification", "drawing"],
            "requirement": ["requirement", "spec", "specification", "parameter", "condition"],
            "person": ["person", "people", "engineer", "reviewer", "checker", "by", "checked", "reviewed"],
            "entity": ["entity", "entities", "item", "object", "node"],
            "literal": ["literal", "value", "data", "text"],
            "confidence": ["confidence", "certainty", "reliability", "accuracy"],
            "documents": ["documents", "files", "pdfs", "papers"],
            
            # ÌîÑÎ°úÏÑ∏Ïä§ Í¥ÄÎ†® (ÎåÄÌè≠ ÌôïÏû•)
            "temperature": ["temperature", "temp", "pumping temperature", "operating temperature", "minimum temperature", "maximum temperature"],
            "pressure": ["pressure", "suction pressure", "discharge pressure", "differential pressure", "vapor pressure"],
            "capacity": ["capacity", "flow", "flowrate", "flow rate", "rated capacity", "normal capacity"],
            "viscosity": ["viscosity", "fluid viscosity", "liquid viscosity"],
            "density": ["density", "specific gravity", "fluid density"],
            "head": ["head", "differential head", "pump head"],
            "npsh": ["npsh", "npsha", "net positive suction head", "available npsh"],
            
            # Ïû¨Î£å/Î∂ÑÎ•ò
            "material": ["material", "materials", "api class", "classification", "casing", "impeller", "shaft"],
            "api": ["api", "api class", "api classification", "american petroleum institute"],
            
            # Ïö¥Ï†Ñ Ï°∞Í±¥
            "duty": ["duty", "operation", "operating", "continuous", "intermittent"],
            "startup": ["startup", "start-up", "start up", "starting", "initial condition"],
            "minimum": ["minimum", "min", "lowest", "bottom"],
            "maximum": ["maximum", "max", "highest", "top"],
            "normal": ["normal", "standard", "typical", "operating"],
            "rated": ["rated", "design", "nominal"],
            
            # ÌôîÌïôÍ≥µÏ†ï Ïú†Ï≤¥
            "fluid": ["fluid", "liquid", "gas oil", "overflash", "ah feed", "am feed"],
            "overflash": ["overflash", "over flash", "overhead"],
            "feed": ["feed", "ah feed", "am feed", "feedstock"],
            
            # ÏïàÏ†Ñ/ÏúÑÌóò
            "flammable": ["flammable", "combustible", "fire hazard"],
            "toxic": ["toxic", "poisonous", "hazardous"],
            "sulfur": ["sulfur", "sulphur", "h2s", "hydrogen sulfide"],
            
            # ÏÑ§Ïπò/ÏúÑÏπò
            "location": ["location", "indoor", "outdoor", "under roof"],
            "insulation": ["insulation", "steam tracing", "steam jacket", "heating"],
            
            # Ï†úÏñ¥/Ï°∞Ïûë
            "manual": ["manual", "hand operated", "manually operated"],
            "automatic": ["automatic", "auto", "automatically operated"],
            
            # NOTE Í¥ÄÎ†® ÌÇ§ÏõåÎìú
            "note": ["note", "notes", "remark", "comment", "description"],
            "foundation": ["foundation", "base", "mounting", "support"],
            "turndown": ["turndown", "turn down", "reduced operation", "minimum operation"],
            "overdesign": ["overdesign", "over design", "safety margin", "design margin"],
            "mdmt": ["mdmt", "minimum design metal temperature", "minimum temperature"],
            
            # ÌïÑÎìú Í¥ÄÎ†®
            "field": ["field", "parameter", "specification", "data", "information"],
            "type": ["type", "classification", "category", "kind"],
            "required": ["required", "specification", "requirement", "needed"]
        }
        
        # ÏïΩÏñ¥/Ï†ÑÏ≤¥Î™Ö Îß§Ìïë
        self.abbreviation_mapping = {
            "npsh": "net positive suction head",
            "npsha": "npsh available",
            "mdmt": "minimum design metal temperature",
            "api": "american petroleum institute",
            "h2s": "hydrogen sulfide",
            "cp": "centipoise",
            "gpm": "gallons per minute",
            "bph": "barrels per hour",
            "psi": "pounds per square inch",
            "deg": "degree",
            "wt": "weight",
            "pt": "point",
            "max": "maximum",
            "min": "minimum",
            "nor": "normal",
            "oper": "operating",
            "temp": "temperature"
        }
        
        # Ïª®ÌÖçÏä§Ìä∏Î≥Ñ ÌÇ§ÏõåÎìú Îß§Ìïë
        self.context_keywords = {
            "temperature_related": ["pumping", "operating", "minimum", "maximum", "design", "metal"],
            "pressure_related": ["suction", "discharge", "differential", "vapor", "rated"],
            "flow_related": ["capacity", "rate", "continuous", "minimum", "maximum", "rated", "normal"],
            "material_related": ["casing", "impeller", "shaft", "api", "class", "classification"],
            "note_related": ["npsha", "foundation", "turndown", "overdesign", "mdmt", "startup", "slop"],
            "safety_related": ["flammable", "toxic", "h2s", "sulfur", "leakage", "hazard"]
        }
    
    def process_query(self, query, graph_data=None):
        """ÏûêÏó∞Ïñ¥ ÏøºÎ¶¨Î•º Ï≤òÎ¶¨ÌïòÏó¨ Í≤∞Í≥º Î∞òÌôò (Î¨∏ÏÑú ÎπÑÍµê Í∏∞Îä• Ï∂îÍ∞Ä)"""
        query = query.lower().strip()
        print(f"üîç Processing enhanced query with document comparison: '{query}'")
        
        # 1. ÏøºÎ¶¨ Ï†ÑÏ≤òÎ¶¨ (ÏïΩÏñ¥ ÌôïÏû•, ÎèôÏùòÏñ¥ Ï†ïÍ∑úÌôî)
        processed_query = self._preprocess_query(query)
        print(f"üîÑ Processed query: '{processed_query}'")
        
        # 2. ÏøºÎ¶¨ ÌÉÄÏûÖ ÏãùÎ≥Ñ
        query_type, matches = self._identify_query_type(processed_query)
        print(f"üéØ Query type: {query_type}, matches: {matches}")
        
        # 3. Î¨∏ÏÑú ÎπÑÍµê Í¥ÄÎ†® ÏøºÎ¶¨Îì§ (ÏÉàÎ°ú Ï∂îÍ∞Ä)
        if query_type == "document_comparison":
            return self._handle_document_comparison()
        elif query_type == "field_differences":
            return self._handle_field_differences()
        elif query_type == "document_list":
            return self._handle_document_list()
        elif query_type == "similarity":
            return self._handle_similarity_analysis()
        
        # 4. Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïö∞ÏÑ† ÏøºÎ¶¨Îì§
        if query_type in ["stats", "recent"]:
            return self._handle_database_query(query_type, matches, graph_data, processed_query)
        
        # 5. ÏÉàÎ°úÏö¥ ÌäπÌôî ÏøºÎ¶¨Îì§
        if query_type == "field_search":
            return self._handle_field_search(matches, graph_data)
        elif query_type == "note_search":
            return self._handle_note_search(matches, graph_data)
        
        # 6. Î©îÎ™®Î¶¨ Îç∞Ïù¥ÌÑ∞ ÏøºÎ¶¨Îì§ (Í∏∞Ï°¥ + Í∞úÏÑ†)
        if graph_data:
            nodes = graph_data.get("nodes", [])
            edges = graph_data.get("edges", [])
            stats = graph_data.get("stats", {})
            learning_results = graph_data.get("learning_results", {})
            
            if query_type == "show_all":
                return self._handle_show_all(matches, nodes, edges)
            elif query_type == "filter_by_confidence":
                return self._handle_confidence_filter(matches, nodes)
            elif query_type == "filter_by_class":
                return self._handle_class_filter(matches, nodes)
            elif query_type == "count":
                return self._handle_count(matches, nodes)
            elif query_type == "relationships":
                return self._handle_relationships(matches, nodes, edges)
        
        # 7. Fallback: Ìñ•ÏÉÅÎêú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í≤ÄÏÉâ
        return self._handle_enhanced_database_fallback(processed_query, query)
    
    # Î¨∏ÏÑú ÎπÑÍµê Í¥ÄÎ†® ÏÉàÎ°úÏö¥ Î©îÏÑúÎìúÎì§
    def _handle_document_comparison(self):
        """Î¨∏ÏÑú ÎπÑÍµê Ï≤òÎ¶¨ (ÌïÑÎìúÎ≥Ñ ÏÉÅÏÑ∏ ÎπÑÍµê Ìè¨Ìï®)"""
        try:
            # Í∏∞Î≥∏ ÎπÑÍµê
            basic_comparison = comparison_manager.compare_documents()
            
            if "error" in basic_comparison:
                return {"type": "error", "message": basic_comparison["error"]}
            
            # ÌïÑÎìúÎ≥Ñ ÏÉÅÏÑ∏ ÎπÑÍµê Ï∂îÍ∞Ä
            detailed_comparison = comparison_manager.compare_field_details()
            
            if "error" in detailed_comparison:
                print(f"‚ö†Ô∏è Detailed comparison warning: {detailed_comparison['error']}")
                detailed_comparison = {}
            
            # ÌÜµÌï© Í≤∞Í≥º
            combined_results = {
                "overview": basic_comparison.get("document_overview", []),
                "field_comparison": basic_comparison.get("field_comparison", {}),
                "entity_comparison": basic_comparison.get("entity_comparison", []),
                "statistics": basic_comparison.get("statistics_comparison", []),
                "similarity": basic_comparison.get("content_similarity", {}),
                # ÏÉàÎ°úÏö¥ ÏÉÅÏÑ∏ ÎπÑÍµê Í≤∞Í≥º
                "detailed_field_table": detailed_comparison.get("detailed_table", []),
                "field_differences": detailed_comparison.get("differences", []),
                "field_matrix": detailed_comparison.get("field_matrix", {}),
                "document_info": detailed_comparison.get("document_info", {})
            }
            
            overview = combined_results.get("overview", [])
            if overview:
                return {
                    "type": "detailed_comparison",
                    "data": combined_results,
                    "message": f"Detailed document comparison completed for {len(overview)} documents"
                }
            else:
                return {"type": "error", "message": "No documents available for comparison"}
                
        except Exception as e:
            return {"type": "error", "message": f"Document comparison failed: {str(e)}"}
    
    def _handle_field_differences(self):
        """ÌïÑÎìú Ï∞®Ïù¥Ï†ê Î∂ÑÏÑù"""
        try:
            comparison_results = comparison_manager.compare_documents()
            field_comparison = comparison_results.get("field_comparison", {})
            
            if not field_comparison:
                return {"type": "error", "message": "No field data available for comparison"}
            
            # ÌïÑÎìú Ï∞®Ïù¥Ï†ê Î∂ÑÏÑù
            field_differences = []
            for field_name, doc_dict in field_comparison.items():
                docs_with_field = list(doc_dict.keys())
                if len(docs_with_field) > 1:
                    confidences = list(doc_dict.values())
                    field_differences.append({
                        "field": field_name,
                        "documents": len(docs_with_field),
                        "avg_confidence": sum(confidences) / len(confidences),
                        "confidence_range": f"{min(confidences):.2f} - {max(confidences):.2f}"
                    })
            
            return {
                "type": "table",
                "data": field_differences,
                "message": f"Field comparison: {len(field_differences)} common fields found",
                "columns": ["field", "documents", "avg_confidence", "confidence_range"]
            }
            
        except Exception as e:
            return {"type": "error", "message": f"Field comparison failed: {str(e)}"}
    
    def _handle_document_list(self):
        """Î¨∏ÏÑú Î™©Î°ù ÌëúÏãú"""
        try:
            docs_df = comparison_manager.get_all_documents()
            
            if docs_df.empty:
                return {"type": "error", "message": "No documents found"}
            
            # ÎÇ†Ïßú Ìè¨Îß∑ÌåÖ
            docs_data = docs_df.to_dict('records')
            for doc in docs_data:
                if 'upload_time' in doc and doc['upload_time']:
                    doc['upload_time'] = doc['upload_time'].split('T')[0]
            
            return {
                "type": "table",
                "data": docs_data,
                "message": f"Found {len(docs_data)} documents in database",
                "columns": ["id", "filename", "upload_time", "content_length", "processing_time"]
            }
            
        except Exception as e:
            return {"type": "error", "message": f"Failed to get document list: {str(e)}"}
    
    def _handle_similarity_analysis(self):
        """Ïú†ÏÇ¨ÎèÑ Î∂ÑÏÑù"""
        try:
            comparison_results = comparison_manager.compare_documents()
            similarity = comparison_results.get("content_similarity", {})
            
            common_entities = similarity.get("common_entities", [])
            similarity_score = similarity.get("similarity_score", 0)
            
            if not common_entities:
                return {"type": "error", "message": "No common entities found between documents"}
            
            return {
                "type": "table",
                "data": common_entities[:10],  # ÏÉÅÏúÑ 10Í∞úÎßå
                "message": f"Similarity analysis: {similarity_score:.2%} similarity, {len(common_entities)} common entities",
                "columns": ["id", "doc_count", "avg_confidence"]
            }
            
        except Exception as e:
            return {"type": "error", "message": f"Similarity analysis failed: {str(e)}"}
    
    # Í∏∞Ï°¥ Î©îÏÑúÎìúÎì§ Ïú†ÏßÄ (ÏÉùÎûµ...)
    def _preprocess_query(self, query):
        """ÏøºÎ¶¨ Ï†ÑÏ≤òÎ¶¨: ÏïΩÏñ¥ ÌôïÏû• Î∞è ÎèôÏùòÏñ¥ Ï†ïÍ∑úÌôî"""
        processed = query
        
        # 1. ÏïΩÏñ¥ ÌôïÏû•
        for abbr, full_form in self.abbreviation_mapping.items():
            processed = re.sub(rf'\b{re.escape(abbr)}\b', full_form, processed, flags=re.IGNORECASE)
        
        # 2. ÌäπÏàò Î¨∏Ïûê Ï†ïÎ¶¨
        processed = re.sub(r'[^\w\s-]', ' ', processed)
        processed = re.sub(r'\s+', ' ', processed).strip()
        
        return processed
    
    def _identify_query_type(self, query):
        """ÏøºÎ¶¨ ÌÉÄÏûÖÍ≥º Îß§Ïπ≠Îêú Í∑∏Î£π ÏãùÎ≥Ñ"""
        for query_type, patterns in self.query_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, query, re.IGNORECASE)
                if match:
                    return query_type, match.groups()
        return "unknown", ()
    
    def _normalize_term(self, term):
        """Ïö©Ïñ¥ Ï†ïÍ∑úÌôî (ÎèôÏùòÏñ¥ Ï≤òÎ¶¨)"""
        if not term:
            return term
            
        term = term.lower()
        for canonical, synonyms in self.synonyms.items():
            if term in synonyms:
                return canonical
        return term
    
    def _handle_database_query(self, query_type, matches, graph_data, processed_query):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í∏∞Î∞ò ÏøºÎ¶¨ Ï≤òÎ¶¨ (Í≤ÄÏÉâ Í∞ïÌôî)"""
        try:
            if query_type == "stats":
                db_stats = self.db_manager.get_database_stats()
                stats_data = {
                    "Total Documents": db_stats.get("total_documents", 0),
                    "Total Nodes": db_stats.get("total_nodes", 0),
                    "Total Edges": db_stats.get("total_edges", 0),
                    "Total Patterns": db_stats.get("total_patterns", 0),
                    "High Confidence Entities": db_stats.get("high_confidence_entities", 0)
                }
                return {
                    "type": "stat",
                    "data": stats_data,
                    "message": "Database statistics across all processed documents"
                }
            
            elif query_type == "recent":
                recent_query = """
                    SELECT filename, upload_time, content_length, processing_time
                    FROM documents 
                    ORDER BY upload_time DESC 
                    LIMIT 10
                """
                result_df = self.db_manager.execute_query(recent_query)
                
                if not result_df.empty:
                    result_df['upload_time'] = result_df['upload_time'].apply(
                        lambda x: x.split('T')[0] if 'T' in str(x) else str(x)
                    )
                    return {
                        "type": "table",
                        "data": result_df.to_dict('records'),
                        "message": f"Found {len(result_df)} recent documents",
                        "columns": ["filename", "upload_time", "content_length", "processing_time"]
                    }
                else:
                    return {"type": "table", "data": [], "message": "No documents found in database"}
            
            elif query_type == "search":
                search_term = matches[0] if matches else ""
                return self._enhanced_database_search(search_term, processed_query)
        
        except Exception as e:
            print(f"‚ùå Enhanced fallback error: {e}")
            return {"type": "error", "message": "Enhanced search failed. Try simpler terms."}
    
    # ÎÇòÎ®∏ÏßÄ Î™®Îì† Í∏∞Ï°¥ Î©îÏÑúÎìúÎì§ÎèÑ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ...
    def _enhanced_database_search(self, search_term, processed_query):
        """Ìñ•ÏÉÅÎêú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í≤ÄÏÉâ (Îã§Ï§ë ÌÇ§ÏõåÎìú, Ïª®ÌÖçÏä§Ìä∏ Í≥†Î†§)"""
        try:
            # Í≤ÄÏÉâÏñ¥ÏóêÏÑú ÏùòÎØ∏ÏûàÎäî ÌÇ§ÏõåÎìúÎì§ Ï∂îÏ∂ú
            keywords = self._extract_meaningful_keywords(search_term, processed_query)
            print(f"üîç Extracted keywords: {keywords}")
            
            if not keywords:
                return {"type": "suggestions", "data": ["temperature", "pressure", "capacity", "npsha", "startup"], 
                       "message": "Try searching for specific terms like:"}
            
            # Îã§Ï§ë ÌÇ§ÏõåÎìú Í≤ÄÏÉâ Ï°∞Í±¥ ÏÉùÏÑ±
            search_conditions = []
            for keyword in keywords[:5]:  # ÏµúÎåÄ 5Í∞ú ÌÇ§ÏõåÎìú
                search_conditions.extend([
                    f"LOWER(n.id) LIKE '%{keyword}%'",
                    f"LOWER(n.ontology_class) LIKE '%{keyword}%'",
                    f"LOWER(d.filename) LIKE '%{keyword}%'"
                ])
            
            search_query = f"""
                SELECT DISTINCT n.id, n.node_type, n.ontology_class, n.confidence, d.filename
                FROM nodes n
                JOIN documents d ON n.document_id = d.id
                WHERE {' OR '.join(search_conditions)}
                ORDER BY n.confidence DESC, n.ontology_class
                LIMIT 25
            """
            
            result_df = self.db_manager.execute_query(search_query)
            
            return {
                "type": "table",
                "data": result_df.to_dict('records') if not result_df.empty else [],
                "message": f"Found {len(result_df)} items matching '{search_term}' (keywords: {', '.join(keywords)})",
                "columns": ["id", "node_type", "ontology_class", "confidence", "filename"]
            }
            
        except Exception as e:
            print(f"‚ùå Enhanced search error: {e}")
            return {"type": "error", "message": f"Enhanced search failed: {str(e)}"}
    
    def _extract_meaningful_keywords(self, search_term, processed_query):
        """Í≤ÄÏÉâÏñ¥ÏóêÏÑú ÏùòÎØ∏ÏûàÎäî ÌÇ§ÏõåÎìúÎì§ Ï∂îÏ∂ú"""
        keywords = set()
        
        # 1. ÏõêÎ≥∏ Í≤ÄÏÉâÏñ¥ Î∂ÑÌï†
        original_words = search_term.split()
        keywords.update([w for w in original_words if len(w) > 2])
        
        # 2. ÎèôÏùòÏñ¥ Îß§ÌïëÏóêÏÑú Í¥ÄÎ†® ÌÇ§ÏõåÎìú Ï∞æÍ∏∞
        for canonical, synonyms in self.synonyms.items():
            if any(word in search_term for word in synonyms):
                keywords.add(canonical)
                keywords.update([s for s in synonyms if len(s) > 2])
        
        # 3. Ïª®ÌÖçÏä§Ìä∏ ÌÇ§ÏõåÎìú Ï∂îÍ∞Ä
        for context, context_keywords in self.context_keywords.items():
            if any(keyword in search_term for keyword in context_keywords):
                keywords.update(context_keywords)
        
        # 4. Î∂àÏö©Ïñ¥ Ï†úÍ±∞
        stop_words = {"for", "the", "and", "or", "in", "on", "at", "to", "from", "with", "by"}
        keywords = keywords - stop_words
        
        return list(keywords)
    
    def _handle_field_search(self, matches, graph_data):
        """ÌïÑÎìú Í≤ÄÏÉâ Ï≤òÎ¶¨"""
        if not graph_data or not matches:
            return {"type": "error", "message": "No field search term provided"}
        
        search_term = matches[0].lower()
        nodes = graph_data.get("nodes", [])
        
        # ÌïÑÎìú Í¥ÄÎ†® ÎÖ∏ÎìúÎì§ Ï∞æÍ∏∞
        field_nodes = [n for n in nodes if n.get("ontology_class") == "Field" and 
                      search_term in n.get("id", "").lower()]
        
        return {
            "type": "table",
            "data": field_nodes,
            "message": f"Found {len(field_nodes)} fields matching '{search_term}'",
            "columns": ["id", "ontology_class", "confidence"]
        }
    
    def _handle_note_search(self, matches, graph_data):
        """NOTE Í≤ÄÏÉâ Ï≤òÎ¶¨"""
        if not graph_data:
            return {"type": "error", "message": "No graph data available"}
        
        nodes = graph_data.get("nodes", [])
        
        if matches and matches[0].isdigit():
            # ÌäπÏ†ï NOTE Î≤àÌò∏ Í≤ÄÏÉâ
            note_num = matches[0]
            note_nodes = [n for n in nodes if n.get("ontology_class") == "Note" and 
                         note_num in n.get("id", "")]
            message = f"Found NOTE {note_num}"
        else:
            # Î™®Îì† NOTE Í≤ÄÏÉâ
            note_nodes = [n for n in nodes if n.get("ontology_class") == "Note"]
            message = f"Found {len(note_nodes)} notes"
        
        return {
            "type": "table",
            "data": note_nodes,
            "message": message,
            "columns": ["id", "ontology_class", "confidence"]
        }
    
    def _handle_show_all(self, matches, nodes, edges):
        """'show all X' ÌÉÄÏûÖ ÏøºÎ¶¨ Ï≤òÎ¶¨ (Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ)"""
        if not matches:
            return {"type": "table", "data": nodes[:10], "message": "Showing all nodes (limited to 10)"}
        
        target = self._normalize_term(matches[0])
        
        if target in ["entity", "entities"]:
            entities = [n for n in nodes if n.get("type") == "entity"]
            return {
                "type": "table", 
                "data": entities,
                "message": f"Found {len(entities)} entities",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in ["literal", "literals", "value", "values"]:
            literals = [n for n in nodes if n.get("type") == "literal"]
            return {
                "type": "table", 
                "data": literals,
                "message": f"Found {len(literals)} literals",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in ["field", "fields"]:
            fields = [n for n in nodes if n.get("ontology_class") == "Field"]
            return {
                "type": "table", 
                "data": fields,
                "message": f"Found {len(fields)} fields",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in ["note", "notes"]:
            notes = [n for n in nodes if n.get("ontology_class") == "Note"]
            return {
                "type": "table", 
                "data": notes,
                "message": f"Found {len(notes)} notes",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in self.synonyms.get("equipment", []):
            equipment = [n for n in nodes if n.get("ontology_class", "").lower() == "equipment"]
            return {
                "type": "table", 
                "data": equipment,
                "message": f"Found {len(equipment)} equipment entities",
                "columns": ["id", "confidence"]
            }
        elif target in self.synonyms.get("project", []):
            projects = [n for n in nodes if n.get("ontology_class", "").lower() == "project"]
            return {
                "type": "table", 
                "data": projects,
                "message": f"Found {len(projects)} project entities",
                "columns": ["id", "confidence"]
            }
        else:
            # ÏùºÎ∞òÏ†ÅÏù∏ Í≤ÄÏÉâ
            filtered = [n for n in nodes if target in n.get("id", "").lower() or 
                       target in n.get("ontology_class", "").lower()]
            return {
                "type": "table", 
                "data": filtered,
                "message": f"Found {len(filtered)} items matching '{target}'",
                "columns": ["id", "type", "ontology_class", "confidence"]
            }
    
    def _handle_confidence_filter(self, matches, nodes):
        """Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ (Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ)"""
        if len(matches) >= 2 and matches[1].replace('.', '').isdigit():
            # ÌäπÏ†ï ÏûÑÍ≥ÑÍ∞í ÏßÄÏ†ïÎêú Í≤ΩÏö∞
            threshold = float(matches[1])
            target = self._normalize_term(matches[0])
        else:
            # 'high confidence' Îì±Ïùò Í≤ΩÏö∞
            threshold = 0.8
            target = self._normalize_term(matches[0]) if matches else "entity"
        
        filtered_nodes = []
        for node in nodes:
            confidence = node.get("confidence", 0)
            if confidence > threshold:
                if target == "entity" and node.get("type") == "entity":
                    filtered_nodes.append(node)
                elif target in node.get("ontology_class", "").lower():
                    filtered_nodes.append(node)
                elif target in ["all", "any", ""]:
                    filtered_nodes.append(node)
        
        return {
            "type": "table",
            "data": filtered_nodes,
            "message": f"Found {len(filtered_nodes)} items with confidence > {threshold}",
            "columns": ["id", "type", "ontology_class", "confidence"]
        }
    
    def _handle_class_filter(self, matches, nodes):
        """ÌÅ¥ÎûòÏä§/ÌÉÄÏûÖ Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ (Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ)"""
        if len(matches) >= 2:
            item_type = self._normalize_term(matches[0])
            class_name = self._normalize_term(matches[1])
            
            filtered = [n for n in nodes if 
                       class_name in n.get("ontology_class", "").lower() or
                       class_name in n.get("type", "").lower()]
            
            return {
                "type": "table",
                "data": filtered,
                "message": f"Found {len(filtered)} {item_type} of type {class_name}",
                "columns": ["id", "type", "ontology_class", "confidence"]
            }
        
        return {"type": "error", "message": "Could not parse class filter query"}
    
    def _handle_count(self, matches, nodes):
        """Í∞úÏàò ÏÑ∏Í∏∞ (ÌôïÏû•Îêú Î≤ÑÏ†Ñ)"""
        if not matches:
            total = len(nodes)
            return {
                "type": "stat",
                "data": {"Total Nodes": total},
                "message": f"Total count: {total}"
            }
        
        target = self._normalize_term(matches[0])
        
        if target in ["entity", "entities"]:
            count = sum(1 for n in nodes if n.get("type") == "entity")
        elif target in ["literal", "literals"]:
            count = sum(1 for n in nodes if n.get("type") == "literal")
        elif target in ["field", "fields"]:
            count = sum(1 for n in nodes if n.get("ontology_class") == "Field")
        elif target in ["note", "notes"]:
            count = sum(1 for n in nodes if n.get("ontology_class") == "Note")
        elif target in self.synonyms.get("equipment", []):
            count = sum(1 for n in nodes if n.get("ontology_class", "").lower() == "equipment")
        elif target in self.synonyms.get("project", []):
            count = sum(1 for n in nodes if n.get("ontology_class", "").lower() == "project")
        else:
            count = sum(1 for n in nodes if target in n.get("ontology_class", "").lower())
        
        return {
            "type": "stat",
            "data": {f"{target.title()} Count": count},
            "message": f"Count of {target}: {count}"
        }
    
    def _handle_relationships(self, matches, nodes, edges):
        """Í¥ÄÍ≥Ñ Ï†ïÎ≥¥ Ï≤òÎ¶¨ (ÌôïÏû•Îêú Î≤ÑÏ†Ñ)"""
        if not edges:
            return {
                "type": "table",
                "data": [],
                "message": "No relationships found in the current graph"
            }
        
        # Í¥ÄÍ≥Ñ ÌÜµÍ≥Ñ
        relation_counts = {}
        for edge in edges:
            rel = edge.get("relation", "unknown")
            relation_counts[rel] = relation_counts.get(rel, 0) + 1
        
        relationship_data = [
            {"Relationship Type": rel, "Count": count}
            for rel, count in relation_counts.items()
        ]
        
        return {
            "type": "table",
            "data": relationship_data,
            "message": f"Found {len(edges)} relationships of {len(relation_counts)} types",
            "columns": ["Relationship Type", "Count"]
        }
    
    def _handle_enhanced_database_fallback(self, processed_query, original_query):
        """Ìñ•ÏÉÅÎêú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ fallback Í≤ÄÏÉâ"""
        try:
            # Î©îÎ™®Î¶¨ Îç∞Ïù¥ÌÑ∞ Î®ºÏ†Ä Í≤ÄÏÉâ
            if hasattr(self, '_last_graph_data') and self._last_graph_data:
                memory_result = self._search_memory_data(original_query, self._last_graph_data)
                if memory_result and memory_result.get('data'):
                    return memory_result

            keywords = self._extract_meaningful_keywords(original_query, processed_query)
            
            if not keywords:
                return {
                    "type": "suggestions",
                    "data": [
                        "temperature ‚Üí pumping temperature fields",
                        "pressure ‚Üí suction/discharge pressure",
                        "capacity ‚Üí flow rate information", 
                        "npsha ‚Üí foundation notes",
                        "startup ‚Üí start-up conditions",
                        "api ‚Üí material classifications",
                        "compare documents ‚Üí document comparison",
                        "field differences ‚Üí field comparison analysis"
                    ],
                    "message": "Try these enhanced search examples:"
                }
            
            # Ïª®ÌÖçÏä§Ìä∏ Í∏∞Î∞ò ÌÇ§ÏõåÎìú ÌôïÏû•
            expanded_keywords = set(keywords)
            for keyword in keywords:
                for context, context_keywords in self.context_keywords.items():
                    if keyword in context_keywords:
                        expanded_keywords.update(context_keywords[:3])  # ÏµúÎåÄ 3Í∞ú Ï∂îÍ∞Ä
            
            keyword_conditions = " OR ".join([
                f"LOWER(n.id) LIKE '%{kw}%' OR LOWER(n.ontology_class) LIKE '%{kw}%'"
                for kw in list(expanded_keywords)[:8]  # ÏµúÎåÄ 8Í∞ú ÌÇ§ÏõåÎìú
            ])
            
            search_query = f"""
                SELECT n.id, n.node_type, n.ontology_class, n.confidence, d.filename
                FROM nodes n
                JOIN documents d ON n.document_id = d.id
                WHERE {keyword_conditions}
                ORDER BY n.confidence DESC, 
                    CASE n.ontology_class 
                        WHEN 'Field' THEN 1 
                        WHEN 'Note' THEN 2 
                        WHEN 'ProcessRequirement' THEN 3 
                        ELSE 4 
                    END
                LIMIT 20
            """
            
            result_df = self.db_manager.execute_query(search_query)
            
            if not result_df.empty:
                return {
                    "type": "table",
                    "data": result_df.to_dict('records'),
                    "message": f"Enhanced search found {len(result_df)} items (expanded from: {', '.join(keywords)})",
                    "columns": ["id", "node_type", "ontology_class", "confidence", "filename"]
                }
            else:
                return {
                    "type": "suggestions",
                    "data": [
                        "Try 'database statistics' to see available data",
                        "Try 'show recent documents' to see processed files",
                        "Try 'compare documents' for document comparison",
                        "Try specific terms like 'temperature', 'pressure', 'pump'"
                    ],
                    "message": "No matches found. Suggestions:"
                }
        
        except Exception as e:
            print(f"‚ùå Enhanced database fallback error: {e}")
            return {
                "type": "error",
                "message": f"Enhanced database search failed: {str(e)}"
            }
    
    def _search_memory_data(self, query, graph_data):
        """Î©îÎ™®Î¶¨ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏßÅÏ†ë Í≤ÄÏÉâ"""
        if not graph_data or 'nodes' not in graph_data:
            return None
        
        nodes = graph_data['nodes']
        query_lower = query.lower()
        
        # Î™®Îì† ÎÖ∏ÎìúÏóêÏÑú Í≤ÄÏÉâ (Entity + Literal)
        matches = [node for node in nodes 
                  if query_lower in node.get('id', '').lower()]
        
        return {
            "type": "table",
            "data": matches,
            "message": f"Found {len(matches)} items in memory matching '{query}'",
            "columns": ["id", "type", "ontology_class", "confidence"]
        }

class EnhancedDocumentComparisonManager(DocumentComparisonManager):
    def __init__(self, db_manager):
        super().__init__(db_manager)
        
        # ÌëúÏ§Ä ÌïÑÎìú Îß§Ìïë ÌÖåÏù¥Î∏î
        self.standard_field_mapping = {
            # Ïò®ÎèÑ Í¥ÄÎ†®
            "temperature": ["PUMPING TEMPERATURE", "TEMPERATURE", "TEMP", "OPERATING TEMPERATURE", "MAX TEMP", "MIN TEMP"],
            "pressure_suction": ["SUCTION PRESSURE", "SUCTION", "INLET PRESSURE"],
            "pressure_discharge": ["DISCHARGE PRESSURE", "DISCHARGE", "OUTLET PRESSURE"], 
            "capacity": ["CAPACITY", "FLOW RATE", "RATED CAPACITY", "NORMAL CAPACITY"],
            "viscosity": ["VISCOSITY", "FLUID VISCOSITY"],
            "specific_gravity": ["SPECIFIC GRAVITY", "DENSITY", "SP.GR"],
            
            # Ïû•ÎπÑ Ï†ïÎ≥¥
            "pump_type": ["PUMP TYPE", "TYPE", "EQUIPMENT TYPE"],
            "driver_type": ["DRIVER TYPE", "DRIVER", "MOTOR TYPE"],
            "service": ["SERVICE", "LIQUID NAME", "FLUID"],
            "duty": ["DUTY", "OPERATION", "OPERATING CASE"],
            
            # ÌîÑÎ°úÏ†ùÌä∏ Ï†ïÎ≥¥  
            "job_no": ["JOB NO", "PROJECT", "JOB NUMBER"],
            "item_no": ["ITEM NO", "EQUIPMENT NO", "TAG NO"],
            "doc_no": ["DOC NO", "DOCUMENT NO", "DRAWING NO"],
            "revision": ["REVISION", "REV", "REV NO"],
            
            # ÏïïÎ†• ÏÑ∏Î∂ÄÏÇ¨Ìï≠
            "vapor_pressure": ["VAPOR PRESSURE", "VP"],
            "design_pressure": ["DESIGN PRESSURE", "DESIGN PRESS"],
            "operating_pressure": ["OPERATING PRESSURE", "OPER PRESS"],
            
            # Í∏∞ÌÉÄ
            "notes": ["NOTES", "REMARKS", "COMMENTS"],
            "location": ["LOCATION", "SITE", "PLANT"],
            "client": ["CLIENT", "CUSTOMER", "OWNER"]
        }
        
        # Îã®ÏúÑ Ï†ïÍ∑úÌôî Îß§Ìïë
        self.unit_normalization = {
            "temperature": {"‚ÑÉ": "¬∞C", "DEG C": "¬∞C", "DEGREE C": "¬∞C"},
            "pressure": {"kg/cm2g": "kg/cm¬≤g", "kg/cm2A": "kg/cm¬≤A", "bar": "bar", "psi": "psi"},
            "flow": {"m3/h": "m¬≥/h", "m3/hr": "m¬≥/h", "M3/HR": "m¬≥/h"},
            "viscosity": {"cP": "cP", "Pa¬∑s": "Pa¬∑s"}
        }
    
    def compare_field_details(self, doc_ids=None):
        """ÌïÑÎìúÎ≥Ñ ÏÉÅÏÑ∏ ÎπÑÍµê"""
        try:
            if not doc_ids:
                docs_df = self.get_all_documents()
                if docs_df.empty:
                    return {"error": "No documents found"}
                doc_ids = docs_df['id'].tolist()[:10]  # ÏµúÎåÄ 10Í∞ú Î¨∏ÏÑú
            
            # Í∞Å Î¨∏ÏÑúÏùò ÌïÑÎìú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
            field_comparison_matrix = {}
            document_info = {}
            
            for doc_id in doc_ids:
                doc_fields = self._extract_document_fields(doc_id)
                if doc_fields:
                    doc_info = self._get_document_info(doc_id)
                    document_info[doc_id] = doc_info
                    
                    # ÌëúÏ§Ä ÌïÑÎìúÎ°ú Îß§Ìïë
                    mapped_fields = self._map_to_standard_fields(doc_fields)
                    
                    for standard_field, field_data in mapped_fields.items():
                        if standard_field not in field_comparison_matrix:
                            field_comparison_matrix[standard_field] = {}
                        
                        field_comparison_matrix[standard_field][doc_id] = field_data
            
            # ÎπÑÍµê Í≤∞Í≥º ÏÉùÏÑ±
            detailed_comparison = self._generate_field_comparison_table(field_comparison_matrix, document_info)
            differences = self._identify_field_differences(field_comparison_matrix, document_info)
            
            return {
                "field_matrix": field_comparison_matrix,
                "detailed_table": detailed_comparison,
                "differences": differences,
                "document_info": document_info
            }
            
        except Exception as e:
            print(f"‚ùå Detailed field comparison error: {e}")
            return {"error": str(e)}
    
    def _extract_document_fields(self, doc_id):
        """Î¨∏ÏÑúÏóêÏÑú ÌïÑÎìú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú"""
        try:
            # ÌïÑÎìú ÎÖ∏ÎìúÎì§ Ï°∞Ìöå
            query = f"""
                SELECT n.id, n.ontology_class, n.confidence
                FROM nodes n
                WHERE n.document_id = {doc_id} 
                AND (n.ontology_class IN ('Field', 'ProcessRequirement') OR n.id LIKE '%Field%')
                ORDER BY n.confidence DESC
            """
            
            result = self.db_manager.execute_query(query)
            
            if result.empty:
                return {}
            
            # ÌïÑÎìú Îç∞Ïù¥ÌÑ∞ ÌååÏã±
            fields = {}
            for _, row in result.iterrows():
                field_id = row['id']
                
                # ÌïÑÎìúÎ™ÖÍ≥º Í∞í Ï∂îÏ∂ú
                field_info = self._parse_field_id(field_id)
                if field_info:
                    fields[field_info['name']] = {
                        'value': field_info['value'],
                        'confidence': row['confidence'],
                        'type': row['ontology_class']
                    }
            
            return fields
            
        except Exception as e:
            print(f"‚ùå Field extraction error for doc {doc_id}: {e}")
            return {}
    
    def _parse_field_id(self, field_id):
        """ÌïÑÎìú IDÏóêÏÑú ÌïÑÎìúÎ™ÖÍ≥º Í∞í Ï∂îÏ∂ú"""
        try:
            # Ïòà: "Field_01_PUMP_TYPE_0" -> "PUMP TYPE"
            # Ïòà: "Field_PUMPING_TEMPERATURE_384_C_1" -> "PUMPING TEMPERATURE", "384¬∞C"
            
            parts = field_id.split('_')
            if len(parts) < 3:
                return None
            
            # 'Field' Ïù¥ÌõÑ Î∂ÄÎ∂Ñ Ï≤òÎ¶¨
            if parts[0] == 'Field':
                field_parts = []
                value_parts = []
                collecting_value = False
                
                for i, part in enumerate(parts[1:], 1):
                    # Ïà´ÏûêÍ∞Ä ÎÇòÏò§Î©¥ Í∞í Î∂ÄÎ∂Ñ ÏãúÏûë
                    if part.isdigit() and not collecting_value:
                        collecting_value = True
                        value_parts.append(part)
                    elif collecting_value:
                        value_parts.append(part)
                    else:
                        field_parts.append(part)
                
                field_name = ' '.join(field_parts).replace('FIELD', '').strip()
                field_value = ' '.join(value_parts) if value_parts else ""
                
                # Îã®ÏúÑ Ï†ïÍ∑úÌôî
                normalized_value = self._normalize_value_unit(field_value)
                
                return {
                    'name': field_name,
                    'value': normalized_value,
                    'original_id': field_id
                }
            
            return None
            
        except Exception as e:
            print(f"‚ùå Field parsing error for {field_id}: {e}")
            return None
    
    def _normalize_value_unit(self, value_str):
        """Í∞íÍ≥º Îã®ÏúÑ Ï†ïÍ∑úÌôî"""
        if not value_str:
            return ""
        
        # Îã®ÏúÑ Ï†ïÍ∑úÌôî
        for unit_type, unit_map in self.unit_normalization.items():
            for old_unit, new_unit in unit_map.items():
                if old_unit in value_str:
                    value_str = value_str.replace(old_unit, new_unit)
        
        return value_str.strip()
    
    def _map_to_standard_fields(self, document_fields):
        """Î¨∏ÏÑú ÌïÑÎìúÎ•º ÌëúÏ§Ä ÌïÑÎìúÎ°ú Îß§Ìïë"""
        mapped_fields = {}
        
        for doc_field_name, field_data in document_fields.items():
            # ÌëúÏ§Ä ÌïÑÎìú Ï∞æÍ∏∞
            standard_field = self._find_standard_field(doc_field_name)
            
            if standard_field:
                mapped_fields[standard_field] = {
                    'value': field_data['value'],
                    'confidence': field_data['confidence'],
                    'original_name': doc_field_name,
                    'type': field_data['type']
                }
        
        return mapped_fields
    
    def _find_standard_field(self, field_name):
        """ÌïÑÎìúÎ™ÖÏùÑ ÌëúÏ§Ä ÌïÑÎìúÎ°ú Îß§Ìïë"""
        field_name_upper = field_name.upper()
        
        for standard_field, variations in self.standard_field_mapping.items():
            for variation in variations:
                if variation.upper() in field_name_upper or field_name_upper in variation.upper():
                    return standard_field
        
        return None  # Îß§ÌïëÎêòÏßÄ ÏïäÏùÄ ÌïÑÎìú
    
    def _get_document_info(self, doc_id):
        """Î¨∏ÏÑú Ï†ïÎ≥¥ Ï°∞Ìöå"""
        try:
            query = f"""
                SELECT filename, upload_time, content_length
                FROM documents
                WHERE id = {doc_id}
            """
            result = self.db_manager.execute_query(query)
            
            if not result.empty:
                return result.iloc[0].to_dict()
            return {}
            
        except Exception as e:
            print(f"‚ùå Document info error: {e}")
            return {}
    
    def _generate_field_comparison_table(self, field_matrix, document_info):
        """ÌïÑÎìú ÎπÑÍµê ÌÖåÏù¥Î∏î ÏÉùÏÑ±"""
        comparison_table = []
        
        for standard_field, doc_data in field_matrix.items():
            row = {
                'Standard_Field': standard_field.replace('_', ' ').title(),
                'Field_Type': self._get_field_category(standard_field)
            }
            
            # Í∞Å Î¨∏ÏÑúÎ≥Ñ Í∞í Ï∂îÍ∞Ä
            for doc_id, field_data in doc_data.items():
                doc_filename = document_info.get(doc_id, {}).get('filename', f'Doc_{doc_id}')
                safe_filename = doc_filename.replace('.pdf', '').replace(' ', '_')[:20]  # ÌååÏùºÎ™Ö Îã®Ï∂ï
                
                row[f'{safe_filename}_Value'] = field_data['value']
                row[f'{safe_filename}_Confidence'] = f"{field_data['confidence']:.2f}"
            
            # Ï∞®Ïù¥Ï†ê ÌëúÏãú
            values = [field_data['value'] for field_data in doc_data.values() if field_data['value']]
            unique_values = list(set(values))
            row['Has_Differences'] = "Yes" if len(unique_values) > 1 else "No"
            row['Unique_Values_Count'] = len(unique_values)
            
            comparison_table.append(row)
        
        return comparison_table
    
    def _get_field_category(self, standard_field):
        """ÌïÑÎìú Ïπ¥ÌÖåÍ≥†Î¶¨ Î∞òÌôò"""
        categories = {
            'Process': ['temperature', 'pressure_suction', 'pressure_discharge', 'capacity', 'viscosity', 'specific_gravity'],
            'Equipment': ['pump_type', 'driver_type', 'service', 'duty'],
            'Project': ['job_no', 'item_no', 'doc_no', 'revision'],
            'Other': ['notes', 'location', 'client']
        }
        
        for category, fields in categories.items():
            if standard_field in fields:
                return category
        
        return 'Unknown'
    
    def _identify_field_differences(self, field_matrix, document_info):
        """ÌïÑÎìú Ï∞®Ïù¥Ï†ê ÏãùÎ≥Ñ"""
        differences = []
        
        for standard_field, doc_data in field_matrix.items():
            values = {}
            for doc_id, field_data in doc_data.items():
                if field_data['value']:
                    doc_name = document_info.get(doc_id, {}).get('filename', f'Doc_{doc_id}')
                    values[doc_name] = field_data['value']
            
            # Í∞íÏù¥ Îã§Î•∏ Í≤ΩÏö∞Îßå Í∏∞Î°ù
            unique_values = list(set(values.values()))
            if len(unique_values) > 1:
                differences.append({
                    'field': standard_field.replace('_', ' ').title(),
                    'category': self._get_field_category(standard_field),
                    'differences': values,
                    'unique_count': len(unique_values)
                })
        
        return sorted(differences, key=lambda x: x['unique_count'], reverse=True)

# Ïò®ÌÜ®Î°úÏßÄ ÌïôÏäµÏûê Î∞è ÏøºÎ¶¨ ÌîÑÎ°úÏÑ∏ÏÑú Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±
ontology_learner = AdvancedOntologyLearner()
db_manager = DatabaseManager()
query_processor = NaturalLanguageQueryProcessor(ontology_learner, db_manager)

# Í∏∞Ï°¥ DocumentComparisonManagerÎ•º Enhanced Î≤ÑÏ†ÑÏúºÎ°ú ÍµêÏ≤¥
comparison_manager = EnhancedDocumentComparisonManager(db_manager)

# Dash Ïï± ÏÉùÏÑ±
app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])

app.layout = dbc.Container([
    html.H1("üè≠ Industrial Knowledge Graph Analyzer", className="text-primary mb-4"),
    
    dbc.Row([
        # ÏôºÏ™Ω Ìå®ÎÑê
        dbc.Col([
            dbc.Card([
                dbc.CardBody([
                    html.H5("üìÑ Document Upload"),
                    dcc.Upload(
                        id="upload-pdf",
                        children=html.Div([
                            "Drag and drop PDF files or ",
                            html.A("click to select", style={"color": "#007bff"})
                        ]),
                        style={
                            'width': '100%',
                            'height': '60px',
                            'lineHeight': '60px',
                            'borderWidth': '2px',
                            'borderStyle': 'dashed',
                            'borderRadius': '5px',
                            'textAlign': 'center',
                            'backgroundColor': '#f8f9fa'
                        },
                        multiple=True  # Îã§Ï§ë ÌååÏùº ÏóÖÎ°úÎìú ÏßÄÏõê
                    )
                ])
            ], className="mb-3"),
            
            dbc.Card([
                dbc.CardBody([
                    html.H5("üéØ Quick Actions"),
                    dbc.Button("üß™ Test Connection", id="test-btn", color="secondary", className="w-100 mb-2", size="sm"),
                    dbc.Button("üìä Load Sample Data", id="sample-btn", color="info", className="w-100 mb-2", size="sm"),
                    dbc.Button("üîÑ Update Charts", id="update-btn", color="warning", className="w-100 mb-2", size="sm"),
                    dbc.Button("üìã Compare Documents", id="compare-btn", color="success", className="w-100 mb-2", size="sm"),
                    dbc.Button("ü§ñ R-GCN Predict", id="rgcn-btn", color="primary", className="w-100 mb-2", size="sm"),
                ])
            ], className="mb-4"),
            
            html.Div(id="status-output", className="mb-4"),
            
            # ÏóÖÎ°úÎìúÎêú Î¨∏ÏÑú Î™©Î°ù
            html.Div(id="document-list", className="mb-4"),
            
            dbc.Card([
                dbc.CardBody([
                    html.H6("üìà System Info"),
                    html.P("‚úÖ Ontology Classes: 7", className="mb-1"),
                    html.P("‚úÖ Object Properties: 7", className="mb-1"),
                    html.P("‚úÖ Datatype Properties: 6", className="mb-1"),
                    html.P("üß† Advanced Learning: Enabled", className="mb-1"),
                    html.P("üíæ Database: Connected", className="mb-1"),
                    html.P("üìã Document Comparison: Enabled", className="mb-1"),
                    html.P("ü§ñ R-GCN: " + ("Enabled" if rgcn_manager else "Disabled"), className="mb-0"),
                ])
            ]),
            
            html.Div(id="learning-stats", className="mt-3")
        ], width=4),
        
        # Ïò§Î•∏Ï™Ω Ìå®ÎÑê
        dbc.Col([
            # ÏûêÏó∞Ïñ¥ ÏøºÎ¶¨ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ (Í∞úÏÑ†Îê®)
            dbc.Card([
                dbc.CardBody([
                    html.H5("üó£Ô∏è Natural Language Query"),
                    dbc.InputGroup([
                        dbc.Input(
                            id="nl-query-input",
                            placeholder="e.g., 'compare documents', 'field differences', 'document similarity', 'show all equipment with high confidence'",
                            type="text"
                        ),
                        dbc.Button("üîç Query", id="query-btn", color="primary")
                    ]),
                    html.Div(id="query-output", className="mt-3")
                ])
            ], className="mb-4"),
            
            # Î¨∏ÏÑú ÎπÑÍµê Í≤∞Í≥º ÌëúÏãú ÏòÅÏó≠
            html.Div(id="comparison-output", className="mb-4"),
            
            html.H4("üìä Node Classification"),
            dcc.Graph(id="pie-chart", style={'height': '400px'}),
            
            html.Hr(),
            
            html.H4("üï∏Ô∏è Knowledge Graph"),
            dcc.Graph(id="network-graph", style={'height': '500px'})
        ], width=8)
    ]),
    
    # Ïà®Í≤®ÏßÑ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•ÏÜå
    dcc.Store(id="data-store", data={}),
    dcc.Store(id="comparison-store", data={})  # ÎπÑÍµê Í≤∞Í≥º Ï†ÄÏû•
], fluid=True)

# Îã§Ï§ë ÌååÏùº ÏóÖÎ°úÎìú Î∞è Î¨∏ÏÑú ÎπÑÍµê ÏΩúÎ∞± - ÏÇ≠Ï†úÎê® (ÏúÑÏùò ÌÜµÌï© ÏΩúÎ∞±ÏúºÎ°ú ÎåÄÏ≤¥)

# Î™®Îì† Î≤ÑÌäº ÏΩúÎ∞±ÏùÑ ÌïòÎÇòÎ°ú ÌÜµÌï©
@app.callback(
    [Output("status-output", "children"), 
     Output("data-store", "data"),
     Output("document-list", "children"),
     Output("comparison-output", "children")],
    [Input("test-btn", "n_clicks"), 
     Input("sample-btn", "n_clicks"), 
     Input("upload-pdf", "contents"),
     Input("rgcn-btn", "n_clicks"),
     Input("compare-btn", "n_clicks")],
    [State("upload-pdf", "filename"),
     State("data-store", "data")],
    prevent_initial_call=True
)
def handle_all_buttons(test_clicks, sample_clicks, pdf_contents_list, rgcn_clicks, compare_clicks, pdf_filenames, stored_data):
    global db_manager, comparison_manager, rgcn_manager
    from dash import ctx
    
    trigger_id = ctx.triggered[0]['prop_id'].split('.')[0] if ctx.triggered else None
    print(f"üî• Button callback triggered: {trigger_id}")
    
    # Í∏∞Î≥∏Í∞íÎì§
    status_msg = ""
    data = stored_data or {}
    doc_list = ""
    comparison_result = ""
    
    if trigger_id == "test-btn":
        status_msg = dbc.Alert("‚úÖ Connection test successful! Database ready.", color="success")
        
    elif trigger_id == "sample-btn":
        G, node_features, predictions = create_sample_graph()
        
        data = {
            "nodes": [{"id": node, **node_features[node]} for node in G.nodes()],
            "edges": [{"source": u, "target": v, "relation": data.get("relation", "unknown")} 
                     for u, v, data in G.edges(data=True)],
            "predictions": predictions,
            "stats": {
                "total_nodes": G.number_of_nodes(),
                "total_edges": G.number_of_edges(),
                "source": "sample"
            }
        }
        
        status_msg = dbc.Alert([
            html.H5("‚úÖ Sample data loaded!", className="alert-heading"),
            html.P(f"Created {G.number_of_nodes()} nodes and {G.number_of_edges()} edges"),
            html.P("Click 'Update Charts' to see visualizations!")
        ], color="success")
        
    elif trigger_id == "upload-pdf" and pdf_contents_list:
        # Îã§Ï§ë ÌååÏùº Ï≤òÎ¶¨
        if not isinstance(pdf_contents_list, list):
            pdf_contents_list = [pdf_contents_list]
            pdf_filenames = [pdf_filenames]
        
        processed_files = []
        all_graph_data = {}
        
        for pdf_content, filename in zip(pdf_contents_list, pdf_filenames):
            try:
                if not filename or not filename.lower().endswith('.pdf'):
                    continue
                
                content_type, content_string = pdf_content.split(',')
                decoded = base64.b64decode(content_string)
                
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                    tmp_file.write(decoded)
                    tmp_file_path = tmp_file.name
                
                try:
                    # PDF ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
                    pdf_text = extract_pdf_text(tmp_file_path)
                    
                    if len(pdf_text.strip()) < 10:
                        continue
                    
                    # Í∑∏ÎûòÌîÑ ÏÉùÏÑ±
                    G, node_features, predictions, learning_results = process_pdf_to_graph(pdf_text, filename)
                    
                    # Í∑∏ÎûòÌîÑ Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ±
                    graph_data = {
                        "nodes": [{"id": node, **node_features[node]} for node in G.nodes()],
                        "edges": [{"source": u, "target": v, "relation": data.get("relation", "unknown")} 
                                 for u, v, data in G.edges(data=True)],
                        "predictions": predictions,
                        "learning_results": learning_results,
                        "stats": {
                            "total_nodes": G.number_of_nodes(),
                            "total_edges": G.number_of_edges(),
                            "source": "pdf",
                            "filename": filename
                        }
                    }
                    
                    # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï†ÄÏû•
                    doc_id = db_manager.save_processing_results(
                        filename, pdf_text, graph_data, learning_results, processing_time=0
                    )
                    
                    if doc_id:
                        # ÎπÑÍµê Í¥ÄÎ¶¨ÏûêÏóê Ï∂îÍ∞Ä
                        comparison_manager.add_document(doc_id, filename, graph_data, learning_results)
                        processed_files.append(filename)
                        all_graph_data[filename] = graph_data
                    
                finally:
                    if os.path.exists(tmp_file_path):
                        os.unlink(tmp_file_path)
            
            except Exception as e:
                print(f"‚ùå Error processing {filename}: {e}")
                continue
        
        if processed_files:
            # Î¨∏ÏÑú Î™©Î°ù ÌëúÏãú
            doc_list_items = [
                html.Li(f"üìÑ {filename}", className="mb-1") 
                for filename in processed_files
            ]
            doc_list = dbc.Card([
                dbc.CardBody([
                    html.H6("üìö Uploaded Documents", className="card-title"),
                    html.Ul(doc_list_items, className="mb-0")
                ])
            ], className="border-success")
            
            status_msg = dbc.Alert([
                html.H5(f"‚úÖ {len(processed_files)} documents processed!", className="alert-heading"),
                html.P("All documents saved to database and ready for comparison"),
                html.P("Try queries: 'compare documents', 'field differences', 'document similarity'")
            ], color="success")
            
            # ÎßàÏßÄÎßâ Ï≤òÎ¶¨Îêú Î¨∏ÏÑúÏùò Îç∞Ïù¥ÌÑ∞ Î∞òÌôò
            data = list(all_graph_data.values())[-1] if all_graph_data else {}
        else:
            status_msg = dbc.Alert("‚ùå No valid PDF files processed", color="warning")
    
    elif trigger_id == "rgcn-btn":
        # R-GCN ÏòàÏ∏° Ï≤òÎ¶¨
        if not stored_data or not rgcn_manager:
            status_msg = dbc.Alert("‚ö†Ô∏è No data available for R-GCN prediction", color="warning")
        else:
            try:
                # R-GCN Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
                rgcn_data = rgcn_manager.prepare_data(stored_data)
                
                if rgcn_data is None:
                    status_msg = dbc.Alert("‚ö†Ô∏è Insufficient data for R-GCN prediction", color="warning")
                else:
                    # Î™®Îç∏ ÌõàÎ†®
                    training_success = rgcn_manager.train_model(rgcn_data)
                    
                    if not training_success:
                        status_msg = dbc.Alert("‚ùå R-GCN training failed", color="danger")
                    else:
                        # ÏòàÏ∏° ÏàòÌñâ
                        predictions = rgcn_manager.predict(rgcn_data)
                        
                        if not predictions:
                            status_msg = dbc.Alert("‚ùå R-GCN prediction failed", color="danger")
                        else:
                            # ÏòàÏ∏° Í≤∞Í≥º Î∂ÑÏÑù
                            pred_counts = Counter(predictions)
                            total_nodes = len(predictions)
                            
                            status_msg = dbc.Alert([
                                html.H5("ü§ñ R-GCN Prediction Completed!", className="alert-heading"),
                                html.P(f"Predicted {total_nodes} nodes with neural network:"),
                                html.Ul([
                                    html.Li(f"Class 0 (Unknown): {pred_counts.get(0, 0)} nodes"),
                                    html.Li(f"Class 1 (Entity): {pred_counts.get(1, 0)} nodes"),
                                    html.Li(f"Class 2 (Literal): {pred_counts.get(2, 0)} nodes")
                                ]),
                                html.P("Neural predictions can be compared with rule-based classifications!")
                            ], color="success")
                            
            except Exception as e:
                status_msg = dbc.Alert(f"‚ùå R-GCN error: {str(e)}", color="danger")
    
    elif trigger_id == "compare-btn":
        # Î¨∏ÏÑú ÎπÑÍµê Ï≤òÎ¶¨
        try:
            result = query_processor._handle_document_comparison()
            
            if result["type"] == "error":
                comparison_result = dbc.Alert(f"‚ùå {result['message']}", color="danger")
            else:
                comparison_data = result["data"]
                
                # Î¨∏ÏÑú Í∞úÏöî ÌÖåÏù¥Î∏î
                overview_table = None
                if comparison_data.get("overview"):
                    overview_df = pd.DataFrame(comparison_data["overview"])
                    overview_df['upload_time'] = overview_df['upload_time'].apply(
                        lambda x: x.split('T')[0] if 'T' in str(x) else str(x)
                    )
                    overview_table = dbc.Table.from_dataframe(
                        overview_df[['filename', 'node_count', 'edge_count', 'avg_confidence', 'upload_time']], 
                        striped=True, bordered=True, hover=True
                    )
                
                # ÌÜµÍ≥Ñ ÎπÑÍµê Ï∞®Ìä∏
                stats_chart = None
                if comparison_data.get("statistics"):
                    stats_df = pd.DataFrame(comparison_data["statistics"])
                    import plotly.express as px
                    fig = px.bar(
                        stats_df, 
                        x='filename', 
                        y=['entity_count', 'literal_count', 'edge_count'],
                        title="Document Statistics Comparison",
                        barmode='group'
                    )
                    fig.update_layout(height=300)
                    stats_chart = dcc.Graph(figure=fig)
                
                # Ïú†ÏÇ¨ÎèÑ Ï†ïÎ≥¥
                similarity_info = comparison_data.get("similarity", {})
                similarity_score = similarity_info.get("similarity_score", 0)
                common_entities_count = len(similarity_info.get("common_entities", []))
                
                comparison_result = dbc.Card([
                    dbc.CardBody([
                        html.H5("üìã Document Comparison Results", className="card-title"),
                        
                        html.H6("üìä Document Overview"),
                        overview_table if overview_table is not None else html.P("No overview data available"),
                        
                        html.Hr(),
                        
                        html.H6("üìà Statistics Comparison"),
                        stats_chart if stats_chart is not None else html.P("No statistics data available"),
                        
                        html.Hr(),
                        
                        html.H6("üîç Similarity Analysis"),
                        html.P(f"üìã Similarity Score: {similarity_score:.2%}"),
                        html.P(f"ü§ù Common Entities: {common_entities_count}"),
                    ])
                ], className="border-info")
                
        except Exception as e:
            comparison_result = dbc.Alert(f"‚ùå Comparison failed: {str(e)}", color="danger")
    
    return status_msg, data, doc_list, comparison_result

# Î¨∏ÏÑú ÎπÑÍµê ÏΩúÎ∞± - ÏÇ≠Ï†úÎê® (ÏúÑÏùò ÌÜµÌï© ÏΩúÎ∞±ÏúºÎ°ú ÎåÄÏ≤¥)

@app.callback(
    Output("pie-chart", "figure"),
    [Input("update-btn", "n_clicks")],
    [State("data-store", "data")]
)
def update_pie_chart(n_clicks, stored_data):
    print(f"üéØ Pie chart callback - clicks: {n_clicks}, data: {bool(stored_data)}")
    
    if n_clicks and stored_data and "predictions" in stored_data:
        predictions = stored_data["predictions"]
        pred_counts = Counter(predictions)
        
        labels = ["Class", "Entity", "Literal"]
        values = [pred_counts.get(i, 0) for i in range(3)]
        colors = ["#ff9999", "#66b3ff", "#99ff99"]
        
        fig = go.Figure()
        fig.add_trace(go.Pie(
            labels=labels,
            values=values,
            hole=0.3,
            textinfo="label+percent+value",
            marker=dict(colors=colors)
        ))
        
        fig.update_layout(
            title=f"Node Classification Results (Total: {sum(values)})",
            height=400,
            showlegend=True
        )
        
        print(f"‚úÖ Pie chart created: {values}")
        return fig
    
    return go.Figure().add_annotation(
        text="Load sample data or upload PDF, then click 'Update Charts'",
        showarrow=False, x=0.5, y=0.5
    )

@app.callback(
    Output("network-graph", "figure"),
    [Input("update-btn", "n_clicks")],
    [State("data-store", "data")]
)
def update_network_graph(n_clicks, stored_data):
    print(f"üï∏Ô∏è Network graph callback - clicks: {n_clicks}, data: {bool(stored_data)}")
    
    if n_clicks and stored_data and "nodes" in stored_data:
        nodes = stored_data["nodes"]
        edges = stored_data["edges"]
        
        G = nx.DiGraph()
        for node in nodes:
            G.add_node(node["id"])
        for edge in edges:
            G.add_edge(edge["source"], edge["target"])
        
        pos = nx.spring_layout(G, k=2, iterations=50)
        
        edge_x, edge_y = [], []
        for edge in G.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
        
        node_x = [pos[node][0] for node in G.nodes()]
        node_y = [pos[node][1] for node in G.nodes()]
        node_text = [node[:15] + "..." if len(node) > 15 else node for node in G.nodes()]
        
        # ÎÖ∏Îìú ÏÉâÏÉÅ Î∞è ÌÅ¨Í∏∞ (Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò)
        node_colors = []
        node_sizes = []
        hover_text = []
        
        for node in G.nodes():
            node_info = next((n for n in nodes if n["id"] == node), {})
            confidence = node_info.get("confidence", 0.5)
            node_type = node_info.get("type", "unknown")
            ontology_class = node_info.get("ontology_class", "Unknown")
            
            # Ïã†Î¢∞ÎèÑÏóê Îî∞Î•∏ ÌÅ¨Í∏∞
            node_sizes.append(15 + confidence * 25)
            
            # ÌÉÄÏûÖÏóê Îî∞Î•∏ ÏÉâÏÉÅ
            if node_type == "entity":
                base_color = [70, 130, 180]  # Ïä§Ìã∏Î∏îÎ£®
            elif node_type == "literal":
                base_color = [60, 179, 113]  # ÎØ∏ÎîîÏóÑÏî®Í∑∏Î¶∞
            else:
                base_color = [255, 99, 71]   # ÌÜ†ÎßàÌÜ†
            
            # Ïã†Î¢∞ÎèÑÏóê Îî∞Î•∏ Ìà¨Î™ÖÎèÑ
            alpha = 0.4 + confidence * 0.6
            color = f"rgba({base_color[0]}, {base_color[1]}, {base_color[2]}, {alpha})"
            node_colors.append(color)
            
            # Ìò∏Î≤Ñ ÌÖçÏä§Ìä∏
            hover_text.append(
                f"<b>{node}</b><br>" +
                f"Type: {node_type}<br>" +
                f"Class: {ontology_class}<br>" +
                f"Confidence: {confidence:.3f}"
            )
        
        fig = go.Figure()
        
        # Ïó£ÏßÄ Ï∂îÍ∞Ä
        fig.add_trace(go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=1.5, color='rgba(125,125,125,0.5)'),
            mode='lines',
            showlegend=False,
            hoverinfo='none'
        ))
        
        # ÎÖ∏Îìú Ï∂îÍ∞Ä (Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò ÏãúÍ∞ÅÌôî)
        fig.add_trace(go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            text=node_text,
            textposition="middle center",
            textfont=dict(size=8, color="white"),
            marker=dict(
                size=node_sizes,
                color=node_colors,
                line=dict(width=1, color='rgba(50,50,50,0.8)')
            ),
            showlegend=False,
            hoverinfo='text',
            hovertext=hover_text
        ))
        
        fig.update_layout(
            title=f"Advanced Knowledge Graph ({G.number_of_nodes()} nodes, {G.number_of_edges()} edges)<br><sub>Node size = confidence, Color = type (Blue=Entity, Green=Literal, Red=Other)</sub>",
            height=500,
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            showlegend=False,
            plot_bgcolor='rgba(240,240,240,0.2)',
            margin=dict(l=20, r=20, t=80, b=20)
        )
        
        print(f"‚úÖ Enhanced network graph created: {G.number_of_nodes()} nodes")
        return fig
    
    return go.Figure().add_annotation(
        text="Load sample data or upload PDF, then click 'Update Charts'",
        showarrow=False, x=0.5, y=0.5
    )

@app.callback(
    Output("learning-stats", "children"),
    [Input("data-store", "data")]
)
def update_learning_stats(stored_data):
    """Ïò®ÌÜ®Î°úÏßÄ ÌïôÏäµ ÌÜµÍ≥Ñ ÌëúÏãú"""
    if not stored_data or "learning_results" not in stored_data:
        return ""
    
    learning_results = stored_data["learning_results"]
    stats = stored_data.get("stats", {})
    
    confidence_scores = learning_results.get("confidence_scores", {})
    domain_insights = learning_results.get("domain_insights", {})
    
    if not confidence_scores and not domain_insights:
        return ""
    
    # Ïã†Î¢∞ÎèÑ Î∂ÑÌè¨
    high_conf = sum(1 for c in confidence_scores.values() if c > 0.8)
    med_conf = sum(1 for c in confidence_scores.values() if 0.5 < c <= 0.8)
    low_conf = sum(1 for c in confidence_scores.values() if c <= 0.5)
    
    return dbc.Card([
        dbc.CardBody([
            html.H6("üß† Learning Analytics", className="card-title"),
            html.P(f"üéñÔ∏è High Confidence: {high_conf}", className="mb-1"),
            html.P(f"üèÉ Medium Confidence: {med_conf}", className="mb-1"),
            html.P(f"ü§î Low Confidence: {low_conf}", className="mb-1"),
            html.Hr(className="my-2"),
            html.P(f"üìà Technical Density: {domain_insights.get('technical_density', 0):.3f}", className="mb-1"),
            html.P(f"üî¨ Document Type: {domain_insights.get('document_type', 'Unknown')}", className="mb-0"),
        ])
    ], className="border-info")

@app.callback(
    Output("query-output", "children"),
    [Input("query-btn", "n_clicks")],
    [State("nl-query-input", "value"), State("data-store", "data")]
)
def handle_natural_language_query(n_clicks, query_text, stored_data):
    """ÏûêÏó∞Ïñ¥ ÏøºÎ¶¨ Ï≤òÎ¶¨ (Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌÜµÌï© + Î¨∏ÏÑú ÎπÑÍµê)"""
    if not n_clicks or not query_text:
        return html.Div([
            html.P("üí° Query Examples:", className="mb-2 font-weight-bold"),
            html.Ul([
                html.Li("database statistics"),
                html.Li("show recent documents"),
                html.Li("compare documents"),
                html.Li("field differences"),
                html.Li("document similarity"),
                html.Li("search for equipment"),
                html.Li("show all entities with high confidence"),
                html.Li("count all nodes"),
                html.Li("show relationships")
            ], className="mb-0")
        ])
    
    print(f"üó£Ô∏è Natural language query: '{query_text}'")
    
    try:
        # ÏøºÎ¶¨ Ï≤òÎ¶¨ (Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïö∞ÏÑ† + Î¨∏ÏÑú ÎπÑÍµê)
        result = query_processor.process_query(query_text, stored_data)
        
        # Í≤∞Í≥º Î†åÎçîÎßÅ
        return render_query_result(result)
        
    except Exception as e:
        print(f"‚ùå Query processing error: {e}")
        return dbc.Alert(f"‚ùå Error processing query: {str(e)}", color="danger")

def render_query_result(result):
    """ÏøºÎ¶¨ Í≤∞Í≥ºÎ•º Î†åÎçîÎßÅ (ÏÉÅÏÑ∏ ÌïÑÎìú ÎπÑÍµê ÏßÄÏõê)"""
    result_type = result.get("type", "error")
    message = result.get("message", "")
    data = result.get("data", [])
    
    components = [
        html.P(f"‚úÖ {message}", className="text-success font-weight-bold mb-3")
    ]
    
    if result_type == "detailed_comparison":
        # ÏÉÅÏÑ∏ Î¨∏ÏÑú ÎπÑÍµê Í≤∞Í≥º Ï≤òÎ¶¨
        comparison_data = data
        
        # 1. Î¨∏ÏÑú Í∞úÏöî
        if comparison_data.get("overview"):
            overview_df = pd.DataFrame(comparison_data["overview"])
            overview_df['upload_time'] = overview_df['upload_time'].apply(
                lambda x: x.split('T')[0] if 'T' in str(x) else str(x)
            )
            components.extend([
                html.H6("üìä Document Overview"),
                dbc.Table.from_dataframe(
                    overview_df[['filename', 'node_count', 'edge_count', 'avg_confidence']], 
                    striped=True, bordered=True, hover=True, size="sm"
                )
            ])
        
        # 2. ÌïÑÎìúÎ≥Ñ Ï∞®Ïù¥Ï†ê (NEW!)
        field_differences = comparison_data.get("field_differences", [])
        if field_differences:
            components.extend([
                html.Hr(),
                html.H6("üîç Field Differences (Key Findings)"),
            ])
            
            for diff in field_differences[:5]:  # ÏÉÅÏúÑ 5Í∞ú Ï∞®Ïù¥Ï†ê
                field_name = diff['field']
                differences = diff['differences']
                
                diff_items = []
                for doc_name, value in differences.items():
                    short_name = doc_name.replace('.pdf', '')[:20]
                    diff_items.append(html.Li(f"{short_name}: {value}"))
                
                components.append(
                    dbc.Card([
                        dbc.CardBody([
                            html.H6(f"üè∑Ô∏è {field_name}", className="card-title"),
                            html.P(f"Category: {diff['category']} | Unique Values: {diff['unique_count']}", 
                                  className="text-muted small"),
                            html.Ul(diff_items, className="mb-0")
                        ])
                    ], className="mb-2")
                )
        
        # 3. ÏÉÅÏÑ∏ ÌïÑÎìú ÎπÑÍµê ÌÖåÏù¥Î∏î (NEW!)
        detailed_table = comparison_data.get("detailed_field_table", [])
        if detailed_table:
            components.extend([
                html.Hr(),
                html.H6("üìã Detailed Field Comparison Matrix"),
            ])
            
            # Ï∞®Ïù¥Í∞Ä ÏûàÎäî ÌïÑÎìúÎßå ÌëúÏãú
            different_fields = [row for row in detailed_table if row.get('Has_Differences') == 'Yes']
            
            if different_fields:
                df = pd.DataFrame(different_fields)
                # Ïª¨Îüº ÏÑ†ÌÉù (ÎÑàÎ¨¥ ÎßéÏúºÎ©¥ ÏùºÎ∂ÄÎßå)
                display_cols = ['Standard_Field', 'Field_Type', 'Has_Differences', 'Unique_Values_Count']
                value_cols = [col for col in df.columns if '_Value' in col]
                display_cols.extend(value_cols[:3])  # ÏµúÎåÄ 3Í∞ú Î¨∏ÏÑúÎßå ÌëúÏãú
                
                if len(display_cols) <= len(df.columns):
                    table = dbc.Table.from_dataframe(
                        df[display_cols], 
                        striped=True, bordered=True, hover=True, size="sm"
                    )
                    components.append(table)
                
                if len(different_fields) > 10:
                    components.append(
                        html.P(f"... and {len(different_fields) - 10} more field differences", 
                              className="text-muted")
                    )
            else:
                components.append(html.P("No field differences found.", className="text-muted"))
        
        # 4. Ïú†ÏÇ¨ÎèÑ Ï†ïÎ≥¥
        similarity = comparison_data.get("similarity", {})
        if similarity:
            components.extend([
                html.Hr(),
                html.H6("üîç Content Similarity"),
                html.P(f"Similarity Score: {similarity.get('similarity_score', 0):.2%}"),
                html.P(f"Common Entities: {len(similarity.get('common_entities', []))}")
            ])
    
    elif result_type == "comparison":
        # Í∏∞Ï°¥ Í∏∞Î≥∏ ÎπÑÍµê Í≤∞Í≥º Ï≤òÎ¶¨ (ÌïòÏúÑ Ìò∏ÌôòÏÑ±)
        comparison_data = data
        
        # Î¨∏ÏÑú Í∞úÏöî
        if comparison_data.get("overview"):
            overview_df = pd.DataFrame(comparison_data["overview"])
            overview_df['upload_time'] = overview_df['upload_time'].apply(
                lambda x: x.split('T')[0] if 'T' in str(x) else str(x)
            )
            components.extend([
                html.H6("üìä Document Overview"),
                dbc.Table.from_dataframe(
                    overview_df[['filename', 'node_count', 'edge_count', 'avg_confidence']], 
                    striped=True, bordered=True, hover=True, size="sm"
                )
            ])
        
        # ÌïÑÎìú ÎπÑÍµê
        field_comparison = comparison_data.get("field_comparison", {})
        if field_comparison:
            field_summary = []
            for field_name, doc_dict in list(field_comparison.items())[:10]:
                field_summary.append({
                    "Field": field_name,
                    "Documents": len(doc_dict),
                    "Avg Confidence": sum(doc_dict.values()) / len(doc_dict)
                })
            
            if field_summary:
                components.extend([
                    html.Hr(),
                    html.H6("üè∑Ô∏è Field Comparison (Top 10)"),
                    dbc.Table.from_dataframe(
                        pd.DataFrame(field_summary), 
                        striped=True, bordered=True, hover=True, size="sm"
                    )
                ])
        
        # Ïú†ÏÇ¨ÎèÑ Ï†ïÎ≥¥
        similarity = comparison_data.get("similarity", {})
        if similarity:
            components.extend([
                html.Hr(),
                html.H6("üîç Content Similarity"),
                html.P(f"Similarity Score: {similarity.get('similarity_score', 0):.2%}"),
                html.P(f"Common Entities: {len(similarity.get('common_entities', []))}")
            ])
    
    elif result_type == "table" and data:
        # Í∏∞Ï°¥ ÌÖåÏù¥Î∏î Ï≤òÎ¶¨
        columns = result.get("columns", [])
        if not columns and data:
            columns = list(data[0].keys()) if data else []
        
        clean_data = []
        for item in data[:20]:
            clean_item = {}
            for key, value in item.items():
                if isinstance(value, float):
                    clean_item[key] = round(value, 3)
                else:
                    clean_item[key] = str(value)[:50]
            clean_data.append(clean_item)
        
        if clean_data:
            df = pd.DataFrame(clean_data)
            table = dbc.Table.from_dataframe(
                df, striped=True, bordered=True, hover=True, responsive=True, size="sm"
            )
            components.append(table)
            
            if len(data) > 20:
                components.append(
                    html.P(f"... and {len(data) - 20} more items", className="text-muted")
                )
    
    elif result_type == "stat" and data:
        # ÌÜµÍ≥Ñ ÌëúÏãú
        stat_cards = []
        for key, value in data.items():
            stat_cards.append(
                dbc.Col([
                    dbc.Card([
                        dbc.CardBody([
                            html.H5(str(value), className="text-primary"),
                            html.P(key, className="mb-0")
                        ])
                    ], className="text-center")
                ], width=3)
            )
        
        components.append(dbc.Row(stat_cards[:4]))
        
        if len(stat_cards) > 4:
            components.append(dbc.Row(stat_cards[4:8], className="mt-2"))
    
    elif result_type == "suggestions" and data:
        components.extend([
            html.P("üí° Try these queries:", className="font-weight-bold"),
            html.Ul([html.Li(suggestion) for suggestion in data])
        ])
    
    elif result_type == "error":
        components = [dbc.Alert(f"‚ùå {message}", color="danger")]
    
    else:
        components.append(html.P("No results found."))
    
    return html.Div(components)

if __name__ == "__main__":
    print("=" * 60)
    print("üöÄ Starting Advanced Industrial Knowledge Graph Analyzer...")
    print("=" * 60)
    print("üìã System Status Check:")
    print("   üß† Enhanced ontology learning: ‚úÖ Enabled")
    print("   üíæ Database system: ‚úÖ Integrated")
    print("   üîç Natural language queries: ‚úÖ Supported")
    print("   üéØ Advanced pattern recognition: ‚úÖ Active")
    print("   üìã Multi-Document Comparison: ‚úÖ Enabled")
    
    # R-GCN ÏÉÅÌÉú ÌôïÏù∏
    if rgcn_manager:
        try:
            status = rgcn_manager.get_status()
            print(f"   ü§ñ R-GCN Model: ‚úÖ Ready ({status.get('device', 'unknown')})")
            print(f"   üîó Relation Types: {status.get('num_relations', 0)} configured")
        except:
            print("   ü§ñ R-GCN Model: ‚ö†Ô∏è Available but status check failed")
    else:
        print("   ü§ñ R-GCN Model: ‚ùå Not available")
    
    # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÉÅÌÉú ÌôïÏù∏
    try:
        db_stats = db_manager.get_database_stats()
        print(f"   üìä Database: ‚úÖ Connected ({db_stats.get('total_documents', 0)} documents)")
    except:
        print("   üìä Database: ‚ö†Ô∏è Connected but status check failed")
    
    print("=" * 60)
    print("üéâ Features Available:")
    print("   üìÑ Multi-PDF Upload & OCR Processing")
    print("   üï∏Ô∏è Knowledge Graph Visualization")
    print("   üó£Ô∏è Natural Language Queries")
    print("   ü§ñ R-GCN Neural Network Predictions")
    print("   üìä Rule-based vs Neural Comparison")
    print("   üíæ Persistent Database Storage")
    print("   üìã Advanced Document Comparison")
    print("=" * 60)
    print("üí° Document Comparison Queries:")
    print("   ‚Ä¢ 'compare documents' - Compare all uploaded documents")
    print("   ‚Ä¢ 'field differences' - Show field differences between docs")
    print("   ‚Ä¢ 'document similarity' - Analyze content similarity")
    print("   ‚Ä¢ 'list all documents' - Show all processed documents")
    print("=" * 60)
    print("üåê Server Starting...")
    print("üîó URL: http://127.0.0.1:8050")
    print("üì± Mobile friendly interface available")
    print("=" * 60)

# PDF Ï≤òÎ¶¨ Î∞è Í∑∏ÎûòÌîÑ ÏÉùÏÑ± (ÏôÑÏ†Ñ Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ)
def process_pdf_to_graph(pdf_content, filename):
    """PDF ÎÇ¥Ïö©ÏùÑ Ï≤òÎ¶¨ÌïòÏó¨ ÏôÑÏ†ÑÌïú ÏßÄÏãù Í∑∏ÎûòÌîÑ ÏÉùÏÑ±"""
    print(f"üìÑ Processing PDF with advanced ontology learning: {filename}")
    
    # Í∞ïÌôîÎêú Ïò®ÌÜ®Î°úÏßÄ ÌïôÏäµ
    learning_results = ontology_learner.learn_from_text(pdf_content)
    
    # Í∑∏ÎûòÌîÑ ÏÉùÏÑ±
    G = nx.DiGraph()
    node_counter = 0
    
    # Î¨∏ÏÑú Î£®Ìä∏ ÎÖ∏Îìú
    doc_node = f"Document_{filename.replace('.', '_')}_{node_counter}"
    G.add_node(doc_node, node_type="entity", ontology_class="Document")
    node_counter += 1
    
    # ========== 1. BASIC PATTERNS Ï≤òÎ¶¨ (ÌôïÏû•Îêú Ìå®ÌÑ¥Îì§) ==========
    patterns = learning_results.get("patterns", {})
    
    # 1-1. Revision Numbers Ï≤òÎ¶¨ (Î™®Îì† Î¶¨ÎπÑÏ†Ñ)
    if "revision_numbers" in patterns:
        for rev_num in patterns["revision_numbers"]:
            rev_node = f"Revision_{rev_num}_{node_counter}"
            G.add_node(rev_node, 
                      node_type="entity", 
                      ontology_class="Revision",
                      confidence=0.9)
            
            G.add_edge(doc_node, rev_node, relation="hasRevision")
            
            # Î¶¨ÎπÑÏ†Ñ Í∞í Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
            rev_value_node = f"RevisionValue_{rev_num}_{node_counter}"
            G.add_node(rev_value_node, 
                      node_type="literal", 
                      ontology_class="string",
                      confidence=0.9)
            
            G.add_edge(rev_node, rev_value_node, relation="hasValue")
            node_counter += 1
            print(f"‚úÖ Added Revision: {rev_num}")
    
    # 1-2. Dates Ï≤òÎ¶¨ (Î™®Îì† ÎÇ†Ïßú)
    if "dates" in patterns:
        for date_str in patterns["dates"]:
            date_node = f"Date_{date_str.replace('-', '_')}_{node_counter}"
            G.add_node(date_node, 
                      node_type="entity", 
                      ontology_class="Date",
                      confidence=0.9)
            
            G.add_edge(doc_node, date_node, relation="hasDate")
            
            # ÎÇ†Ïßú Í∞í Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
            date_value_node = f"DateValue_{date_str.replace('-', '_')}_{node_counter}"
            G.add_node(date_value_node, 
                      node_type="literal", 
                      ontology_class="date",
                      confidence=0.9)
            
            G.add_edge(date_node, date_value_node, relation="hasValue")
            node_counter += 1
            print(f"‚úÖ Added Date: {date_str}")
    
    # 1-3. Person Names Ï≤òÎ¶¨ (Î™®Îì† ÏÇ¨Îûå)
    if "person_names" in patterns:
        for person_name in patterns["person_names"]:
            person_node = f"Person_{person_name}_{node_counter}"
            G.add_node(person_node, 
                      node_type="entity", 
                      ontology_class="Person",
                      confidence=0.9)
            
            G.add_edge(doc_node, person_node, relation="reviewedBy")
            
            # ÏÇ¨Îûå Ïù¥Î¶Ñ Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
            name_value_node = f"PersonName_{person_name}_{node_counter}"
            G.add_node(name_value_node, 
                      node_type="literal", 
                      ontology_class="string",
                      confidence=0.9)
            
            G.add_edge(person_node, name_value_node, relation="hasValue")
            node_counter += 1
            print(f"‚úÖ Added Person: {person_name}")
    
    # 1-4. Equipment IDs Ï≤òÎ¶¨ (Î™®Îì† Ïû•ÎπÑ)
    if "equipment_ids" in patterns:
        for eq_id in patterns["equipment_ids"][:10]:  # ÏµúÎåÄ 10Í∞ú
            eq_node = f"Equipment_{eq_id.replace('-', '_').replace('/', '_').replace(' ', '_')}_{node_counter}"
            G.add_node(eq_node, 
                      node_type="entity", 
                      ontology_class="Equipment",
                      confidence=0.85)
            
            G.add_edge(doc_node, eq_node, relation="hasEquipment")
            
            # Ïû•ÎπÑ ID Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
            eq_value_node = f"EquipmentID_{eq_id.replace('-', '_').replace('/', '_').replace(' ', '_')}_{node_counter}"
            G.add_node(eq_value_node, 
                      node_type="literal", 
                      ontology_class="string",
                      confidence=0.85)
            
            G.add_edge(eq_node, eq_value_node, relation="hasValue")
            node_counter += 1
            print(f"‚úÖ Added Equipment: {eq_id}")
    
    # 1-5. Project IDs Ï≤òÎ¶¨
    if "project_ids" in patterns:
        for proj_id in patterns["project_ids"][:5]:  # ÏµúÎåÄ 5Í∞ú
            proj_node = f"Project_{proj_id.replace('-', '_').replace(' ', '_')}_{node_counter}"
            G.add_node(proj_node, 
                      node_type="entity", 
                      ontology_class="Project",
                      confidence=0.9)
            
            G.add_edge(doc_node, proj_node, relation="hasProject")
            
            # ÌîÑÎ°úÏ†ùÌä∏ ID Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
            proj_value_node = f"ProjectID_{proj_id.replace('-', '_').replace(' ', '_')}_{node_counter}"
            G.add_node(proj_value_node, 
                      node_type="literal", 
                      ontology_class="string",
                      confidence=0.9)
            
            G.add_edge(proj_node, proj_value_node, relation="hasValue")
            node_counter += 1
            print(f"‚úÖ Added Project: {proj_id}")
    
    # 1-6. Process Terms Ï≤òÎ¶¨ (ÌôîÌïôÍ≥µÏ†ï Ïö©Ïñ¥Îì§)
    if "process_terms" in patterns:
        for term, category in patterns["process_terms"][:15]:  # ÏµúÎåÄ 15Í∞ú
            term_node = f"ProcessTerm_{term.replace(' ', '_').replace('/', '_')}_{node_counter}"
            G.add_node(term_node, 
                      node_type="entity", 
                      ontology_class="ProcessTerm",
                      confidence=0.8)
            
            G.add_edge(doc_node, term_node, relation="hasProcessTerm")
            
            # Ïö©Ïñ¥ Í∞í Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
            term_value_node = f"TermValue_{term.replace(' ', '_').replace('/', '_')}_{node_counter}"
            G.add_node(term_value_node, 
                      node_type="literal", 
                      ontology_class="string",
                      confidence=0.8)
            
            G.add_edge(term_node, term_value_node, relation="hasValue")
            node_counter += 1
            print(f"‚úÖ Added Process Term: {term} ({category})")
    
    # 1-7. Materials Ï≤òÎ¶¨ (API Î∂ÑÎ•ò Îì±)
    if "materials" in patterns:
        for material in patterns["materials"][:5]:  # ÏµúÎåÄ 5Í∞ú
            mat_node = f"Material_{material.replace(' ', '_').replace('-', '_')}_{node_counter}"
            G.add_node(mat_node, 
                      node_type="entity", 
                      ontology_class="Material",
                      confidence=0.85)
            
            G.add_edge(doc_node, mat_node, relation="hasMaterial")
            
            # Ïû¨Î£å Í∞í Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
            mat_value_node = f"MaterialValue_{material.replace(' ', '_').replace('-', '_')}_{node_counter}"
            G.add_node(mat_value_node, 
                      node_type="literal", 
                      ontology_class="string",
                      confidence=0.85)
            
            G.add_edge(mat_node, mat_value_node, relation="hasValue")
            node_counter += 1
            print(f"‚úÖ Added Material: {material}")
    
    # ========== 2. FIELD-VALUE PAIRS Ï≤òÎ¶¨ ==========
    field_value_pairs = learning_results.get("field_value_pairs", [])
    
    for i, field_pair in enumerate(field_value_pairs[:20]):  # ÏµúÎåÄ 20Í∞ú
        field_name = field_pair.get("field_name", "Unknown")
        field_value = field_pair.get("field_value", "")
        field_number = field_pair.get("field_number", "")
        
        # ÌïÑÎìú ÏóîÌã∞Ìã∞ ÎÖ∏Îìú
        field_node = f"Field_{field_name.replace(' ', '_').replace('/', '_').replace('(', '').replace(')', '')}_{node_counter}"
        G.add_node(field_node, 
                  node_type="entity", 
                  ontology_class="Field",
                  confidence=0.85)
        
        G.add_edge(doc_node, field_node, relation="hasField")
        
        # ÌïÑÎìúÎ™Ö Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
        field_name_node = f"FieldName_{field_name.replace(' ', '_').replace('/', '_')}_{node_counter}"
        G.add_node(field_name_node, 
                  node_type="literal", 
                  ontology_class="string",
                  confidence=0.85)
        
        G.add_edge(field_node, field_name_node, relation="hasFieldName")
        
        # ÌïÑÎìúÍ∞í Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
        if field_value:
            field_value_node = f"FieldValue_{field_value.replace(' ', '_').replace('/', '_')}_{node_counter}"
            G.add_node(field_value_node, 
                      node_type="literal", 
                      ontology_class="string",
                      confidence=0.85)
            
            G.add_edge(field_node, field_value_node, relation="hasFieldValue")
        
        # ÌïÑÎìú Î≤àÌò∏Í∞Ä ÏûàÏúºÎ©¥ Ï∂îÍ∞Ä
        if field_number:
            field_num_node = f"FieldNumber_{field_number}_{node_counter}"
            G.add_node(field_num_node, 
                      node_type="literal", 
                      ontology_class="integer",
                      confidence=0.9)
            
            G.add_edge(field_node, field_num_node, relation="hasFieldNumber")
        
        node_counter += 1
        print(f"‚úÖ Added Field: {field_number} {field_name} = {field_value[:30]}...")
    
    # ========== 3. NOTES Ï≤òÎ¶¨ ==========
    notes = learning_results.get("notes", [])
    
    for note in notes[:10]:  # ÏµúÎåÄ 10Í∞ú
        note_text = note.get("note_text", "")
        note_number = note.get("note_number", "")
        note_type = note.get("type", "general_note")
        
        # NOTE ÏóîÌã∞Ìã∞ ÎÖ∏Îìú
        if note_number:
            note_node = f"Note_{note_number}_{node_counter}"
        else:
            note_node = f"Note_General_{node_counter}"
            
        G.add_node(note_node, 
                  node_type="entity", 
                  ontology_class="Note",
                  confidence=0.9)
        
        G.add_edge(doc_node, note_node, relation="hasNote")
        
        # NOTE Î≤àÌò∏ Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
        if note_number:
            note_num_node = f"NoteNumber_{note_number}_{node_counter}"
            G.add_node(note_num_node, 
                      node_type="literal", 
                      ontology_class="integer",
                      confidence=0.9)
            
            G.add_edge(note_node, note_num_node, relation="hasNoteNumber")
        
        # NOTE ÌÖçÏä§Ìä∏ Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
        if note_text:
            if note_number:
                note_text_node = f"NoteText_{note_number}_{node_counter}"
            else:
                note_text_node = f"NoteText_General_{node_counter}"
                
            G.add_node(note_text_node, 
                      node_type="literal", 
                      ontology_class="text",
                      confidence=0.9)
            
            G.add_edge(note_node, note_text_node, relation="hasNoteText")
        
        node_counter += 1
        print(f"‚úÖ Added Note: {note_number if note_number else 'General'}")

    # ========== 4. NUMERICAL VALUES Ï≤òÎ¶¨ (Îã®ÏúÑÎ≥Ñ Í∑∏Î£πÌôî) ==========
    if "numerical_values" in patterns:
        unit_groups = {}
        for value, unit, unit_type in patterns["numerical_values"]:
            if unit_type not in unit_groups:
                unit_groups[unit_type] = []
            unit_groups[unit_type].append((value, unit))
        
        for unit_type, values in unit_groups.items():
            # Îã®ÏúÑ ÌÉÄÏûÖÎ≥Ñ Í∑∏Î£π ÎÖ∏Îìú
            group_node = f"ProcessGroup_{unit_type}_{node_counter}"
            G.add_node(group_node, 
                      node_type="entity", 
                      ontology_class="ProcessRequirement",
                      confidence=0.8)
            
            G.add_edge(doc_node, group_node, relation="hasProcessReq")
            
            # Í∞úÎ≥Ñ Í∞íÎì§ Ï∂îÍ∞Ä (ÏµúÎåÄ 5Í∞ú)
            for value, unit in values[:5]:
                val_node = f"Value_{value}_{unit.replace('/', '_').replace(' ', '_')}_{node_counter}"
                G.add_node(val_node, 
                          node_type="literal", 
                          ontology_class="decimal",
                          confidence=0.9)
                
                G.add_edge(group_node, val_node, relation="hasValue")
                node_counter += 1
                print(f"‚úÖ Added {unit_type}: {value} {unit}")
    
    # ========== 5. Í∏∞Ï°¥ CONTEXTUAL ENTITIES Ï≤òÎ¶¨ (Î≥¥ÏôÑ) ==========
    matched_entities = learning_results.get("matched_entities", {})
    
    for class_name, entity_matches in matched_entities.items():
        for match_info in entity_matches[:3]:  # ÏµúÎåÄ 3Í∞úÎßå (Ï§ëÎ≥µ Î∞©ÏßÄ)
            entity = match_info["entity"]
            confidence = match_info["confidence"]
            
            # Ïù¥ÎØ∏ Ï∂îÍ∞ÄÎêú ÏóîÌã∞Ìã∞Ïù∏ÏßÄ ÌôïÏù∏ (Ï§ëÎ≥µ Î∞©ÏßÄ)
            entity_clean = entity.replace(' ', '_').replace('/', '_')
            existing_nodes = [n for n in G.nodes() if entity_clean in n]
            
            if not existing_nodes and confidence > 0.5:
                entity_node = f"{class_name}_{entity_clean}_{node_counter}"
                G.add_node(entity_node, 
                          node_type="entity", 
                          ontology_class=class_name,
                          confidence=confidence)
                
                G.add_edge(doc_node, entity_node, relation=f"has{class_name}")
                
                # ÏóîÌã∞Ìã∞ Í∞í Î¶¨ÌÑ∞Îü¥ ÎÖ∏Îìú
                value_node = f"EntityValue_{entity_clean}_{node_counter}"
                G.add_node(value_node, 
                          node_type="literal", 
                          ontology_class="string",
                          confidence=confidence)
                
                G.add_edge(entity_node, value_node, relation="hasValue")
                node_counter += 1
    
    # ========== 6. Í∑∏ÎûòÌîÑ ÏôÑÏÑ± Ï≤òÎ¶¨ ==========
    
    # ÎÖ∏Îìú ÌäπÏßï ÏÉùÏÑ±
    node_features = {}
    for node in G.nodes():
        node_data = G.nodes[node]
        node_features[node] = {
            "type": node_data.get("node_type", "entity"),
            "ontology_class": node_data.get("ontology_class", "Unknown"),
            "confidence": node_data.get("confidence", 0.5)
        }
    
    # Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò ÏòàÏ∏° ÏÉùÏÑ±
    predictions = []
    for node in G.nodes():
        node_info = node_features[node]
        confidence = node_info["confidence"]
        node_type = node_info["type"]
        
        if confidence > 0.8:
            if node_type == "entity":
                predictions.append(1)
            elif node_type == "literal":
                predictions.append(2)
            else:
                predictions.append(0)
        else:
            predictions.append(1 if node_type == "entity" else 2)
    
    print(f"‚úÖ Complete enhanced graph generated:")
    print(f"   - Total Nodes: {G.number_of_nodes()}")
    print(f"   - Total Edges: {G.number_of_edges()}")
    print(f"   - Entity Nodes: {sum(1 for n in node_features.values() if n['type'] == 'entity')}")
    print(f"   - Literal Nodes: {sum(1 for n in node_features.values() if n['type'] == 'literal')}")
    print(f"   - High Confidence: {sum(1 for n in node_features.values() if n['confidence'] > 0.8)}")
    
    return G, node_features, predictions, learning_results

# ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ìï®Ïàò
def create_sample_graph():
    G = nx.DiGraph()
    
    nodes = [
        ("Project_7T04", {"node_type": "entity", "ontology_class": "Project"}),
        ("Equipment_P2105", {"node_type": "entity", "ontology_class": "Equipment"}), 
        ("ProcessReq_Temp", {"node_type": "entity", "ontology_class": "ProcessRequirement"}),
        ("7T04", {"node_type": "literal", "ontology_class": "string"}),
        ("P-2105 A/B", {"node_type": "literal", "ontology_class": "string"}),
        ("384 ‚ÑÉ", {"node_type": "literal", "ontology_class": "decimal"})
    ]
    
    for node_id, attrs in nodes:
        G.add_node(node_id, **attrs)
    
    edges = [
        ("Project_7T04", "7T04", "hasValue"),
        ("Equipment_P2105", "P-2105 A/B", "hasValue"),
        ("ProcessReq_Temp", "384 ‚ÑÉ", "hasValue"),
        ("Project_7T04", "Equipment_P2105", "hasEquipment"),
        ("Equipment_P2105", "ProcessReq_Temp", "hasProcessReq")
    ]
    
    for src, dst, rel in edges:
        G.add_edge(src, dst, relation=rel)
    
    node_features = {}
    for node in G.nodes():
        node_data = G.nodes[node]
        node_features[node] = {
            "type": node_data.get("node_type", "entity"),
            "ontology_class": node_data.get("ontology_class", "Unknown"),
            "confidence": 0.8 if node_data.get("node_type") == "entity" else 0.9
        }
    
    predictions = []
    for node in G.nodes():
        node_type = node_features[node]["type"]
        if node_type == "entity":
            predictions.append(1)
        elif node_type == "literal":
            predictions.append(2)
        else:
            predictions.append(0)
    
    return G, node_features, predictions

class NaturalLanguageQueryProcessor:
    def __init__(self, ontology_learner, db_manager):
        self.ontology_learner = ontology_learner
        self.db_manager = db_manager
        
        # ÌôïÏû•Îêú ÏøºÎ¶¨ Ìå®ÌÑ¥ Îß§Ìïë (Î¨∏ÏÑú ÎπÑÍµê Ï∂îÍ∞Ä)
        self.query_patterns = {
            "show_all": [
                r"show\s+all\s+(\w+)",
                r"list\s+all\s+(\w+)",
                r"find\s+all\s+(\w+)",
                r"get\s+all\s+(\w+)"
            ],
            "filter_by_confidence": [
                r"(?:show|find|list)\s+(\w+)\s+with\s+(?:high|good)\s+confidence",
                r"(?:show|find|list)\s+high\s+confidence\s+(\w+)",
                r"(\w+)\s+with\s+confidence\s+>\s*(\d+\.?\d*)"
            ],
            "filter_by_class": [
                r"(?:show|find|list)\s+(\w+)\s+(?:of\s+type|class)\s+(\w+)",
                r"(\w+)\s+that\s+are\s+(\w+)",
                r"all\s+(\w+)\s+(\w+)"
            ],
            "count": [
                r"(?:how\s+many|count)\s+(\w+)",
                r"number\s+of\s+(\w+)",
                r"total\s+(\w+)"
            ],
            "stats": [
                r"(?:statistics|stats)\s+(?:for|of|about)?\s*(\w+)?",
                r"(?:summary|overview)\s+(?:of|about)?\s*(\w+)?",
                r"(?:analyze|analysis)\s+(\w+)?",
                r"database\s+(?:stats|statistics)"
            ],
            "relationships": [
                r"(?:show|find|list)\s+(?:relationships?|relations?|connections?)",
                r"what\s+is\s+connected\s+to\s+(\w+)",
                r"(\w+)\s+(?:connected|related|linked)\s+to\s+(\w+)"
            ],
            "recent": [
                r"(?:recent|latest|newest)\s+(\w+)",
                r"last\s+(\d+)\s+(\w+)",
                r"show\s+recent\s+(?:documents|files)"
            ],
            "search": [
                r"search\s+(?:for\s+)?(.+)",
                r"find\s+(.+)\s+in\s+database",
                r"lookup\s+(.+)"
            ],
            # ÏÉàÎ°úÏö¥ ÌäπÌôî Ìå®ÌÑ¥Îì§
            "field_search": [
                r"(?:show|find|list)\s+field\s+(.+)",
                r"field\s+(.+)",
                r"what\s+is\s+(.+)\s+field"
            ],
            "note_search": [
                r"(?:show|find|list)\s+note\s*(\d*)",
                r"note\s+(\d+)",
                r"notes?\s+about\s+(.+)"
            ],
            # Î¨∏ÏÑú ÎπÑÍµê Í¥ÄÎ†® Ìå®ÌÑ¥ Ï∂îÍ∞Ä
            "document_comparison": [
                r"compare\s+documents?",
                r"document\s+comparison",
                r"compare\s+all\s+documents?",
                r"comparison\s+between\s+documents?"
            ],
            "field_differences": [
                r"field\s+differences?",
                r"different\s+fields?",
                r"compare\s+fields?",
                r"field\s+comparison"
            ],
            "document_list": [
                r"list\s+(?:all\s+)?documents?",
                r"show\s+(?:all\s+)?documents?",
                r"what\s+documents?\s+do\s+you\s+have"
            ],
            "similarity": [
                r"similar(?:ity)?\s+(?:between\s+)?documents?",
                r"how\s+similar",
                r"common\s+(?:elements?|entities?|fields?)"
            ]
        }
        
        # ÎåÄÌè≠ ÌôïÏû•Îêú ÎèôÏùòÏñ¥ Îß§Ìïë (ÌôîÌïôÍ≥µÏ†ï ÌäπÌôî)
        self.synonyms = {
            # Í∏∞Î≥∏ Ïû•ÎπÑ Í¥ÄÎ†®
            "equipment": ["equipment", "machine", "device", "pump", "motor", "driver", "vessel", "tank", "compressor"],
            "project": ["project", "job", "document", "doc", "specification", "drawing"],
            "requirement": ["requirement", "spec", "specification", "parameter", "condition"],
            "person": ["person", "people", "engineer", "reviewer", "checker", "by", "checked", "reviewed"],
            "entity": ["entity", "entities", "item", "object", "node"],
            "literal": ["literal", "value", "data", "text"],
            "confidence": ["confidence", "certainty", "reliability", "accuracy"],
            "documents": ["documents", "files", "pdfs", "papers"],
            
            # ÌîÑÎ°úÏÑ∏Ïä§ Í¥ÄÎ†® (ÎåÄÌè≠ ÌôïÏû•)
            "temperature": ["temperature", "temp", "pumping temperature", "operating temperature", "minimum temperature", "maximum temperature"],
            "pressure": ["pressure", "suction pressure", "discharge pressure", "differential pressure", "vapor pressure"],
            "capacity": ["capacity", "flow", "flowrate", "flow rate", "rated capacity", "normal capacity"],
            "viscosity": ["viscosity", "fluid viscosity", "liquid viscosity"],
            "density": ["density", "specific gravity", "fluid density"],
            "head": ["head", "differential head", "pump head"],
            "npsh": ["npsh", "npsha", "net positive suction head", "available npsh"],
            
            # Ïû¨Î£å/Î∂ÑÎ•ò
            "material": ["material", "materials", "api class", "classification", "casing", "impeller", "shaft"],
            "api": ["api", "api class", "api classification", "american petroleum institute"],
            
            # Ïö¥Ï†Ñ Ï°∞Í±¥
            "duty": ["duty", "operation", "operating", "continuous", "intermittent"],
            "startup": ["startup", "start-up", "start up", "starting", "initial condition"],
            "minimum": ["minimum", "min", "lowest", "bottom"],
            "maximum": ["maximum", "max", "highest", "top"],
            "normal": ["normal", "standard", "typical", "operating"],
            "rated": ["rated", "design", "nominal"],
            
            # ÌôîÌïôÍ≥µÏ†ï Ïú†Ï≤¥
            "fluid": ["fluid", "liquid", "gas oil", "overflash", "ah feed", "am feed"],
            "overflash": ["overflash", "over flash", "overhead"],
            "feed": ["feed", "ah feed", "am feed", "feedstock"],
            
            # ÏïàÏ†Ñ/ÏúÑÌóò
            "flammable": ["flammable", "combustible", "fire hazard"],
            "toxic": ["toxic", "poisonous", "hazardous"],
            "sulfur": ["sulfur", "sulphur", "h2s", "hydrogen sulfide"],
            
            # ÏÑ§Ïπò/ÏúÑÏπò
            "location": ["location", "indoor", "outdoor", "under roof"],
            "insulation": ["insulation", "steam tracing", "steam jacket", "heating"],
            
            # Ï†úÏñ¥/Ï°∞Ïûë
            "manual": ["manual", "hand operated", "manually operated"],
            "automatic": ["automatic", "auto", "automatically operated"],
            
            # NOTE Í¥ÄÎ†® ÌÇ§ÏõåÎìú
            "note": ["note", "notes", "remark", "comment", "description"],
            "foundation": ["foundation", "base", "mounting", "support"],
            "turndown": ["turndown", "turn down", "reduced operation", "minimum operation"],
            "overdesign": ["overdesign", "over design", "safety margin", "design margin"],
            "mdmt": ["mdmt", "minimum design metal temperature", "minimum temperature"],
            
            # ÌïÑÎìú Í¥ÄÎ†®
            "field": ["field", "parameter", "specification", "data", "information"],
            "type": ["type", "classification", "category", "kind"],
            "required": ["required", "specification", "requirement", "needed"]
        }
        
        # ÏïΩÏñ¥/Ï†ÑÏ≤¥Î™Ö Îß§Ìïë
        self.abbreviation_mapping = {
            "npsh": "net positive suction head",
            "npsha": "npsh available",
            "mdmt": "minimum design metal temperature",
            "api": "american petroleum institute",
            "h2s": "hydrogen sulfide",
            "cp": "centipoise",
            "gpm": "gallons per minute",
            "bph": "barrels per hour",
            "psi": "pounds per square inch",
            "deg": "degree",
            "wt": "weight",
            "pt": "point",
            "max": "maximum",
            "min": "minimum",
            "nor": "normal",
            "oper": "operating",
            "temp": "temperature"
        }
        
        # Ïª®ÌÖçÏä§Ìä∏Î≥Ñ ÌÇ§ÏõåÎìú Îß§Ìïë
        self.context_keywords = {
            "temperature_related": ["pumping", "operating", "minimum", "maximum", "design", "metal"],
            "pressure_related": ["suction", "discharge", "differential", "vapor", "rated"],
            "flow_related": ["capacity", "rate", "continuous", "minimum", "maximum", "rated", "normal"],
            "material_related": ["casing", "impeller", "shaft", "api", "class", "classification"],
            "note_related": ["npsha", "foundation", "turndown", "overdesign", "mdmt", "startup", "slop"],
            "safety_related": ["flammable", "toxic", "h2s", "sulfur", "leakage", "hazard"]
        }
    
    def process_query(self, query, graph_data=None):
        """ÏûêÏó∞Ïñ¥ ÏøºÎ¶¨Î•º Ï≤òÎ¶¨ÌïòÏó¨ Í≤∞Í≥º Î∞òÌôò (Î¨∏ÏÑú ÎπÑÍµê Í∏∞Îä• Ï∂îÍ∞Ä)"""
        query = query.lower().strip()
        print(f"üîç Processing enhanced query with document comparison: '{query}'")
        
        # 1. ÏøºÎ¶¨ Ï†ÑÏ≤òÎ¶¨ (ÏïΩÏñ¥ ÌôïÏû•, ÎèôÏùòÏñ¥ Ï†ïÍ∑úÌôî)
        processed_query = self._preprocess_query(query)
        print(f"üîÑ Processed query: '{processed_query}'")
        
        # 2. ÏøºÎ¶¨ ÌÉÄÏûÖ ÏãùÎ≥Ñ
        query_type, matches = self._identify_query_type(processed_query)
        print(f"üéØ Query type: {query_type}, matches: {matches}")
        
        # 3. Î¨∏ÏÑú ÎπÑÍµê Í¥ÄÎ†® ÏøºÎ¶¨Îì§ (ÏÉàÎ°ú Ï∂îÍ∞Ä)
        if query_type == "document_comparison":
            return self._handle_document_comparison()
        elif query_type == "field_differences":
            return self._handle_field_differences()
        elif query_type == "document_list":
            return self._handle_document_list()
        elif query_type == "similarity":
            return self._handle_similarity_analysis()
        
        # 4. Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïö∞ÏÑ† ÏøºÎ¶¨Îì§
        if query_type in ["stats", "recent"]:
            return self._handle_database_query(query_type, matches, graph_data, processed_query)
        
        # 5. ÏÉàÎ°úÏö¥ ÌäπÌôî ÏøºÎ¶¨Îì§
        if query_type == "field_search":
            return self._handle_field_search(matches, graph_data)
        elif query_type == "note_search":
            return self._handle_note_search(matches, graph_data)
        
        # 6. Î©îÎ™®Î¶¨ Îç∞Ïù¥ÌÑ∞ ÏøºÎ¶¨Îì§ (Í∏∞Ï°¥ + Í∞úÏÑ†)
        if graph_data:
            nodes = graph_data.get("nodes", [])
            edges = graph_data.get("edges", [])
            stats = graph_data.get("stats", {})
            learning_results = graph_data.get("learning_results", {})
            
            if query_type == "show_all":
                return self._handle_show_all(matches, nodes, edges)
            elif query_type == "filter_by_confidence":
                return self._handle_confidence_filter(matches, nodes)
            elif query_type == "filter_by_class":
                return self._handle_class_filter(matches, nodes)
            elif query_type == "count":
                return self._handle_count(matches, nodes)
            elif query_type == "relationships":
                return self._handle_relationships(matches, nodes, edges)
        
        # 7. Fallback: Ìñ•ÏÉÅÎêú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í≤ÄÏÉâ
        return self._handle_enhanced_database_fallback(processed_query, query)
    
    # Î¨∏ÏÑú ÎπÑÍµê Í¥ÄÎ†® ÏÉàÎ°úÏö¥ Î©îÏÑúÎìúÎì§
    def _handle_document_comparison(self):
        """Î¨∏ÏÑú ÎπÑÍµê Ï≤òÎ¶¨ (ÌïÑÎìúÎ≥Ñ ÏÉÅÏÑ∏ ÎπÑÍµê Ìè¨Ìï®)"""
        try:
            # Í∏∞Î≥∏ ÎπÑÍµê
            basic_comparison = comparison_manager.compare_documents()
            
            if "error" in basic_comparison:
                return {"type": "error", "message": basic_comparison["error"]}
            
            # ÌïÑÎìúÎ≥Ñ ÏÉÅÏÑ∏ ÎπÑÍµê Ï∂îÍ∞Ä
            detailed_comparison = comparison_manager.compare_field_details()
            
            if "error" in detailed_comparison:
                print(f"‚ö†Ô∏è Detailed comparison warning: {detailed_comparison['error']}")
                detailed_comparison = {}
            
            # ÌÜµÌï© Í≤∞Í≥º
            combined_results = {
                "overview": basic_comparison.get("document_overview", []),
                "field_comparison": basic_comparison.get("field_comparison", {}),
                "entity_comparison": basic_comparison.get("entity_comparison", []),
                "statistics": basic_comparison.get("statistics_comparison", []),
                "similarity": basic_comparison.get("content_similarity", {}),
                # ÏÉàÎ°úÏö¥ ÏÉÅÏÑ∏ ÎπÑÍµê Í≤∞Í≥º
                "detailed_field_table": detailed_comparison.get("detailed_table", []),
                "field_differences": detailed_comparison.get("differences", []),
                "field_matrix": detailed_comparison.get("field_matrix", {}),
                "document_info": detailed_comparison.get("document_info", {})
            }
            
            overview = combined_results.get("overview", [])
            if overview:
                return {
                    "type": "detailed_comparison",
                    "data": combined_results,
                    "message": f"Detailed document comparison completed for {len(overview)} documents"
                }
            else:
                return {"type": "error", "message": "No documents available for comparison"}
                
        except Exception as e:
            return {"type": "error", "message": f"Document comparison failed: {str(e)}"}
    
    def _handle_field_differences(self):
        """ÌïÑÎìú Ï∞®Ïù¥Ï†ê Î∂ÑÏÑù"""
        try:
            comparison_results = comparison_manager.compare_documents()
            field_comparison = comparison_results.get("field_comparison", {})
            
            if not field_comparison:
                return {"type": "error", "message": "No field data available for comparison"}
            
            # ÌïÑÎìú Ï∞®Ïù¥Ï†ê Î∂ÑÏÑù
            field_differences = []
            for field_name, doc_dict in field_comparison.items():
                docs_with_field = list(doc_dict.keys())
                if len(docs_with_field) > 1:
                    confidences = list(doc_dict.values())
                    field_differences.append({
                        "field": field_name,
                        "documents": len(docs_with_field),
                        "avg_confidence": sum(confidences) / len(confidences),
                        "confidence_range": f"{min(confidences):.2f} - {max(confidences):.2f}"
                    })
            
            return {
                "type": "table",
                "data": field_differences,
                "message": f"Field comparison: {len(field_differences)} common fields found",
                "columns": ["field", "documents", "avg_confidence", "confidence_range"]
            }
            
        except Exception as e:
            return {"type": "error", "message": f"Field comparison failed: {str(e)}"}
    
    def _handle_document_list(self):
        """Î¨∏ÏÑú Î™©Î°ù ÌëúÏãú"""
        try:
            docs_df = comparison_manager.get_all_documents()
            
            if docs_df.empty:
                return {"type": "error", "message": "No documents found"}
            
            # ÎÇ†Ïßú Ìè¨Îß∑ÌåÖ
            docs_data = docs_df.to_dict('records')
            for doc in docs_data:
                if 'upload_time' in doc and doc['upload_time']:
                    doc['upload_time'] = doc['upload_time'].split('T')[0]
            
            return {
                "type": "table",
                "data": docs_data,
                "message": f"Found {len(docs_data)} documents in database",
                "columns": ["id", "filename", "upload_time", "content_length", "processing_time"]
            }
            
        except Exception as e:
            return {"type": "error", "message": f"Failed to get document list: {str(e)}"}
    
    def _handle_similarity_analysis(self):
        """Ïú†ÏÇ¨ÎèÑ Î∂ÑÏÑù"""
        try:
            comparison_results = comparison_manager.compare_documents()
            similarity = comparison_results.get("content_similarity", {})
            
            common_entities = similarity.get("common_entities", [])
            similarity_score = similarity.get("similarity_score", 0)
            
            if not common_entities:
                return {"type": "error", "message": "No common entities found between documents"}
            
            return {
                "type": "table",
                "data": common_entities[:10],  # ÏÉÅÏúÑ 10Í∞úÎßå
                "message": f"Similarity analysis: {similarity_score:.2%} similarity, {len(common_entities)} common entities",
                "columns": ["id", "doc_count", "avg_confidence"]
            }
            
        except Exception as e:
            return {"type": "error", "message": f"Similarity analysis failed: {str(e)}"}
    
    # Í∏∞Ï°¥ Î©îÏÑúÎìúÎì§ Ïú†ÏßÄ (ÏÉùÎûµ...)
    def _preprocess_query(self, query):
        """ÏøºÎ¶¨ Ï†ÑÏ≤òÎ¶¨: ÏïΩÏñ¥ ÌôïÏû• Î∞è ÎèôÏùòÏñ¥ Ï†ïÍ∑úÌôî"""
        processed = query
        
        # 1. ÏïΩÏñ¥ ÌôïÏû•
        for abbr, full_form in self.abbreviation_mapping.items():
            processed = re.sub(rf'\b{re.escape(abbr)}\b', full_form, processed, flags=re.IGNORECASE)
        
        # 2. ÌäπÏàò Î¨∏Ïûê Ï†ïÎ¶¨
        processed = re.sub(r'[^\w\s-]', ' ', processed)
        processed = re.sub(r'\s+', ' ', processed).strip()
        
        return processed
    
    def _identify_query_type(self, query):
        """ÏøºÎ¶¨ ÌÉÄÏûÖÍ≥º Îß§Ïπ≠Îêú Í∑∏Î£π ÏãùÎ≥Ñ"""
        for query_type, patterns in self.query_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, query, re.IGNORECASE)
                if match:
                    return query_type, match.groups()
        return "unknown", ()
    
    def _normalize_term(self, term):
        """Ïö©Ïñ¥ Ï†ïÍ∑úÌôî (ÎèôÏùòÏñ¥ Ï≤òÎ¶¨)"""
        if not term:
            return term
            
        term = term.lower()
        for canonical, synonyms in self.synonyms.items():
            if term in synonyms:
                return canonical
        return term
    
    def _handle_database_query(self, query_type, matches, graph_data, processed_query):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í∏∞Î∞ò ÏøºÎ¶¨ Ï≤òÎ¶¨ (Í≤ÄÏÉâ Í∞ïÌôî)"""
        try:
            if query_type == "stats":
                db_stats = self.db_manager.get_database_stats()
                stats_data = {
                    "Total Documents": db_stats.get("total_documents", 0),
                    "Total Nodes": db_stats.get("total_nodes", 0),
                    "Total Edges": db_stats.get("total_edges", 0),
                    "Total Patterns": db_stats.get("total_patterns", 0),
                    "High Confidence Entities": db_stats.get("high_confidence_entities", 0)
                }
                return {
                    "type": "stat",
                    "data": stats_data,
                    "message": "Database statistics across all processed documents"
                }
            
            elif query_type == "recent":
                recent_query = """
                    SELECT filename, upload_time, content_length, processing_time
                    FROM documents 
                    ORDER BY upload_time DESC 
                    LIMIT 10
                """
                result_df = self.db_manager.execute_query(recent_query)
                
                if not result_df.empty:
                    result_df['upload_time'] = result_df['upload_time'].apply(
                        lambda x: x.split('T')[0] if 'T' in str(x) else str(x)
                    )
                    return {
                        "type": "table",
                        "data": result_df.to_dict('records'),
                        "message": f"Found {len(result_df)} recent documents",
                        "columns": ["filename", "upload_time", "content_length", "processing_time"]
                    }
                else:
                    return {"type": "table", "data": [], "message": "No documents found in database"}
            
            elif query_type == "search":
                search_term = matches[0] if matches else ""
                return self._enhanced_database_search(search_term, processed_query)
        
        except Exception as e:
            print(f"‚ùå Enhanced fallback error: {e}")
            return {"type": "error", "message": "Enhanced search failed. Try simpler terms."}
    
    # ÎÇòÎ®∏ÏßÄ Î™®Îì† Í∏∞Ï°¥ Î©îÏÑúÎìúÎì§ÎèÑ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ...
    def _enhanced_database_search(self, search_term, processed_query):
        """Ìñ•ÏÉÅÎêú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í≤ÄÏÉâ (Îã§Ï§ë ÌÇ§ÏõåÎìú, Ïª®ÌÖçÏä§Ìä∏ Í≥†Î†§)"""
        try:
            # Í≤ÄÏÉâÏñ¥ÏóêÏÑú ÏùòÎØ∏ÏûàÎäî ÌÇ§ÏõåÎìúÎì§ Ï∂îÏ∂ú
            keywords = self._extract_meaningful_keywords(search_term, processed_query)
            print(f"üîç Extracted keywords: {keywords}")
            
            if not keywords:
                return {"type": "suggestions", "data": ["temperature", "pressure", "capacity", "npsha", "startup"], 
                       "message": "Try searching for specific terms like:"}
            
            # Îã§Ï§ë ÌÇ§ÏõåÎìú Í≤ÄÏÉâ Ï°∞Í±¥ ÏÉùÏÑ±
            search_conditions = []
            for keyword in keywords[:5]:  # ÏµúÎåÄ 5Í∞ú ÌÇ§ÏõåÎìú
                search_conditions.extend([
                    f"LOWER(n.id) LIKE '%{keyword}%'",
                    f"LOWER(n.ontology_class) LIKE '%{keyword}%'",
                    f"LOWER(d.filename) LIKE '%{keyword}%'"
                ])
            
            search_query = f"""
                SELECT DISTINCT n.id, n.node_type, n.ontology_class, n.confidence, d.filename
                FROM nodes n
                JOIN documents d ON n.document_id = d.id
                WHERE {' OR '.join(search_conditions)}
                ORDER BY n.confidence DESC, n.ontology_class
                LIMIT 25
            """
            
            result_df = self.db_manager.execute_query(search_query)
            
            return {
                "type": "table",
                "data": result_df.to_dict('records') if not result_df.empty else [],
                "message": f"Found {len(result_df)} items matching '{search_term}' (keywords: {', '.join(keywords)})",
                "columns": ["id", "node_type", "ontology_class", "confidence", "filename"]
            }
            
        except Exception as e:
            print(f"‚ùå Enhanced search error: {e}")
            return {"type": "error", "message": f"Enhanced search failed: {str(e)}"}
    
    def _extract_meaningful_keywords(self, search_term, processed_query):
        """Í≤ÄÏÉâÏñ¥ÏóêÏÑú ÏùòÎØ∏ÏûàÎäî ÌÇ§ÏõåÎìúÎì§ Ï∂îÏ∂ú"""
        keywords = set()
        
        # 1. ÏõêÎ≥∏ Í≤ÄÏÉâÏñ¥ Î∂ÑÌï†
        original_words = search_term.split()
        keywords.update([w for w in original_words if len(w) > 2])
        
        # 2. ÎèôÏùòÏñ¥ Îß§ÌïëÏóêÏÑú Í¥ÄÎ†® ÌÇ§ÏõåÎìú Ï∞æÍ∏∞
        for canonical, synonyms in self.synonyms.items():
            if any(word in search_term for word in synonyms):
                keywords.add(canonical)
                keywords.update([s for s in synonyms if len(s) > 2])
        
        # 3. Ïª®ÌÖçÏä§Ìä∏ ÌÇ§ÏõåÎìú Ï∂îÍ∞Ä
        for context, context_keywords in self.context_keywords.items():
            if any(keyword in search_term for keyword in context_keywords):
                keywords.update(context_keywords)
        
        # 4. Î∂àÏö©Ïñ¥ Ï†úÍ±∞
        stop_words = {"for", "the", "and", "or", "in", "on", "at", "to", "from", "with", "by"}
        keywords = keywords - stop_words
        
        return list(keywords)
    
    def _handle_field_search(self, matches, graph_data):
        """ÌïÑÎìú Í≤ÄÏÉâ Ï≤òÎ¶¨"""
        if not graph_data or not matches:
            return {"type": "error", "message": "No field search term provided"}
        
        search_term = matches[0].lower()
        nodes = graph_data.get("nodes", [])
        
        # ÌïÑÎìú Í¥ÄÎ†® ÎÖ∏ÎìúÎì§ Ï∞æÍ∏∞
        field_nodes = [n for n in nodes if n.get("ontology_class") == "Field" and 
                      search_term in n.get("id", "").lower()]
        
        return {
            "type": "table",
            "data": field_nodes,
            "message": f"Found {len(field_nodes)} fields matching '{search_term}'",
            "columns": ["id", "ontology_class", "confidence"]
        }
    
    def _handle_note_search(self, matches, graph_data):
        """NOTE Í≤ÄÏÉâ Ï≤òÎ¶¨"""
        if not graph_data:
            return {"type": "error", "message": "No graph data available"}
        
        nodes = graph_data.get("nodes", [])
        
        if matches and matches[0].isdigit():
            # ÌäπÏ†ï NOTE Î≤àÌò∏ Í≤ÄÏÉâ
            note_num = matches[0]
            note_nodes = [n for n in nodes if n.get("ontology_class") == "Note" and 
                         note_num in n.get("id", "")]
            message = f"Found NOTE {note_num}"
        else:
            # Î™®Îì† NOTE Í≤ÄÏÉâ
            note_nodes = [n for n in nodes if n.get("ontology_class") == "Note"]
            message = f"Found {len(note_nodes)} notes"
        
        return {
            "type": "table",
            "data": note_nodes,
            "message": message,
            "columns": ["id", "ontology_class", "confidence"]
        }
    
    def _handle_show_all(self, matches, nodes, edges):
        """'show all X' ÌÉÄÏûÖ ÏøºÎ¶¨ Ï≤òÎ¶¨ (Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ)"""
        if not matches:
            return {"type": "table", "data": nodes[:10], "message": "Showing all nodes (limited to 10)"}
        
        target = self._normalize_term(matches[0])
        
        if target in ["entity", "entities"]:
            entities = [n for n in nodes if n.get("type") == "entity"]
            return {
                "type": "table", 
                "data": entities,
                "message": f"Found {len(entities)} entities",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in ["literal", "literals", "value", "values"]:
            literals = [n for n in nodes if n.get("type") == "literal"]
            return {
                "type": "table", 
                "data": literals,
                "message": f"Found {len(literals)} literals",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in ["field", "fields"]:
            fields = [n for n in nodes if n.get("ontology_class") == "Field"]
            return {
                "type": "table", 
                "data": fields,
                "message": f"Found {len(fields)} fields",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in ["note", "notes"]:
            notes = [n for n in nodes if n.get("ontology_class") == "Note"]
            return {
                "type": "table", 
                "data": notes,
                "message": f"Found {len(notes)} notes",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in self.synonyms.get("equipment", []):
            equipment = [n for n in nodes if n.get("ontology_class", "").lower() == "equipment"]
            return {
                "type": "table", 
                "data": equipment,
                "message": f"Found {len(equipment)} equipment entities",
                "columns": ["id", "confidence"]
            }
        elif target in self.synonyms.get("project", []):
            projects = [n for n in nodes if n.get("ontology_class", "").lower() == "project"]
            return {
                "type": "table", 
                "data": projects,
                "message": f"Found {len(projects)} project entities",
                "columns": ["id", "confidence"]
            }
        else:
            # ÏùºÎ∞òÏ†ÅÏù∏ Í≤ÄÏÉâ
            filtered = [n for n in nodes if target in n.get("id", "").lower() or 
                       target in n.get("ontology_class", "").lower()]
            return {
                "type": "table", 
                "data": filtered,
                "message": f"Found {len(filtered)} items matching '{target}'",
                "columns": ["id", "type", "ontology_class", "confidence"]
            }
    
    def _handle_confidence_filter(self, matches, nodes):
        """Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ (Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ)"""
        if len(matches) >= 2 and matches[1].replace('.', '').isdigit():
            # ÌäπÏ†ï ÏûÑÍ≥ÑÍ∞í ÏßÄÏ†ïÎêú Í≤ΩÏö∞
            threshold = float(matches[1])
            target = self._normalize_term(matches[0])
        else:
            # 'high confidence' Îì±Ïùò Í≤ΩÏö∞
            threshold = 0.8
            target = self._normalize_term(matches[0]) if matches else "entity"
        
        filtered_nodes = []
        for node in nodes:
            confidence = node.get("confidence", 0)
            if confidence > threshold:
                if target == "entity" and node.get("type") == "entity":
                    filtered_nodes.append(node)
                elif target in node.get("ontology_class", "").lower():
                    filtered_nodes.append(node)
                elif target in ["all", "any", ""]:
                    filtered_nodes.append(node)
        
        return {
            "type": "table",
            "data": filtered_nodes,
            "message": f"Found {len(filtered_nodes)} items with confidence > {threshold}",
            "columns": ["id", "type", "ontology_class", "confidence"]
        }
    
    def _handle_class_filter(self, matches, nodes):
        """ÌÅ¥ÎûòÏä§/ÌÉÄÏûÖ Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ (Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ)"""
        if len(matches) >= 2:
            item_type = self._normalize_term(matches[0])
            class_name = self._normalize_term(matches[1])
            
            filtered = [n for n in nodes if 
                       class_name in n.get("ontology_class", "").lower() or
                       class_name in n.get("type", "").lower()]
            
            return {
                "type": "table",
                "data": filtered,
                "message": f"Found {len(filtered)} {item_type} of type {class_name}",
                "columns": ["id", "type", "ontology_class", "confidence"]
            }
        
        return {"type": "error", "message": "Could not parse class filter query"}
    
    def _handle_count(self, matches, nodes):
        """Í∞úÏàò ÏÑ∏Í∏∞ (ÌôïÏû•Îêú Î≤ÑÏ†Ñ)"""
        if not matches:
            total = len(nodes)
            return {
                "type": "stat",
                "data": {"Total Nodes": total},
                "message": f"Total count: {total}"
            }
        
        target = self._normalize_term(matches[0])
        
        if target in ["entity", "entities"]:
            count = sum(1 for n in nodes if n.get("type") == "entity")
        elif target in ["literal", "literals"]:
            count = sum(1 for n in nodes if n.get("type") == "literal")
        elif target in ["field", "fields"]:
            count = sum(1 for n in nodes if n.get("ontology_class") == "Field")
        elif target in ["note", "notes"]:
            count = sum(1 for n in nodes if n.get("ontology_class") == "Note")
        elif target in self.synonyms.get("equipment", []):
            count = sum(1 for n in nodes if n.get("ontology_class", "").lower() == "equipment")
        elif target in self.synonyms.get("project", []):
            count = sum(1 for n in nodes if n.get("ontology_class", "").lower() == "project")
        else:
            count = sum(1 for n in nodes if target in n.get("ontology_class", "").lower())
        
        return {
            "type": "stat",
            "data": {f"{target.title()} Count": count},
            "message": f"Count of {target}: {count}"
        }
    
    def _handle_relationships(self, matches, nodes, edges):
        """Í¥ÄÍ≥Ñ Ï†ïÎ≥¥ Ï≤òÎ¶¨ (ÌôïÏû•Îêú Î≤ÑÏ†Ñ)"""
        if not edges:
            return {
                "type": "table",
                "data": [],
                "message": "No relationships found in the current graph"
            }
        
        # Í¥ÄÍ≥Ñ ÌÜµÍ≥Ñ
        relation_counts = {}
        for edge in edges:
            rel = edge.get("relation", "unknown")
            relation_counts[rel] = relation_counts.get(rel, 0) + 1
        
        relationship_data = [
            {"Relationship Type": rel, "Count": count}
            for rel, count in relation_counts.items()
        ]
        
        return {
            "type": "table",
            "data": relationship_data,
            "message": f"Found {len(edges)} relationships of {len(relation_counts)} types",
            "columns": ["Relationship Type", "Count"]
        }
    
    def _handle_enhanced_database_fallback(self, processed_query, original_query):
        """Ìñ•ÏÉÅÎêú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ fallback Í≤ÄÏÉâ"""
        try:
            # Î©îÎ™®Î¶¨ Îç∞Ïù¥ÌÑ∞ Î®ºÏ†Ä Í≤ÄÏÉâ
            if hasattr(self, '_last_graph_data') and self._last_graph_data:
                memory_result = self._search_memory_data(original_query, self._last_graph_data)
                if memory_result and memory_result.get('data'):
                    return memory_result

            keywords = self._extract_meaningful_keywords(original_query, processed_query)
            
            if not keywords:
                return {
                    "type": "suggestions",
                    "data": [
                        "temperature ‚Üí pumping temperature fields",
                        "pressure ‚Üí suction/discharge pressure",
                        "capacity ‚Üí flow rate information", 
                        "npsha ‚Üí foundation notes",
                        "startup ‚Üí start-up conditions",
                        "api ‚Üí material classifications",
                        "compare documents ‚Üí document comparison",
                        "field differences ‚Üí field comparison analysis"
                    ],
                    "message": "Try these enhanced search examples:"
                }
            
            # Ïª®ÌÖçÏä§Ìä∏ Í∏∞Î∞ò ÌÇ§ÏõåÎìú ÌôïÏû•
            expanded_keywords = set(keywords)
            for keyword in keywords:
                for context, context_keywords in self.context_keywords.items():
                    if keyword in context_keywords:
                        expanded_keywords.update(context_keywords[:3])  # ÏµúÎåÄ 3Í∞ú Ï∂îÍ∞Ä
            
            keyword_conditions = " OR ".join([
                f"LOWER(n.id) LIKE '%{kw}%' OR LOWER(n.ontology_class) LIKE '%{kw}%'"
                for kw in list(expanded_keywords)[:8]  # ÏµúÎåÄ 8Í∞ú ÌÇ§ÏõåÎìú
            ])
            
            search_query = f"""
                SELECT n.id, n.node_type, n.ontology_class, n.confidence, d.filename
                FROM nodes n
                JOIN documents d ON n.document_id = d.id
                WHERE {keyword_conditions}
                ORDER BY n.confidence DESC, 
                    CASE n.ontology_class 
                        WHEN 'Field' THEN 1 
                        WHEN 'Note' THEN 2 
                        WHEN 'ProcessRequirement' THEN 3 
                        ELSE 4 
                    END
                LIMIT 20
            """
            
            result_df = self.db_manager.execute_query(search_query)
            
            if not result_df.empty:
                return {
                    "type": "table",
                    "data": result_df.to_dict('records'),
                    "message": f"Enhanced search found {len(result_df)} items (expanded from: {', '.join(keywords)})",
                    "columns": ["id", "node_type", "ontology_class", "confidence", "filename"]
                }
            else:
                return {
                    "type": "suggestions",
                    "data": [
                        "Try 'database statistics' to see available data",
                        "Try 'show recent documents' to see processed files",
                        "Try 'compare documents' for document comparison",
                        "Try specific terms like 'temperature', 'pressure', 'pump'"
                    ],
                    "message": "No matches found. Suggestions:"
                }
        
        except Exception as e:
            print(f"‚ùå Enhanced database fallback error: {e}")
            return {
                "type": "error",
                "message": f"Enhanced database search failed: {str(e)}"
            }
    
    def _search_memory_data(self, query, graph_data):
        """Î©îÎ™®Î¶¨ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏßÅÏ†ë Í≤ÄÏÉâ"""
        if not graph_data or 'nodes' not in graph_data:
            return None
        
        nodes = graph_data['nodes']
        query_lower = query.lower()
        
        # Î™®Îì† ÎÖ∏ÎìúÏóêÏÑú Í≤ÄÏÉâ (Entity + Literal)
        matches = [node for node in nodes 
                  if query_lower in node.get('id', '').lower()]
        
        return {
            "type": "table",
            "data": matches,
            "message": f"Found {len(matches)} items in memory matching '{query}'",
            "columns": ["id", "type", "ontology_class", "confidence"]
        }

class EnhancedDocumentComparisonManager(DocumentComparisonManager):
    def __init__(self, db_manager):
        super().__init__(db_manager)
        
        # ÌëúÏ§Ä ÌïÑÎìú Îß§Ìïë ÌÖåÏù¥Î∏î
        self.standard_field_mapping = {
            # Ïò®ÎèÑ Í¥ÄÎ†®
            "temperature": ["PUMPING TEMPERATURE", "TEMPERATURE", "TEMP", "OPERATING TEMPERATURE", "MAX TEMP", "MIN TEMP"],
            "pressure_suction": ["SUCTION PRESSURE", "SUCTION", "INLET PRESSURE"],
            "pressure_discharge": ["DISCHARGE PRESSURE", "DISCHARGE", "OUTLET PRESSURE"], 
            "capacity": ["CAPACITY", "FLOW RATE", "RATED CAPACITY", "NORMAL CAPACITY"],
            "viscosity": ["VISCOSITY", "FLUID VISCOSITY"],
            "specific_gravity": ["SPECIFIC GRAVITY", "DENSITY", "SP.GR"],
            
            # Ïû•ÎπÑ Ï†ïÎ≥¥
            "pump_type": ["PUMP TYPE", "TYPE", "EQUIPMENT TYPE"],
            "driver_type": ["DRIVER TYPE", "DRIVER", "MOTOR TYPE"],
            "service": ["SERVICE", "LIQUID NAME", "FLUID"],
            "duty": ["DUTY", "OPERATION", "OPERATING CASE"],
            
            # ÌîÑÎ°úÏ†ùÌä∏ Ï†ïÎ≥¥  
            "job_no": ["JOB NO", "PROJECT", "JOB NUMBER"],
            "item_no": ["ITEM NO", "EQUIPMENT NO", "TAG NO"],
            "doc_no": ["DOC NO", "DOCUMENT NO", "DRAWING NO"],
            "revision": ["REVISION", "REV", "REV NO"],
            
            # ÏïïÎ†• ÏÑ∏Î∂ÄÏÇ¨Ìï≠
            "vapor_pressure": ["VAPOR PRESSURE", "VP"],
            "design_pressure": ["DESIGN PRESSURE", "DESIGN PRESS"],
            "operating_pressure": ["OPERATING PRESSURE", "OPER PRESS"],
            
            # Í∏∞ÌÉÄ
            "notes": ["NOTES", "REMARKS", "COMMENTS"],
            "location": ["LOCATION", "SITE", "PLANT"],
            "client": ["CLIENT", "CUSTOMER", "OWNER"]
        }
        
        # Îã®ÏúÑ Ï†ïÍ∑úÌôî Îß§Ìïë
        self.unit_normalization = {
            "temperature": {"‚ÑÉ": "¬∞C", "DEG C": "¬∞C", "DEGREE C": "¬∞C"},
            "pressure": {"kg/cm2g": "kg/cm¬≤g", "kg/cm2A": "kg/cm¬≤A", "bar": "bar", "psi": "psi"},
            "flow": {"m3/h": "m¬≥/h", "m3/hr": "m¬≥/h", "M3/HR": "m¬≥/h"},
            "viscosity": {"cP": "cP", "Pa¬∑s": "Pa¬∑s"}
        }
    
    def compare_field_details(self, doc_ids=None):
        """ÌïÑÎìúÎ≥Ñ ÏÉÅÏÑ∏ ÎπÑÍµê"""
        try:
            if not doc_ids:
                docs_df = self.get_all_documents()
                if docs_df.empty:
                    return {"error": "No documents found"}
                doc_ids = docs_df['id'].tolist()[:10]  # ÏµúÎåÄ 10Í∞ú Î¨∏ÏÑú
            
            # Í∞Å Î¨∏ÏÑúÏùò ÌïÑÎìú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
            field_comparison_matrix = {}
            document_info = {}
            
            for doc_id in doc_ids:
                doc_fields = self._extract_document_fields(doc_id)
                if doc_fields:
                    doc_info = self._get_document_info(doc_id)
                    document_info[doc_id] = doc_info
                    
                    # ÌëúÏ§Ä ÌïÑÎìúÎ°ú Îß§Ìïë
                    mapped_fields = self._map_to_standard_fields(doc_fields)
                    
                    for standard_field, field_data in mapped_fields.items():
                        if standard_field not in field_comparison_matrix:
                            field_comparison_matrix[standard_field] = {}
                        
                        field_comparison_matrix[standard_field][doc_id] = field_data
            
            # ÎπÑÍµê Í≤∞Í≥º ÏÉùÏÑ±
            detailed_comparison = self._generate_field_comparison_table(field_comparison_matrix, document_info)
            differences = self._identify_field_differences(field_comparison_matrix, document_info)
            
            return {
                "field_matrix": field_comparison_matrix,
                "detailed_table": detailed_comparison,
                "differences": differences,
                "document_info": document_info
            }
            
        except Exception as e:
            print(f"‚ùå Detailed field comparison error: {e}")
            return {"error": str(e)}
    
    def _extract_document_fields(self, doc_id):
        """Î¨∏ÏÑúÏóêÏÑú ÌïÑÎìú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú"""
        try:
            # ÌïÑÎìú ÎÖ∏ÎìúÎì§ Ï°∞Ìöå
            query = f"""
                SELECT n.id, n.ontology_class, n.confidence
                FROM nodes n
                WHERE n.document_id = {doc_id} 
                AND (n.ontology_class IN ('Field', 'ProcessRequirement') OR n.id LIKE '%Field%')
                ORDER BY n.confidence DESC
            """
            
            result = self.db_manager.execute_query(query)
            
            if result.empty:
                return {}
            
            # ÌïÑÎìú Îç∞Ïù¥ÌÑ∞ ÌååÏã±
            fields = {}
            for _, row in result.iterrows():
                field_id = row['id']
                
                # ÌïÑÎìúÎ™ÖÍ≥º Í∞í Ï∂îÏ∂ú
                field_info = self._parse_field_id(field_id)
                if field_info:
                    fields[field_info['name']] = {
                        'value': field_info['value'],
                        'confidence': row['confidence'],
                        'type': row['ontology_class']
                    }
            
            return fields
            
        except Exception as e:
            print(f"‚ùå Field extraction error for doc {doc_id}: {e}")
            return {}
    
    def _parse_field_id(self, field_id):
        """ÌïÑÎìú IDÏóêÏÑú ÌïÑÎìúÎ™ÖÍ≥º Í∞í Ï∂îÏ∂ú"""
        try:
            # Ïòà: "Field_01_PUMP_TYPE_0" -> "PUMP TYPE"
            # Ïòà: "Field_PUMPING_TEMPERATURE_384_C_1" -> "PUMPING TEMPERATURE", "384¬∞C"
            
            parts = field_id.split('_')
            if len(parts) < 3:
                return None
            
            # 'Field' Ïù¥ÌõÑ Î∂ÄÎ∂Ñ Ï≤òÎ¶¨
            if parts[0] == 'Field':
                field_parts = []
                value_parts = []
                collecting_value = False
                
                for i, part in enumerate(parts[1:], 1):
                    # Ïà´ÏûêÍ∞Ä ÎÇòÏò§Î©¥ Í∞í Î∂ÄÎ∂Ñ ÏãúÏûë
                    if part.isdigit() and not collecting_value:
                        collecting_value = True
                        value_parts.append(part)
                    elif collecting_value:
                        value_parts.append(part)
                    else:
                        field_parts.append(part)
                
                field_name = ' '.join(field_parts).replace('FIELD', '').strip()
                field_value = ' '.join(value_parts) if value_parts else ""
                
                # Îã®ÏúÑ Ï†ïÍ∑úÌôî
                normalized_value = self._normalize_value_unit(field_value)
                
                return {
                    'name': field_name,
                    'value': normalized_value,
                    'original_id': field_id
                }
            
            return None
            
        except Exception as e:
            print(f"‚ùå Field parsing error for {field_id}: {e}")
            return None
    
    def _normalize_value_unit(self, value_str):
        """Í∞íÍ≥º Îã®ÏúÑ Ï†ïÍ∑úÌôî"""
        if not value_str:
            return ""
        
        # Îã®ÏúÑ Ï†ïÍ∑úÌôî
        for unit_type, unit_map in self.unit_normalization.items():
            for old_unit, new_unit in unit_map.items():
                if old_unit in value_str:
                    value_str = value_str.replace(old_unit, new_unit)
        
        return value_str.strip()
    
    def _map_to_standard_fields(self, document_fields):
        """Î¨∏ÏÑú ÌïÑÎìúÎ•º ÌëúÏ§Ä ÌïÑÎìúÎ°ú Îß§Ìïë"""
        mapped_fields = {}
        
        for doc_field_name, field_data in document_fields.items():
            # ÌëúÏ§Ä ÌïÑÎìú Ï∞æÍ∏∞
            standard_field = self._find_standard_field(doc_field_name)
            
            if standard_field:
                mapped_fields[standard_field] = {
                    'value': field_data['value'],
                    'confidence': field_data['confidence'],
                    'original_name': doc_field_name,
                    'type': field_data['type']
                }
        
        return mapped_fields
    
    def _find_standard_field(self, field_name):
        """ÌïÑÎìúÎ™ÖÏùÑ ÌëúÏ§Ä ÌïÑÎìúÎ°ú Îß§Ìïë"""
        field_name_upper = field_name.upper()
        
        for standard_field, variations in self.standard_field_mapping.items():
            for variation in variations:
                if variation.upper() in field_name_upper or field_name_upper in variation.upper():
                    return standard_field
        
        return None  # Îß§ÌïëÎêòÏßÄ ÏïäÏùÄ ÌïÑÎìú
    
    def _get_document_info(self, doc_id):
        """Î¨∏ÏÑú Ï†ïÎ≥¥ Ï°∞Ìöå"""
        try:
            query = f"""
                SELECT filename, upload_time, content_length
                FROM documents
                WHERE id = {doc_id}
            """
            result = self.db_manager.execute_query(query)
            
            if not result.empty:
                return result.iloc[0].to_dict()
            return {}
            
        except Exception as e:
            print(f"‚ùå Document info error: {e}")
            return {}
    
    def _generate_field_comparison_table(self, field_matrix, document_info):
        """ÌïÑÎìú ÎπÑÍµê ÌÖåÏù¥Î∏î ÏÉùÏÑ±"""
        comparison_table = []
        
        for standard_field, doc_data in field_matrix.items():
            row = {
                'Standard_Field': standard_field.replace('_', ' ').title(),
                'Field_Type': self._get_field_category(standard_field)
            }
            
            # Í∞Å Î¨∏ÏÑúÎ≥Ñ Í∞í Ï∂îÍ∞Ä
            for doc_id, field_data in doc_data.items():
                doc_filename = document_info.get(doc_id, {}).get('filename', f'Doc_{doc_id}')
                safe_filename = doc_filename.replace('.pdf', '').replace(' ', '_')[:20]  # ÌååÏùºÎ™Ö Îã®Ï∂ï
                
                row[f'{safe_filename}_Value'] = field_data['value']
                row[f'{safe_filename}_Confidence'] = f"{field_data['confidence']:.2f}"
            
            # Ï∞®Ïù¥Ï†ê ÌëúÏãú
            values = [field_data['value'] for field_data in doc_data.values() if field_data['value']]
            unique_values = list(set(values))
            row['Has_Differences'] = "Yes" if len(unique_values) > 1 else "No"
            row['Unique_Values_Count'] = len(unique_values)
            
            comparison_table.append(row)
        
        return comparison_table
    
    def _get_field_category(self, standard_field):
        """ÌïÑÎìú Ïπ¥ÌÖåÍ≥†Î¶¨ Î∞òÌôò"""
        categories = {
            'Process': ['temperature', 'pressure_suction', 'pressure_discharge', 'capacity', 'viscosity', 'specific_gravity'],
            'Equipment': ['pump_type', 'driver_type', 'service', 'duty'],
            'Project': ['job_no', 'item_no', 'doc_no', 'revision'],
            'Other': ['notes', 'location', 'client']
        }
        
        for category, fields in categories.items():
            if standard_field in fields:
                return category
        
        return 'Unknown'
    
    def _identify_field_differences(self, field_matrix, document_info):
        """ÌïÑÎìú Ï∞®Ïù¥Ï†ê ÏãùÎ≥Ñ"""
        differences = []
        
        for standard_field, doc_data in field_matrix.items():
            values = {}
            for doc_id, field_data in doc_data.items():
                if field_data['value']:
                    doc_name = document_info.get(doc_id, {}).get('filename', f'Doc_{doc_id}')
                    values[doc_name] = field_data['value']
            
            # Í∞íÏù¥ Îã§Î•∏ Í≤ΩÏö∞Îßå Í∏∞Î°ù
            unique_values = list(set(values.values()))
            if len(unique_values) > 1:
                differences.append({
                    'field': standard_field.replace('_', ' ').title(),
                    'category': self._get_field_category(standard_field),
                    'differences': values,
                    'unique_count': len(unique_values)
                })
        
        return sorted(differences, key=lambda x: x['unique_count'], reverse=True)

# Ïò®ÌÜ®Î°úÏßÄ ÌïôÏäµÏûê Î∞è ÏøºÎ¶¨ ÌîÑÎ°úÏÑ∏ÏÑú Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±
ontology_learner = AdvancedOntologyLearner()
db_manager = DatabaseManager()
query_processor = NaturalLanguageQueryProcessor(ontology_learner, db_manager)

# Í∏∞Ï°¥ DocumentComparisonManagerÎ•º Enhanced Î≤ÑÏ†ÑÏúºÎ°ú ÍµêÏ≤¥
comparison_manager = EnhancedDocumentComparisonManager(db_manager)

# Dash Ïï± ÏÉùÏÑ±
app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])

app.layout = dbc.Container([
    html.H1("üè≠ Industrial Knowledge Graph Analyzer", className="text-primary mb-4"),
    
    dbc.Row([
        # ÏôºÏ™Ω Ìå®ÎÑê
        dbc.Col([
            dbc.Card([
                dbc.CardBody([
                    html.H5("üìÑ Document Upload"),
                    dcc.Upload(
                        id="upload-pdf",
                        children=html.Div([
                            "Drag and drop PDF files or ",
                            html.A("click to select", style={"color": "#007bff"})
                        ]),
                        style={
                            'width': '100%',
                            'height': '60px',
                            'lineHeight': '60px',
                            'borderWidth': '2px',
                            'borderStyle': 'dashed',
                            'borderRadius': '5px',
                            'textAlign': 'center',
                            'backgroundColor': '#f8f9fa'
                        },
                        multiple=True  # Îã§Ï§ë ÌååÏùº ÏóÖÎ°úÎìú ÏßÄÏõê
                    )
                ])
            ], className="mb-3"),
            
            dbc.Card([
                dbc.CardBody([
                    html.H5("üéØ Quick Actions"),
                    dbc.Button("üß™ Test Connection", id="test-btn", color="secondary", className="w-100 mb-2", size="sm"),
                    dbc.Button("üìä Load Sample Data", id="sample-btn", color="info", className="w-100 mb-2", size="sm"),
                    dbc.Button("üîÑ Update Charts", id="update-btn", color="warning", className="w-100 mb-2", size="sm"),
                    dbc.Button("üìã Compare Documents", id="compare-btn", color="success", className="w-100 mb-2", size="sm"),
                    dbc.Button("ü§ñ R-GCN Predict", id="rgcn-btn", color="primary", className="w-100 mb-2", size="sm"),
                ])
            ], className="mb-4"),
            
            html.Div(id="status-output", className="mb-4"),
            
            # ÏóÖÎ°úÎìúÎêú Î¨∏ÏÑú Î™©Î°ù
            html.Div(id="document-list", className="mb-4"),
            
            dbc.Card([
                dbc.CardBody([
                    html.H6("üìà System Info"),
                    html.P("‚úÖ Ontology Classes: 7", className="mb-1"),
                    html.P("‚úÖ Object Properties: 7", className="mb-1"),
                    html.P("‚úÖ Datatype Properties: 6", className="mb-1"),
                    html.P("üß† Advanced Learning: Enabled", className="mb-1"),
                    html.P("üíæ Database: Connected", className="mb-1"),
                    html.P("üìã Document Comparison: Enabled", className="mb-1"),
                    html.P("ü§ñ R-GCN: " + ("Enabled" if rgcn_manager else "Disabled"), className="mb-0"),
                ])
            ]),
            
            html.Div(id="learning-stats", className="mt-3")
        ], width=4),
        
        # Ïò§Î•∏Ï™Ω Ìå®ÎÑê
        dbc.Col([
            # ÏûêÏó∞Ïñ¥ ÏøºÎ¶¨ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ (Í∞úÏÑ†Îê®)
            dbc.Card([
                dbc.CardBody([
                    html.H5("üó£Ô∏è Natural Language Query"),
                    dbc.InputGroup([
                        dbc.Input(
                            id="nl-query-input",
                            placeholder="e.g., 'compare documents', 'field differences', 'document similarity', 'show all equipment with high confidence'",
                            type="text"
                        ),
                        dbc.Button("üîç Query", id="query-btn", color="primary")
                    ]),
                    html.Div(id="query-output", className="mt-3")
                ])
            ], className="mb-4"),
            
            # Î¨∏ÏÑú ÎπÑÍµê Í≤∞Í≥º ÌëúÏãú ÏòÅÏó≠
            html.Div(id="comparison-output", className="mb-4"),
            
            html.H4("üìä Node Classification"),
            dcc.Graph(id="pie-chart", style={'height': '400px'}),
            
            html.Hr(),
            
            html.H4("üï∏Ô∏è Knowledge Graph"),
            dcc.Graph(id="network-graph", style={'height': '500px'})
        ], width=8)
    ]),
    
    # Ïà®Í≤®ÏßÑ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•ÏÜå
    dcc.Store(id="data-store", data={}),
    dcc.Store(id="comparison-store", data={})  # ÎπÑÍµê Í≤∞Í≥º Ï†ÄÏû•
], fluid=True)

# Îã§Ï§ë ÌååÏùº ÏóÖÎ°úÎìú Î∞è Î¨∏ÏÑú ÎπÑÍµê ÏΩúÎ∞± - ÏÇ≠Ï†úÎê® (ÏúÑÏùò ÌÜµÌï© ÏΩúÎ∞±ÏúºÎ°ú ÎåÄÏ≤¥)

# Î™®Îì† Î≤ÑÌäº ÏΩúÎ∞±ÏùÑ ÌïòÎÇòÎ°ú ÌÜµÌï©
@app.callback(
    [Output("status-output", "children"), 
     Output("data-store", "data"),
     Output("document-list", "children"),
     Output("comparison-output", "children")],
    [Input("test-btn", "n_clicks"), 
     Input("sample-btn", "n_clicks"), 
     Input("upload-pdf", "contents"),
     Input("rgcn-btn", "n_clicks"),
     Input("compare-btn", "n_clicks")],
    [State("upload-pdf", "filename"),
     State("data-store", "data")],
    prevent_initial_call=True
)
def handle_all_buttons(test_clicks, sample_clicks, pdf_contents_list, rgcn_clicks, compare_clicks, pdf_filenames, stored_data):
    global db_manager, comparison_manager, rgcn_manager
    from dash import ctx
    
    trigger_id = ctx.triggered[0]['prop_id'].split('.')[0] if ctx.triggered else None
    print(f"üî• Button callback triggered: {trigger_id}")
    
    # Í∏∞Î≥∏Í∞íÎì§
    status_msg = ""
    data = stored_data or {}
    doc_list = ""
    comparison_result = ""
    
    if trigger_id == "test-btn":
        status_msg = dbc.Alert("‚úÖ Connection test successful! Database ready.", color="success")
        
    elif trigger_id == "sample-btn":
        G, node_features, predictions = create_sample_graph()
        
        data = {
            "nodes": [{"id": node, **node_features[node]} for node in G.nodes()],
            "edges": [{"source": u, "target": v, "relation": data.get("relation", "unknown")} 
                     for u, v, data in G.edges(data=True)],
            "predictions": predictions,
            "stats": {
                "total_nodes": G.number_of_nodes(),
                "total_edges": G.number_of_edges(),
                "source": "sample"
            }
        }
        
        status_msg = dbc.Alert([
            html.H5("‚úÖ Sample data loaded!", className="alert-heading"),
            html.P(f"Created {G.number_of_nodes()} nodes and {G.number_of_edges()} edges"),
            html.P("Click 'Update Charts' to see visualizations!")
        ], color="success")
        
    elif trigger_id == "upload-pdf" and pdf_contents_list:
        # Îã§Ï§ë ÌååÏùº Ï≤òÎ¶¨
        if not isinstance(pdf_contents_list, list):
            pdf_contents_list = [pdf_contents_list]
            pdf_filenames = [pdf_filenames]
        
        processed_files = []
        all_graph_data = {}
        
        for pdf_content, filename in zip(pdf_contents_list, pdf_filenames):
            try:
                if not filename or not filename.lower().endswith('.pdf'):
                    continue
                
                content_type, content_string = pdf_content.split(',')
                decoded = base64.b64decode(content_string)
                
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                    tmp_file.write(decoded)
                    tmp_file_path = tmp_file.name
                
                try:
                    # PDF ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
                    pdf_text = extract_pdf_text(tmp_file_path)
                    
                    if len(pdf_text.strip()) < 10:
                        continue
                    
                    # Í∑∏ÎûòÌîÑ ÏÉùÏÑ±
                    G, node_features, predictions, learning_results = process_pdf_to_graph(pdf_text, filename)
                    
                    # Í∑∏ÎûòÌîÑ Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ±
                    graph_data = {
                        "nodes": [{"id": node, **node_features[node]} for node in G.nodes()],
                        "edges": [{"source": u, "target": v, "relation": data.get("relation", "unknown")} 
                                 for u, v, data in G.edges(data=True)],
                        "predictions": predictions,
                        "learning_results": learning_results,
                        "stats": {
                            "total_nodes": G.number_of_nodes(),
                            "total_edges": G.number_of_edges(),
                            "source": "pdf",
                            "filename": filename
                        }
                    }
                    
                    # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï†ÄÏû•
                    doc_id = db_manager.save_processing_results(
                        filename, pdf_text, graph_data, learning_results, processing_time=0
                    )
                    
                    if doc_id:
                        # ÎπÑÍµê Í¥ÄÎ¶¨ÏûêÏóê Ï∂îÍ∞Ä
                        comparison_manager.add_document(doc_id, filename, graph_data, learning_results)
                        processed_files.append(filename)
                        all_graph_data[filename] = graph_data
                    
                finally:
                    if os.path.exists(tmp_file_path):
                        os.unlink(tmp_file_path)
            
            except Exception as e:
                print(f"‚ùå Error processing {filename}: {e}")
                continue
        
        if processed_files:
            # Î¨∏ÏÑú Î™©Î°ù ÌëúÏãú
            doc_list_items = [
                html.Li(f"üìÑ {filename}", className="mb-1") 
                for filename in processed_files
            ]
            doc_list = dbc.Card([
                dbc.CardBody([
                    html.H6("üìö Uploaded Documents", className="card-title"),
                    html.Ul(doc_list_items, className="mb-0")
                ])
            ], className="border-success")
            
            status_msg = dbc.Alert([
                html.H5(f"‚úÖ {len(processed_files)} documents processed!", className="alert-heading"),
                html.P("All documents saved to database and ready for comparison"),
                html.P("Try queries: 'compare documents', 'field differences', 'document similarity'")
            ], color="success")
            
            # ÎßàÏßÄÎßâ Ï≤òÎ¶¨Îêú Î¨∏ÏÑúÏùò Îç∞Ïù¥ÌÑ∞ Î∞òÌôò
            data = list(all_graph_data.values())[-1] if all_graph_data else {}
        else:
            status_msg = dbc.Alert("‚ùå No valid PDF files processed", color="warning")
    
    elif trigger_id == "rgcn-btn":
        # R-GCN ÏòàÏ∏° Ï≤òÎ¶¨
        if not stored_data or not rgcn_manager:
            status_msg = dbc.Alert("‚ö†Ô∏è No data available for R-GCN prediction", color="warning")
        else:
            try:
                # R-GCN Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
                rgcn_data = rgcn_manager.prepare_data(stored_data)
                
                if rgcn_data is None:
                    status_msg = dbc.Alert("‚ö†Ô∏è Insufficient data for R-GCN prediction", color="warning")
                else:
                    # Î™®Îç∏ ÌõàÎ†®
                    training_success = rgcn_manager.train_model(rgcn_data)
                    
                    if not training_success:
                        status_msg = dbc.Alert("‚ùå R-GCN training failed", color="danger")
                    else:
                        # ÏòàÏ∏° ÏàòÌñâ
                        predictions = rgcn_manager.predict(rgcn_data)
                        
                        if not predictions:
                            status_msg = dbc.Alert("‚ùå R-GCN prediction failed", color="danger")
                        else:
                            # ÏòàÏ∏° Í≤∞Í≥º Î∂ÑÏÑù
                            pred_counts = Counter(predictions)
                            total_nodes = len(predictions)
                            
                            status_msg = dbc.Alert([
                                html.H5("ü§ñ R-GCN Prediction Completed!", className="alert-heading"),
                                html.P(f"Predicted {total_nodes} nodes with neural network:"),
                                html.Ul([
                                    html.Li(f"Class 0 (Unknown): {pred_counts.get(0, 0)} nodes"),
                                    html.Li(f"Class 1 (Entity): {pred_counts.get(1, 0)} nodes"),
                                    html.Li(f"Class 2 (Literal): {pred_counts.get(2, 0)} nodes")
                                ]),
                                html.P("Neural predictions can be compared with rule-based classifications!")
                            ], color="success")
                            
            except Exception as e:
                status_msg = dbc.Alert(f"‚ùå R-GCN error: {str(e)}", color="danger")
    
    elif trigger_id == "compare-btn":
        # Î¨∏ÏÑú ÎπÑÍµê Ï≤òÎ¶¨
        try:
            result = query_processor._handle_document_comparison()
            
            if result["type"] == "error":
                comparison_result = dbc.Alert(f"‚ùå {result['message']}", color="danger")
            else:
                comparison_data = result["data"]
                
                # Î¨∏ÏÑú Í∞úÏöî ÌÖåÏù¥Î∏î
                overview_table = None
                if comparison_data.get("overview"):
                    overview_df = pd.DataFrame(comparison_data["overview"])
                    overview_df['upload_time'] = overview_df['upload_time'].apply(
                        lambda x: x.split('T')[0] if 'T' in str(x) else str(x)
                    )
                    overview_table = dbc.Table.from_dataframe(
                        overview_df[['filename', 'node_count', 'edge_count', 'avg_confidence', 'upload_time']], 
                        striped=True, bordered=True, hover=True
                    )
                
                # ÌÜµÍ≥Ñ ÎπÑÍµê Ï∞®Ìä∏
                stats_chart = None
                if comparison_data.get("statistics"):
                    stats_df = pd.DataFrame(comparison_data["statistics"])
                    import plotly.express as px
                    fig = px.bar(
                        stats_df, 
                        x='filename', 
                        y=['entity_count', 'literal_count', 'edge_count'],
                        title="Document Statistics Comparison",
                        barmode='group'
                    )
                    fig.update_layout(height=300)
                    stats_chart = dcc.Graph(figure=fig)
                
                # Ïú†ÏÇ¨ÎèÑ Ï†ïÎ≥¥
                similarity_info = comparison_data.get("similarity", {})
                similarity_score = similarity_info.get("similarity_score", 0)
                common_entities_count = len(similarity_info.get("common_entities", []))
                
                comparison_result = dbc.Card([
                    dbc.CardBody([
                        html.H5("üìã Document Comparison Results", className="card-title"),
                        
                        html.H6("üìä Document Overview"),
                        overview_table if overview_table is not None else html.P("No overview data available"),
                        
                        html.Hr(),
                        
                        html.H6("üìà Statistics Comparison"),
                        stats_chart if stats_chart is not None else html.P("No statistics data available"),
                        
                        html.Hr(),
                        
                        html.H6("üîç Similarity Analysis"),
                        html.P(f"üìã Similarity Score: {similarity_score:.2%}"),
                        html.P(f"ü§ù Common Entities: {common_entities_count}"),
                    ])
                ], className="border-info")
                
        except Exception as e:
            comparison_result = dbc.Alert(f"‚ùå Comparison failed: {str(e)}", color="danger")
    
    return status_msg, data, doc_list, comparison_result

# Î¨∏ÏÑú ÎπÑÍµê ÏΩúÎ∞± - ÏÇ≠Ï†úÎê® (ÏúÑÏùò ÌÜµÌï© ÏΩúÎ∞±ÏúºÎ°ú ÎåÄÏ≤¥)

@app.callback(
    Output("pie-chart", "figure"),
    [Input("update-btn", "n_clicks")],
    [State("data-store", "data")]
)
def update_pie_chart(n_clicks, stored_data):
    print(f"üéØ Pie chart callback - clicks: {n_clicks}, data: {bool(stored_data)}")
    
    if n_clicks and stored_data and "predictions" in stored_data:
        predictions = stored_data["predictions"]
        pred_counts = Counter(predictions)
        
        labels = ["Class", "Entity", "Literal"]
        values = [pred_counts.get(i, 0) for i in range(3)]
        colors = ["#ff9999", "#66b3ff", "#99ff99"]
        
        fig = go.Figure()
        fig.add_trace(go.Pie(
            labels=labels,
            values=values,
            hole=0.3,
            textinfo="label+percent+value",
            marker=dict(colors=colors)
        ))
        
        fig.update_layout(
            title=f"Node Classification Results (Total: {sum(values)})",
            height=400,
            showlegend=True
        )
        
        print(f"‚úÖ Pie chart created: {values}")
        return fig
    
    return go.Figure().add_annotation(
        text="Load sample data or upload PDF, then click 'Update Charts'",
        showarrow=False, x=0.5, y=0.5
    )

@app.callback(
    Output("network-graph", "figure"),
    [Input("update-btn", "n_clicks")],
    [State("data-store", "data")]
)
def update_network_graph(n_clicks, stored_data):
    print(f"üï∏Ô∏è Network graph callback - clicks: {n_clicks}, data: {bool(stored_data)}")
    
    if n_clicks and stored_data and "nodes" in stored_data:
        nodes = stored_data["nodes"]
        edges = stored_data["edges"]
        
        G = nx.DiGraph()
        for node in nodes:
            G.add_node(node["id"])
        for edge in edges:
            G.add_edge(edge["source"], edge["target"])
        
        pos = nx.spring_layout(G, k=2, iterations=50)
        
        edge_x, edge_y = [], []
        for edge in G.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
        
        node_x = [pos[node][0] for node in G.nodes()]
        node_y = [pos[node][1] for node in G.nodes()]
        node_text = [node[:15] + "..." if len(node) > 15 else node for node in G.nodes()]
        
        # ÎÖ∏Îìú ÏÉâÏÉÅ Î∞è ÌÅ¨Í∏∞ (Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò)
        node_colors = []
        node_sizes = []
        hover_text = []
        
        for node in G.nodes():
            node_info = next((n for n in nodes if n["id"] == node), {})
            confidence = node_info.get("confidence", 0.5)
            node_type = node_info.get("type", "unknown")
            ontology_class = node_info.get("ontology_class", "Unknown")
            
            # Ïã†Î¢∞ÎèÑÏóê Îî∞Î•∏ ÌÅ¨Í∏∞
            node_sizes.append(15 + confidence * 25)
            
            # ÌÉÄÏûÖÏóê Îî∞Î•∏ ÏÉâÏÉÅ
            if node_type == "entity":
                base_color = [70, 130, 180]  # Ïä§Ìã∏Î∏îÎ£®
            elif node_type == "literal":
                base_color = [60, 179, 113]  # ÎØ∏ÎîîÏóÑÏî®Í∑∏Î¶∞
            else:
                base_color = [255, 99, 71]   # ÌÜ†ÎßàÌÜ†
            
            # Ïã†Î¢∞ÎèÑÏóê Îî∞Î•∏ Ìà¨Î™ÖÎèÑ
            alpha = 0.4 + confidence * 0.6
            color = f"rgba({base_color[0]}, {base_color[1]}, {base_color[2]}, {alpha})"
            node_colors.append(color)
            
            # Ìò∏Î≤Ñ ÌÖçÏä§Ìä∏
            hover_text.append(
                f"<b>{node}</b><br>" +
                f"Type: {node_type}<br>" +
                f"Class: {ontology_class}<br>" +
                f"Confidence: {confidence:.3f}"
            )
        
        fig = go.Figure()
        
        # Ïó£ÏßÄ Ï∂îÍ∞Ä
        fig.add_trace(go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=1.5, color='rgba(125,125,125,0.5)'),
            mode='lines',
            showlegend=False,
            hoverinfo='none'
        ))
        
        # ÎÖ∏Îìú Ï∂îÍ∞Ä (Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò ÏãúÍ∞ÅÌôî)
        fig.add_trace(go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            text=node_text,
            textposition="middle center",
            textfont=dict(size=8, color="white"),
            marker=dict(
                size=node_sizes,
                color=node_colors,
                line=dict(width=1, color='rgba(50,50,50,0.8)')
            ),
            showlegend=False,
            hoverinfo='text',
            hovertext=hover_text
        ))
        
        fig.update_layout(
            title=f"Advanced Knowledge Graph ({G.number_of_nodes()} nodes, {G.number_of_edges()} edges)<br><sub>Node size = confidence, Color = type (Blue=Entity, Green=Literal, Red=Other)</sub>",
            height=500,
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            showlegend=False,
            plot_bgcolor='rgba(240,240,240,0.2)',
            margin=dict(l=20, r=20, t=80, b=20)
        )
        
        print(f"‚úÖ Enhanced network graph created: {G.number_of_nodes()} nodes")
        return fig
    
    return go.Figure().add_annotation(
        text="Load sample data or upload PDF, then click 'Update Charts'",
        showarrow=False, x=0.5, y=0.5
    )

@app.callback(
    Output("learning-stats", "children"),
    [Input("data-store", "data")]
)
def update_learning_stats(stored_data):
    """Ïò®ÌÜ®Î°úÏßÄ ÌïôÏäµ ÌÜµÍ≥Ñ ÌëúÏãú"""
    if not stored_data or "learning_results" not in stored_data:
        return ""
    
    learning_results = stored_data["learning_results"]
    stats = stored_data.get("stats", {})
    
    confidence_scores = learning_results.get("confidence_scores", {})
    domain_insights = learning_results.get("domain_insights", {})
    
    if not confidence_scores and not domain_insights:
        return ""
    
    # Ïã†Î¢∞ÎèÑ Î∂ÑÌè¨
    high_conf = sum(1 for c in confidence_scores.values() if c > 0.8)
    med_conf = sum(1 for c in confidence_scores.values() if 0.5 < c <= 0.8)
    low_conf = sum(1 for c in confidence_scores.values() if c <= 0.5)
    
    return dbc.Card([
        dbc.CardBody([
            html.H6("üß† Learning Analytics", className="card-title"),
            html.P(f"üéñÔ∏è High Confidence: {high_conf}", className="mb-1"),
            html.P(f"üèÉ Medium Confidence: {med_conf}", className="mb-1"),
            html.P(f"ü§î Low Confidence: {low_conf}", className="mb-1"),
            html.Hr(className="my-2"),
            html.P(f"üìà Technical Density: {domain_insights.get('technical_density', 0):.3f}", className="mb-1"),
            html.P(f"üî¨ Document Type: {domain_insights.get('document_type', 'Unknown')}", className="mb-0"),
        ])
    ], className="border-info")

@app.callback(
    Output("query-output", "children"),
    [Input("query-btn", "n_clicks")],
    [State("nl-query-input", "value"), State("data-store", "data")]
)
def handle_natural_language_query(n_clicks, query_text, stored_data):
    """ÏûêÏó∞Ïñ¥ ÏøºÎ¶¨ Ï≤òÎ¶¨ (Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌÜµÌï© + Î¨∏ÏÑú ÎπÑÍµê)"""
    if not n_clicks or not query_text:
        return html.Div([
            html.P("üí° Query Examples:", className="mb-2 font-weight-bold"),
            html.Ul([
                html.Li("database statistics"),
                html.Li("show recent documents"),
                html.Li("compare documents"),
                html.Li("field differences"),
                html.Li("document similarity"),
                html.Li("search for equipment"),
                html.Li("show all entities with high confidence"),
                html.Li("count all nodes"),
                html.Li("show relationships")
            ], className="mb-0")
        ])
    
    print(f"üó£Ô∏è Natural language query: '{query_text}'")
    
    try:
        # ÏøºÎ¶¨ Ï≤òÎ¶¨ (Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïö∞ÏÑ† + Î¨∏ÏÑú ÎπÑÍµê)
        result = query_processor.process_query(query_text, stored_data)
        
        # Í≤∞Í≥º Î†åÎçîÎßÅ
        return render_query_result(result)
        
    except Exception as e:
        print(f"‚ùå Query processing error: {e}")
        return dbc.Alert(f"‚ùå Error processing query: {str(e)}", color="danger")

def render_query_result(result):
    """ÏøºÎ¶¨ Í≤∞Í≥ºÎ•º Î†åÎçîÎßÅ (ÏÉÅÏÑ∏ ÌïÑÎìú ÎπÑÍµê ÏßÄÏõê)"""
    result_type = result.get("type", "error")
    message = result.get("message", "")
    data = result.get("data", [])
    
    components = [
        html.P(f"‚úÖ {message}", className="text-success font-weight-bold mb-3")
    ]
    
    if result_type == "detailed_comparison":
        # ÏÉÅÏÑ∏ Î¨∏ÏÑú ÎπÑÍµê Í≤∞Í≥º Ï≤òÎ¶¨
        comparison_data = data
        
        # 1. Î¨∏ÏÑú Í∞úÏöî
        if comparison_data.get("overview"):
            overview_df = pd.DataFrame(comparison_data["overview"])
            overview_df['upload_time'] = overview_df['upload_time'].apply(
                lambda x: x.split('T')[0] if 'T' in str(x) else str(x)
            )
            components.extend([
                html.H6("üìä Document Overview"),
                dbc.Table.from_dataframe(
                    overview_df[['filename', 'node_count', 'edge_count', 'avg_confidence']], 
                    striped=True, bordered=True, hover=True, size="sm"
                )
            ])
        
        # 2. ÌïÑÎìúÎ≥Ñ Ï∞®Ïù¥Ï†ê (NEW!)
        field_differences = comparison_data.get("field_differences", [])
        if field_differences:
            components.extend([
                html.Hr(),
                html.H6("üîç Field Differences (Key Findings)"),
            ])
            
            for diff in field_differences[:5]:  # ÏÉÅÏúÑ 5Í∞ú Ï∞®Ïù¥Ï†ê
                field_name = diff['field']
                differences = diff['differences']
                
                diff_items = []
                for doc_name, value in differences.items():
                    short_name = doc_name.replace('.pdf', '')[:20]
                    diff_items.append(html.Li(f"{short_name}: {value}"))
                
                components.append(
                    dbc.Card([
                        dbc.CardBody([
                            html.H6(f"üè∑Ô∏è {field_name}", className="card-title"),
                            html.P(f"Category: {diff['category']} | Unique Values: {diff['unique_count']}", 
                                  className="text-muted small"),
                            html.Ul(diff_items, className="mb-0")
                        ])
                    ], className="mb-2")
                )
        
        # 3. ÏÉÅÏÑ∏ ÌïÑÎìú ÎπÑÍµê ÌÖåÏù¥Î∏î (NEW!)
        detailed_table = comparison_data.get("detailed_field_table", [])
        if detailed_table:
            components.extend([
                html.Hr(),
                html.H6("üìã Detailed Field Comparison Matrix"),
            ])
            
            # Ï∞®Ïù¥Í∞Ä ÏûàÎäî ÌïÑÎìúÎßå ÌëúÏãú
            different_fields = [row for row in detailed_table if row.get('Has_Differences') == 'Yes']
            
            if different_fields:
                df = pd.DataFrame(different_fields)
                # Ïª¨Îüº ÏÑ†ÌÉù (ÎÑàÎ¨¥ ÎßéÏúºÎ©¥ ÏùºÎ∂ÄÎßå)
                display_cols = ['Standard_Field', 'Field_Type', 'Has_Differences', 'Unique_Values_Count']
                value_cols = [col for col in df.columns if '_Value' in col]
                display_cols.extend(value_cols[:3])  # ÏµúÎåÄ 3Í∞ú Î¨∏ÏÑúÎßå ÌëúÏãú
                
                if len(display_cols) <= len(df.columns):
                    table = dbc.Table.from_dataframe(
                        df[display_cols], 
                        striped=True, bordered=True, hover=True, size="sm"
                    )
                    components.append(table)
                
                if len(different_fields) > 10:
                    components.append(
                        html.P(f"... and {len(different_fields) - 10} more field differences", 
                              className="text-muted")
                    )
            else:
                components.append(html.P("No field differences found.", className="text-muted"))
        
        # 4. Ïú†ÏÇ¨ÎèÑ Ï†ïÎ≥¥
        similarity = comparison_data.get("similarity", {})
        if similarity:
            components.extend([
                html.Hr(),
                html.H6("üîç Content Similarity"),
                html.P(f"Similarity Score: {similarity.get('similarity_score', 0):.2%}"),
                html.P(f"Common Entities: {len(similarity.get('common_entities', []))}")
            ])
    
    elif result_type == "comparison":
        # Í∏∞Ï°¥ Í∏∞Î≥∏ ÎπÑÍµê Í≤∞Í≥º Ï≤òÎ¶¨ (ÌïòÏúÑ Ìò∏ÌôòÏÑ±)
        comparison_data = data
        
        # Î¨∏ÏÑú Í∞úÏöî
        if comparison_data.get("overview"):
            overview_df = pd.DataFrame(comparison_data["overview"])
            overview_df['upload_time'] = overview_df['upload_time'].apply(
                lambda x: x.split('T')[0] if 'T' in str(x) else str(x)
            )
            components.extend([
                html.H6("üìä Document Overview"),
                dbc.Table.from_dataframe(
                    overview_df[['filename', 'node_count', 'edge_count', 'avg_confidence']], 
                    striped=True, bordered=True, hover=True, size="sm"
                )
            ])
        
        # ÌïÑÎìú ÎπÑÍµê
        field_comparison = comparison_data.get("field_comparison", {})
        if field_comparison:
            field_summary = []
            for field_name, doc_dict in list(field_comparison.items())[:10]:
                field_summary.append({
                    "Field": field_name,
                    "Documents": len(doc_dict),
                    "Avg Confidence": sum(doc_dict.values()) / len(doc_dict)
                })
            
            if field_summary:
                components.extend([
                    html.Hr(),
                    html.H6("üè∑Ô∏è Field Comparison (Top 10)"),
                    dbc.Table.from_dataframe(
                        pd.DataFrame(field_summary), 
                        striped=True, bordered=True, hover=True, size="sm"
                    )
                ])
        
        # Ïú†ÏÇ¨ÎèÑ Ï†ïÎ≥¥
        similarity = comparison_data.get("similarity", {})
        if similarity:
            components.extend([
                html.Hr(),
                html.H6("üîç Content Similarity"),
                html.P(f"Similarity Score: {similarity.get('similarity_score', 0):.2%}"),
                html.P(f"Common Entities: {len(similarity.get('common_entities', []))}")
            ])
    
    elif result_type == "table" and data:
        # Í∏∞Ï°¥ ÌÖåÏù¥Î∏î Ï≤òÎ¶¨
        columns = result.get("columns", [])
        if not columns and data:
            columns = list(data[0].keys()) if data else []
        
        clean_data = []
        for item in data[:20]:
            clean_item = {}
            for key, value in item.items():
                if isinstance(value, float):
                    clean_item[key] = round(value, 3)
                else:
                    clean_item[key] = str(value)[:50]
            clean_data.append(clean_item)
        
        if clean_data:
            df = pd.DataFrame(clean_data)
            table = dbc.Table.from_dataframe(
                df, striped=True, bordered=True, hover=True, responsive=True, size="sm"
            )
            components.append(table)
            
            if len(data) > 20:
                components.append(
                    html.P(f"... and {len(data) - 20} more items", className="text-muted")
                )
    
    elif result_type == "stat" and data:
        # ÌÜµÍ≥Ñ ÌëúÏãú
        stat_cards = []
        for key, value in data.items():
            stat_cards.append(
                dbc.Col([
                    dbc.Card([
                        dbc.CardBody([
                            html.H5(str(value), className="text-primary"),
                            html.P(key, className="mb-0")
                        ])
                    ], className="text-center")
                ], width=3)
            )
        
        components.append(dbc.Row(stat_cards[:4]))
        
        if len(stat_cards) > 4:
            components.append(dbc.Row(stat_cards[4:8], className="mt-2"))
    
    elif result_type == "suggestions" and data:
        components.extend([
            html.P("üí° Try these queries:", className="font-weight-bold"),
            html.Ul([html.Li(suggestion) for suggestion in data])
        ])
    
    elif result_type == "error":
        components = [dbc.Alert(f"‚ùå {message}", color="danger")]
    
    else:
        components.append(html.P("No results found."))
    
    return html.Div(components)

if __name__ == "__main__":
    print("=" * 60)
    print("üöÄ Starting Advanced Industrial Knowledge Graph Analyzer...")
    print("=" * 60)
    print("üìã System Status Check:")
    print("   üß† Enhanced ontology learning: ‚úÖ Enabled")
    print("   üíæ Database system: ‚úÖ Integrated")
    print("   üîç Natural language queries: ‚úÖ Supported")
    print("   üéØ Advanced pattern recognition: ‚úÖ Active")
    print("   üìã Multi-Document Comparison: ‚úÖ Enabled")
    
    # R-GCN ÏÉÅÌÉú ÌôïÏù∏
    if rgcn_manager:
        try:
            status = rgcn_manager.get_status()
            print(f"   ü§ñ R-GCN Model: ‚úÖ Ready ({status.get('device', 'unknown')})")
            print(f"   üîó Relation Types: {status.get('num_relations', 0)} configured")
        except:
            print("   ü§ñ R-GCN Model: ‚ö†Ô∏è Available but status check failed")
    else:
        print("   ü§ñ R-GCN Model: ‚ùå Not available")
    
    # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÉÅÌÉú ÌôïÏù∏
    try:
        db_stats = db_manager.get_database_stats()
        print(f"   üìä Database: ‚úÖ Connected ({db_stats.get('total_documents', 0)} documents)")
    except:
        print("   üìä Database: ‚ö†Ô∏è Connected but status check failed")
    
    print("=" * 60)
    print("üéâ Features Available:")
    print("   üìÑ Multi-PDF Upload & OCR Processing")
    print("   üï∏Ô∏è Knowledge Graph Visualization")
    print("   üó£Ô∏è Natural Language Queries")
    print("   ü§ñ R-GCN Neural Network Predictions")
    print("   üìä Rule-based vs Neural Comparison")
    print("   üíæ Persistent Database Storage")
    print("   üìã Advanced Document Comparison")
    print("=" * 60)
    print("üí° Document Comparison Queries:")
    print("   ‚Ä¢ 'compare documents' - Compare all uploaded documents")
    print("   ‚Ä¢ 'field differences' - Show field differences between docs")
    print("   ‚Ä¢ 'document similarity' - Analyze content similarity")
    print("   ‚Ä¢ 'list all documents' - Show all processed documents")
    print("=" * 60)
    print("üåê Server Starting...")
    print("üîó URL: http://127.0.0.1:8050")
    print("üì± Mobile friendly interface available")
    print("=" * 60)
    
    app.run(debug=True, port=8050, host='127.0.0.1')