import os
import base64
import json
import pandas as pd
import networkx as nx
import plotly.graph_objects as go
from dash import Dash, dcc, html, Input, Output, State
import dash_bootstrap_components as dbc
from collections import Counter
import tempfile
import re
import sqlite3
from datetime import datetime

# PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì )
try:
    from pdfminer.high_level import extract_text
    PDF_AVAILABLE = True
except ImportError:
    print("PDFMiner not available - using mock text extraction")
    PDF_AVAILABLE = False

# DuckDB ì§€ì› (ì„ íƒì )
try:
    import duckdb
    DUCKDB_AVAILABLE = True
except ImportError:
    print("DuckDB not available - using SQLite instead")
    DUCKDB_AVAILABLE = False

# ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤
class DatabaseManager:
    def __init__(self, db_path="knowledge_graph.db"):
        self.db_path = db_path
        self.use_duckdb = DUCKDB_AVAILABLE
        self.init_database()
    
    def get_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë°˜í™˜"""
        if self.use_duckdb:
            return duckdb.connect(self.db_path.replace('.db', '.duckdb'))
        else:
            return sqlite3.connect(self.db_path)
    
    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì´ˆê¸°í™”"""
        try:
            conn = self.get_connection()
            
            # ë¬¸ì„œ í…Œì´ë¸”
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS documents (
                        id INTEGER PRIMARY KEY,
                        filename TEXT NOT NULL,
                        upload_time TEXT NOT NULL,
                        content_length INTEGER,
                        processing_time REAL,
                        text_preview TEXT
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS documents (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        filename TEXT NOT NULL,
                        upload_time TEXT NOT NULL,
                        content_length INTEGER,
                        processing_time REAL,
                        text_preview TEXT
                    );
                """)
            
            # ë…¸ë“œ í…Œì´ë¸”
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS nodes (
                        id TEXT PRIMARY KEY,
                        document_id INTEGER,
                        node_type TEXT,
                        ontology_class TEXT,
                        confidence REAL,
                        predicted_class INTEGER,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS nodes (
                        id TEXT PRIMARY KEY,
                        document_id INTEGER,
                        node_type TEXT,
                        ontology_class TEXT,
                        confidence REAL,
                        predicted_class INTEGER,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            
            # ì—£ì§€ í…Œì´ë¸”
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS edges (
                        id INTEGER PRIMARY KEY,
                        document_id INTEGER,
                        source_node TEXT,
                        target_node TEXT,
                        relation_type TEXT,
                        confidence REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS edges (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        document_id INTEGER,
                        source_node TEXT,
                        target_node TEXT,
                        relation_type TEXT,
                        confidence REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            
            # ì˜¨í†¨ë¡œì§€ íŒ¨í„´ í…Œì´ë¸”
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS ontology_patterns (
                        id INTEGER PRIMARY KEY,
                        pattern_type TEXT,
                        pattern_value TEXT,
                        frequency INTEGER,
                        confidence_avg REAL,
                        last_seen TEXT,
                        document_count INTEGER
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS ontology_patterns (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        pattern_type TEXT,
                        pattern_value TEXT,
                        frequency INTEGER,
                        confidence_avg REAL,
                        last_seen TEXT,
                        document_count INTEGER
                    );
                """)
            
            # ë„ë©”ì¸ ì¸ì‚¬ì´íŠ¸ í…Œì´ë¸”
            if self.use_duckdb:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS domain_insights (
                        id INTEGER PRIMARY KEY,
                        document_id INTEGER,
                        document_type TEXT,
                        industry_domain TEXT,
                        complexity_score REAL,
                        technical_density REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            else:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS domain_insights (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        document_id INTEGER,
                        document_type TEXT,
                        industry_domain TEXT,
                        complexity_score REAL,
                        technical_density REAL,
                        creation_time TEXT,
                        FOREIGN KEY (document_id) REFERENCES documents (id)
                    );
                """)
            
            if self.use_duckdb:
                conn.commit()
            else:
                conn.commit()
            
            conn.close()
            print(f"âœ… Database initialized: {'DuckDB' if self.use_duckdb else 'SQLite'}")
            
        except Exception as e:
            print(f"âŒ Database initialization error: {e}")
    
    def save_processing_results(self, filename, text_content, graph_data, learning_results, processing_time=0):
        """ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            conn = self.get_connection()
            current_time = datetime.now().isoformat()
            
            # 1. ë¬¸ì„œ ì •ë³´ ì €ì¥
            text_preview = text_content[:500] if text_content else ""
            
            if self.use_duckdb:
                # DuckDB: ìˆ˜ë™ ID í• ë‹¹
                doc_count = conn.execute("SELECT COUNT(*) FROM documents").fetchone()[0]
                doc_id = doc_count + 1
                
                conn.execute("""
                    INSERT INTO documents (id, filename, upload_time, content_length, processing_time, text_preview)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, [doc_id, filename, current_time, len(text_content), processing_time, text_preview])
            else:
                # SQLite: AUTO INCREMENT ì‚¬ìš©
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO documents (filename, upload_time, content_length, processing_time, text_preview)
                    VALUES (?, ?, ?, ?, ?)
                """, (filename, current_time, len(text_content), processing_time, text_preview))
                doc_id = cursor.lastrowid
            
            # 2. ë…¸ë“œ ì €ì¥
            nodes = graph_data.get("nodes", [])
            predictions = graph_data.get("predictions", [])
            
            for i, node in enumerate(nodes):
                predicted_class = predictions[i] if i < len(predictions) else 0
                
                if self.use_duckdb:
                    conn.execute("""
                        INSERT INTO nodes (id, document_id, node_type, ontology_class, confidence, predicted_class, creation_time)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, [
                        node["id"], doc_id, node.get("type", "unknown"), 
                        node.get("ontology_class", "Unknown"), node.get("confidence", 0.5),
                        predicted_class, current_time
                    ])
                else:
                    conn.execute("""
                        INSERT INTO nodes (id, document_id, node_type, ontology_class, confidence, predicted_class, creation_time)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        node["id"], doc_id, node.get("type", "unknown"), 
                        node.get("ontology_class", "Unknown"), node.get("confidence", 0.5),
                        predicted_class, current_time
                    ))
            
            # 3. ì—£ì§€ ì €ì¥
            edges = graph_data.get("edges", [])
            
            if self.use_duckdb:
                edge_id = 1
                for edge in edges:
                    conn.execute("""
                        INSERT INTO edges (id, document_id, source_node, target_node, relation_type, confidence, creation_time)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, [
                        edge_id, doc_id, edge.get("source", ""), edge.get("target", ""),
                        edge.get("relation", "unknown"), 0.8, current_time
                    ])
                    edge_id += 1
            else:
                for edge in edges:
                    conn.execute("""
                        INSERT INTO edges (document_id, source_node, target_node, relation_type, confidence, creation_time)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        doc_id, edge.get("source", ""), edge.get("target", ""),
                        edge.get("relation", "unknown"), 0.8, current_time
                    ))
            
            # 4. ì˜¨í†¨ë¡œì§€ íŒ¨í„´ ì €ì¥/ì—…ë°ì´íŠ¸
            patterns = learning_results.get("patterns", {})
            confidence_scores = learning_results.get("confidence_scores", {})
            
            pattern_id = 1
            for pattern_type, pattern_list in patterns.items():
                for pattern_value in pattern_list:
                    pattern_str = str(pattern_value)[:200]  # ê¸¸ì´ ì œí•œ
                    avg_confidence = sum(confidence_scores.values()) / max(len(confidence_scores), 1)
                    
                    if self.use_duckdb:
                        conn.execute("""
                            INSERT INTO ontology_patterns (id, pattern_type, pattern_value, frequency, confidence_avg, last_seen, document_count)
                            VALUES (?, ?, ?, ?, ?, ?, ?)
                        """, [pattern_id, pattern_type, pattern_str, 1, avg_confidence, current_time, 1])
                        pattern_id += 1
                    else:
                        conn.execute("""
                            INSERT INTO ontology_patterns (pattern_type, pattern_value, frequency, confidence_avg, last_seen, document_count)
                            VALUES (?, ?, ?, ?, ?, ?)
                        """, (pattern_type, pattern_str, 1, avg_confidence, current_time, 1))
            
            # 5. ë„ë©”ì¸ ì¸ì‚¬ì´íŠ¸ ì €ì¥
            domain_insights = learning_results.get("domain_insights", {})
            if domain_insights:
                if self.use_duckdb:
                    insight_id = 1
                    conn.execute("""
                        INSERT INTO domain_insights (id, document_id, document_type, industry_domain, complexity_score, technical_density, creation_time)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, [
                        insight_id, doc_id, domain_insights.get("document_type", "unknown"),
                        domain_insights.get("industry_domain", "unknown"),
                        domain_insights.get("complexity_score", 0),
                        domain_insights.get("technical_density", 0),
                        current_time
                    ])
                else:
                    conn.execute("""
                        INSERT INTO domain_insights (document_id, document_type, industry_domain, complexity_score, technical_density, creation_time)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        doc_id, domain_insights.get("document_type", "unknown"),
                        domain_insights.get("industry_domain", "unknown"),
                        domain_insights.get("complexity_score", 0),
                        domain_insights.get("technical_density", 0),
                        current_time
                    ))
            
            if self.use_duckdb:
                conn.commit()
            else:
                conn.commit()
            
            conn.close()
            
            print(f"âœ… Data saved to database - Document ID: {doc_id}")
            return doc_id
            
        except Exception as e:
            print(f"âŒ Database save error: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def execute_query(self, query):
        """SQL ì¿¼ë¦¬ ì‹¤í–‰"""
        try:
            conn = self.get_connection()
            
            if self.use_duckdb:
                result = conn.execute(query).fetch_df()
            else:
                result = pd.read_sql_query(query, conn)
            
            conn.close()
            return result
            
        except Exception as e:
            print(f"âŒ Query execution error: {e}")
            return pd.DataFrame()
    
    def get_database_stats(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ë°˜í™˜"""
        try:
            stats = {}
            
            # ê¸°ë³¸ í†µê³„
            stats["total_documents"] = self.execute_query("SELECT COUNT(*) as count FROM documents")["count"].iloc[0]
            stats["total_nodes"] = self.execute_query("SELECT COUNT(*) as count FROM nodes")["count"].iloc[0]
            stats["total_edges"] = self.execute_query("SELECT COUNT(*) as count FROM edges")["count"].iloc[0]
            stats["total_patterns"] = self.execute_query("SELECT COUNT(*) as count FROM ontology_patterns")["count"].iloc[0]
            
            # ìµœê·¼ ë¬¸ì„œ
            recent_docs = self.execute_query("""
                SELECT filename, upload_time, content_length 
                FROM documents 
                ORDER BY upload_time DESC 
                LIMIT 5
            """)
            stats["recent_documents"] = recent_docs.to_dict('records') if not recent_docs.empty else []
            
            # ë…¸ë“œ íƒ€ì…ë³„ ë¶„í¬
            node_distribution = self.execute_query("""
                SELECT node_type, COUNT(*) as count 
                FROM nodes 
                GROUP BY node_type
            """)
            stats["node_distribution"] = node_distribution.to_dict('records') if not node_distribution.empty else []
            
            # ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ë³„ ë¶„í¬  
            class_distribution = self.execute_query("""
                SELECT ontology_class, COUNT(*) as count, AVG(confidence) as avg_confidence
                FROM nodes 
                GROUP BY ontology_class 
                ORDER BY count DESC
            """)
            stats["class_distribution"] = class_distribution.to_dict('records') if not class_distribution.empty else []
            
            # ê³ ì‹ ë¢°ë„ ì—”í‹°í‹° ìˆ˜
            high_confidence_count = self.execute_query("""
                SELECT COUNT(*) as count 
                FROM nodes 
                WHERE confidence > 0.8
            """)["count"].iloc[0]
            stats["high_confidence_entities"] = high_confidence_count
            
            return stats
            
        except Exception as e:
            print(f"âŒ Database stats error: {e}")
            return {}
        
def extract_pdf_text(pdf_path):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if PDF_AVAILABLE:
        try:
            return extract_text(pdf_path)
        except Exception as e:
            print(f"PDF extraction error: {e}")
            return "Error extracting PDF text"
    else:
        # Mock í…ìŠ¤íŠ¸ ë°˜í™˜
        return """Industrial Equipment Specification
        Project: 7T04 - Centrifugal Pump Process Data
        Equipment ID: P-2105 A/B
        Pump Type: CENTRIFUGAL
        Driver Type: MOTOR / MOTOR
        Capacity: 71.1 m3/h (AM Feed)
        Temperature: 384 â„ƒ
        Pressure: 17.1 kg/cm2A
        Viscosity: 1.0 cP
        Revision: 14
        Checked by: HJL / SKL
        Date: 2012-12-26"""

# ì˜¨í†¨ë¡œì§€ í´ë˜ìŠ¤ (ëŒ€í­ ê°•í™”ëœ ë²„ì „)
class AdvancedOntologyLearner:
    def __init__(self):
        # ê¸°ë³¸ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ
        self.classes = {"Project", "Equipment", "ProcessRequirement", "Person", "Document", "Revision", "Note"}
        self.object_properties = {"hasEquipment", "hasProcessReq", "reviewedBy", "hasProject", "hasValue", "hasRevision", "hasNote"}
        self.datatype_properties = {"jobNo", "itemNo", "value", "noteText", "revisionNumber", "revisionDate"}
        
        # í•™ìŠµëœ íŒ¨í„´ ì €ì¥ì†Œ
        self.learned_patterns = {}
        self.entity_instances = {}
        self.confidence_scores = {}
        
        # ë„ë©”ì¸ íŠ¹í™” í‚¤ì›Œë“œ
        self.domain_keywords = {
            "Project": ["job", "project", "doc", "document", "specification", "drawing"],
            "Equipment": ["pump", "motor", "driver", "centrifugal", "equipment", "vessel", "tank"],
            "ProcessRequirement": ["temperature", "pressure", "capacity", "flow", "viscosity", "density"],
            "Person": ["checked", "reviewed", "approved", "by", "engineer", "manager"],
            "Revision": ["revision", "rev", "version", "updated", "modified"],
            "Note": ["note", "remark", "comment", "description"]
        }
        
        # ë‹¨ìœ„ ë° ì¸¡ì •ê°’ íŒ¨í„´
        self.unit_patterns = {
            "temperature": ["â„ƒ", "Â°C", "Â°F", "K"],
            "pressure": ["kg/cm2", "bar", "psi", "Pa", "kPa", "MPa"],
            "flow": ["m3/h", "l/min", "gpm", "bph"],
            "viscosity": ["cP", "PaÂ·s", "cSt"],
            "length": ["mm", "cm", "m", "in", "ft"],
            "percentage": ["%"]
        }
        
    def learn_from_text(self, text):
        print("ğŸ§  Advanced ontology learning from text...")
        
        # 1. ê¸°ë³¸ íŒ¨í„´ ì¶”ì¶œ
        basic_patterns = self._extract_basic_patterns(text)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì—”í‹°í‹° ì¸ì‹
        contextual_entities = self._extract_contextual_entities(text)
        
        # 3. ê´€ê³„ ì¶”ì¶œ
        relations = self._extract_relations(text, contextual_entities)
        
        # 4. ì‹ ë¢°ë„ ê³„ì‚°
        confidence_scores = self._calculate_confidence(basic_patterns, contextual_entities)
        
        # 5. ì˜¨í†¨ë¡œì§€ ë§¤ì¹­
        ontology_matches = self._match_to_ontology(contextual_entities)
        
        # 6. í•™ìŠµ ê²°ê³¼ ì €ì¥
        self._update_learned_knowledge(basic_patterns, contextual_entities, relations)
        
        results = {
            "patterns": basic_patterns,
            "new_entities": contextual_entities,
            "new_relations": relations,
            "matched_entities": ontology_matches,
            "confidence_scores": confidence_scores,
            "domain_insights": self._generate_domain_insights(text)
        }
        
        print(f"ğŸ“Š Enhanced learning completed:")
        print(f"   - Patterns: {len(basic_patterns)} types")
        print(f"   - Entities: {sum(len(v) for v in contextual_entities.values())} total")
        print(f"   - Relations: {len(relations)} identified")
        print(f"   - Avg Confidence: {sum(confidence_scores.values())/len(confidence_scores) if confidence_scores else 0:.2f}")
        
        return results
    
    def _extract_basic_patterns(self, text):
        """ê¸°ë³¸ íŒ¨í„´ ì¶”ì¶œ (í–¥ìƒëœ ì •ê·œí‘œí˜„ì‹)"""
        patterns = {}
        
        # í”„ë¡œì íŠ¸/ë¬¸ì„œ ë²ˆí˜¸ íŒ¨í„´
        project_patterns = [
            r'(?:Job\s*No\.?\s*:?\s*)([A-Z0-9]{2,}(?:-[A-Z0-9]+)*)',
            r'(?:Project\s*:?\s*)([A-Z0-9]{2,}(?:\s+[A-Z0-9]+)*)',
            r'(?:Doc\.?\s*No\.?\s*:?\s*)([A-Z0-9]{2,}(?:-[A-Z0-9]+)*)',
            r'\b([A-Z]\d+[A-Z]?\d*)\b'
        ]
        
        project_ids = []
        for pattern in project_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            project_ids.extend(matches)
        
        if project_ids:
            patterns["project_ids"] = list(set(project_ids))
        
        # ì¥ë¹„ ì‹ë³„ì íŒ¨í„´
        equipment_patterns = [
            r'([A-Z]-\d+(?:\s*[A-Z](?:/[A-Z])?)?)',
            r'([A-Z]{2,3}-\d+)',
            r'([A-Z]\d{3,}[A-Z]?)'
        ]
        
        equipment_ids = []
        for pattern in equipment_patterns:
            matches = re.findall(pattern, text)
            equipment_ids.extend(matches)
        
        if equipment_ids:
            patterns["equipment_ids"] = list(set(equipment_ids))
        
        # í–¥ìƒëœ ìˆ˜ì¹˜ ê°’ íŒ¨í„´
        numerical_patterns = []
        for unit_type, units in self.unit_patterns.items():
            for unit in units:
                pattern = rf'(\d+\.?\d*)\s*{re.escape(unit)}'
                matches = re.findall(pattern, text)
                for match in matches:
                    numerical_patterns.append((match, unit, unit_type))
        
        if numerical_patterns:
            patterns["numerical_values"] = numerical_patterns
        
        # ì‚¬ëŒ ì´ë¦„/ì´ë‹ˆì…œ íŒ¨í„´
        person_contexts = [
            r'(?:Checked\s+by\s*:?\s*)([A-Z]{2,4}(?:\s*/\s*[A-Z]{2,4})*)',
            r'(?:Reviewed\s+by\s*:?\s*)([A-Z]{2,4}(?:\s*/\s*[A-Z]{2,4})*)',
            r'(?:Approved\s+by\s*:?\s*)([A-Z]{2,4}(?:\s*/\s*[A-Z]{2,4})*)',
            r'(?:By\s*:?\s*)([A-Z]{2,4}(?:\s*/\s*[A-Z]{2,4})*)'
        ]
        
        person_names = []
        for pattern in person_contexts:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                names = re.split(r'\s*/\s*', match)
                person_names.extend(names)
        
        if person_names:
            patterns["person_names"] = list(set(person_names))
        
        return patterns
    
    def _extract_contextual_entities(self, text):
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = {}
        sentences = re.split(r'[.!?\n]+', text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 5:
                continue
            
            for class_name, keywords in self.domain_keywords.items():
                if any(keyword.lower() in sentence.lower() for keyword in keywords):
                    if class_name not in entities:
                        entities[class_name] = []
                    
                    specific_entities = self._extract_specific_entities(sentence, class_name)
                    entities[class_name].extend(specific_entities)
        
        # ì¤‘ë³µ ì œê±°
        for class_name in entities:
            entities[class_name] = list(set(entities[class_name]))
        
        return entities
    
    def _extract_specific_entities(self, sentence, class_name):
        """íŠ¹ì • í´ë˜ìŠ¤ì— ëŒ€í•œ êµ¬ì²´ì  ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = []
        
        if class_name == "Equipment":
            equipment_types = re.findall(r'\b(CENTRIFUGAL|PUMP|MOTOR|DRIVER|COMPRESSOR|VESSEL|TANK|HEAT\s+EXCHANGER)\b', sentence.upper())
            entities.extend(equipment_types)
            
            equipment_ids = re.findall(r'\b([A-Z]-?\d+(?:\s*[A-Z]/?)*)\b', sentence)
            entities.extend(equipment_ids)
        
        elif class_name == "ProcessRequirement":
            process_vars = re.findall(r'\b(temperature|pressure|flow|capacity|viscosity|density)\b', sentence.lower())
            entities.extend(process_vars)
            
            for unit_type, units in self.unit_patterns.items():
                for unit in units:
                    pattern = rf'(\d+\.?\d*\s*{re.escape(unit)})'
                    matches = re.findall(pattern, sentence)
                    entities.extend(matches)
        
        elif class_name == "Project":
            project_info = re.findall(r'\b([A-Z0-9]{3,}(?:-[A-Z0-9]+)*)\b', sentence)
            entities.extend(project_info)
        
        elif class_name == "Person":
            if any(word in sentence.lower() for word in ['by', 'checked', 'reviewed', 'approved']):
                names = re.findall(r'\b([A-Z]{2,4})\b', sentence)
                entities.extend(names)
        
        return entities
    
    def _extract_relations(self, text, entities):
        """ì—”í‹°í‹° ê°„ ê´€ê³„ ì¶”ì¶œ"""
        relations = []
        
        relation_patterns = {
            "hasEquipment": (r'(project|job).*?(equipment|pump|motor)', "Project", "Equipment"),
            "hasProcessReq": (r'(equipment|pump).*?(temperature|pressure|flow)', "Equipment", "ProcessRequirement"),
            "reviewedBy": (r'(revision|document).*?(?:by|checked|reviewed).*?([A-Z]{2,4})', "Revision", "Person"),
            "hasValue": (r'(\w+).*?(\d+\.?\d*\s*[a-zA-Z/%]+)', "ProcessRequirement", "Literal")
        }
        
        for relation_name, (pattern, domain_class, range_class) in relation_patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple) and len(match) >= 2:
                    relations.append({
                        "property": relation_name,
                        "domain": domain_class,
                        "range": range_class,
                        "instances": match,
                        "confidence": 0.7
                    })
        
        return relations
    
    def _calculate_confidence(self, patterns, entities):
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        confidence_scores = {}
        
        for class_name, entity_list in entities.items():
            for entity in entity_list:
                confidence = 0.5
                
                if class_name in self.domain_keywords:
                    if any(keyword in entity.lower() for keyword in self.domain_keywords[class_name]):
                        confidence += 0.2
                
                if re.match(r'^[A-Z]-?\d+', entity):
                    confidence += 0.2
                
                if re.match(r'^\d+\.?\d*\s*[a-zA-Z/%]+$', entity):
                    confidence += 0.3
                
                confidence_scores[entity] = min(confidence, 1.0)
        
        return confidence_scores
    
    def _match_to_ontology(self, entities):
        """ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆì™€ ë§¤ì¹­"""
        matches = {}
        
        for class_name, entity_list in entities.items():
            if class_name in self.classes:
                matches[class_name] = []
                for entity in entity_list:
                    match_info = {
                        "entity": entity,
                        "ontology_class": class_name,
                        "confidence": self.confidence_scores.get(entity, 0.5),
                        "properties": self._suggest_properties(entity, class_name)
                    }
                    matches[class_name].append(match_info)
        
        return matches
    
    def _suggest_properties(self, entity, class_name):
        """ì—”í‹°í‹°ì— ì í•©í•œ í”„ë¡œí¼í‹° ì œì•ˆ"""
        suggestions = []
        
        if class_name == "Equipment":
            suggestions.extend(["itemNo", "equipmentType", "capacity"])
        elif class_name == "Project":
            suggestions.extend(["jobNo", "projectName", "docNo"])
        elif class_name == "ProcessRequirement":
            suggestions.extend(["value", "unit", "condition"])
        elif class_name == "Person":
            suggestions.extend(["name", "role"])
        
        return suggestions
    
    def _generate_domain_insights(self, text):
        """ë„ë©”ì¸ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = {
            "document_type": "Industrial Equipment Specification",
            "industry_domain": "Manufacturing",
            "complexity_score": 0.7,
            "technical_density": 0.6
        }
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë³µì¡ë„ ê³„ì‚°
        complexity_keywords = ["specification", "drawing", "process", "equipment"]
        complexity_count = sum(1 for word in text.split() if word.lower() in complexity_keywords)
        
        insights["complexity_score"] += complexity_count * 0.1
        
        return insights
    
def _generate_domain_insights(self, text):
        """ë„ë©”ì¸ íŠ¹í™” ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = {
            "document_type": "technical_specification",
            "industry_domain": "process_engineering",
            "complexity_score": min(len(text) / 1000, 5.0),
            "technical_density": len(re.findall(r'\d+\.?\d*\s*[a-zA-Z/%]+', text)) / max(len(text.split()), 1)
        }
        
        return insights
    
    def _update_learned_knowledge(self, patterns, entities, relations):
        """í•™ìŠµëœ ì§€ì‹ ì—…ë°ì´íŠ¸"""
        for pattern_type, values in patterns.items():
            if pattern_type not in self.learned_patterns:
                self.learned_patterns[pattern_type] = {}
            
            for value in values:
                if isinstance(value, tuple):
                    value = str(value)
                self.learned_patterns[pattern_type][value] = self.learned_patterns[pattern_type].get(value, 0) + 1
        
        for class_name, entity_list in entities.items():
            if class_name not in self.entity_instances:
                self.entity_instances[class_name] = set()
            self.entity_instances[class_name].update(entity_list)

# PDF ì²˜ë¦¬ ë° ê·¸ë˜í”„ ìƒì„± (ê°•í™”ëœ ë²„ì „)
def process_pdf_to_graph(pdf_content, filename):
    """PDF ë‚´ìš©ì„ ì²˜ë¦¬í•˜ì—¬ ê°•í™”ëœ ì§€ì‹ ê·¸ë˜í”„ ìƒì„±"""
    print(f"ğŸ“„ Processing PDF with advanced ontology learning: {filename}")
    
    # ê°•í™”ëœ ì˜¨í†¨ë¡œì§€ í•™ìŠµ
    learning_results = ontology_learner.learn_from_text(pdf_content)
    
    # ê·¸ë˜í”„ ìƒì„±
    G = nx.DiGraph()
    node_counter = 0
    
    # ë¬¸ì„œ ë£¨íŠ¸ ë…¸ë“œ
    doc_node = f"Document_{filename.replace('.', '_')}_{node_counter}"
    G.add_node(doc_node, node_type="entity", ontology_class="Document")
    node_counter += 1
    
    # ì˜¨í†¨ë¡œì§€ ë§¤ì¹­ëœ ì—”í‹°í‹° ìš°ì„  ì²˜ë¦¬
    matched_entities = learning_results.get("matched_entities", {})
    confidence_scores = learning_results.get("confidence_scores", {})
    
    for class_name, entity_matches in matched_entities.items():
        for match_info in entity_matches[:5]:  # ìµœëŒ€ 5ê°œë§Œ
            entity = match_info["entity"]
            confidence = match_info["confidence"]
            
            # ê³ ì‹ ë¢°ë„ ì—”í‹°í‹°ë§Œ ê·¸ë˜í”„ì— ì¶”ê°€
            if confidence > 0.6:
                entity_node = f"{class_name}_{entity.replace(' ', '_').replace('/', '_')}_{node_counter}"
                G.add_node(entity_node, 
                          node_type="entity", 
                          ontology_class=class_name,
                          confidence=confidence)
                
                G.add_edge(doc_node, entity_node, relation=f"has{class_name}")
                
                # ì—”í‹°í‹° ê°’ ë¦¬í„°ëŸ´ ë…¸ë“œ
                value_node = f"Literal_{entity}_{node_counter}"
                G.add_node(value_node, 
                          node_type="literal", 
                          ontology_class="string",
                          confidence=confidence + 0.1)
                
                G.add_edge(entity_node, value_node, relation="hasValue")
                node_counter += 1
    
    # ìˆ˜ì¹˜ ê°’ íŠ¹ë³„ ì²˜ë¦¬
    patterns = learning_results.get("patterns", {})
    if "numerical_values" in patterns:
        unit_groups = {}
        for value, unit, unit_type in patterns["numerical_values"]:
            if unit_type not in unit_groups:
                unit_groups[unit_type] = []
            unit_groups[unit_type].append((value, unit))
        
        for unit_type, values in unit_groups.items():
            group_node = f"ProcessGroup_{unit_type}_{node_counter}"
            G.add_node(group_node, 
                      node_type="entity", 
                      ontology_class="ProcessRequirement",
                      confidence=0.8)
            
            G.add_edge(doc_node, group_node, relation="hasProcessReq")
            
            for value, unit in values[:3]:
                val_node = f"Value_{value}_{unit.replace('/', '_')}_{node_counter}"
                G.add_node(val_node, 
                          node_type="literal", 
                          ontology_class="decimal",
                          confidence=0.9)
                
                G.add_edge(group_node, val_node, relation="hasValue")
                node_counter += 1
    
    # ë…¸ë“œ íŠ¹ì§• ìƒì„±
    node_features = {}
    for node in G.nodes():
        node_data = G.nodes[node]
        node_features[node] = {
            "type": node_data.get("node_type", "entity"),
            "ontology_class": node_data.get("ontology_class", "Unknown"),
            "confidence": node_data.get("confidence", 0.5)
        }
    
    # ì‹ ë¢°ë„ ê¸°ë°˜ ì˜ˆì¸¡ ìƒì„±
    predictions = []
    for node in G.nodes():
        node_info = node_features[node]
        confidence = node_info["confidence"]
        node_type = node_info["type"]
        
        if confidence > 0.8:
            if node_type == "entity":
                predictions.append(1)
            elif node_type == "literal":
                predictions.append(2)
            else:
                predictions.append(0)
        else:
            predictions.append(1 if node_type == "entity" else 2)
    
    print(f"âœ… Advanced graph generated:")
    print(f"   - Nodes: {G.number_of_nodes()}")
    print(f"   - Edges: {G.number_of_edges()}")
    print(f"   - High confidence entities: {sum(1 for n in node_features.values() if n['confidence'] > 0.8)}")
    
    return G, node_features, predictions, learning_results

# ìƒ˜í”Œ ë°ì´í„° ìƒì„± í•¨ìˆ˜
def create_sample_graph():
    G = nx.DiGraph()
    
    nodes = [
        ("Project_7T04", {"node_type": "entity", "ontology_class": "Project"}),
        ("Equipment_P2105", {"node_type": "entity", "ontology_class": "Equipment"}), 
        ("ProcessReq_Temp", {"node_type": "entity", "ontology_class": "ProcessRequirement"}),
        ("7T04", {"node_type": "literal", "ontology_class": "string"}),
        ("P-2105 A/B", {"node_type": "literal", "ontology_class": "string"}),
        ("384 â„ƒ", {"node_type": "literal", "ontology_class": "decimal"})
    ]
    
    for node_id, attrs in nodes:
        G.add_node(node_id, **attrs)
    
    edges = [
        ("Project_7T04", "7T04", "hasValue"),
        ("Equipment_P2105", "P-2105 A/B", "hasValue"),
        ("ProcessReq_Temp", "384 â„ƒ", "hasValue"),
        ("Project_7T04", "Equipment_P2105", "hasEquipment"),
        ("Equipment_P2105", "ProcessReq_Temp", "hasProcessReq")
    ]
    
    for src, dst, rel in edges:
        G.add_edge(src, dst, relation=rel)
    
    node_features = {}
    for node in G.nodes():
        node_data = G.nodes[node]
        node_features[node] = {
            "type": node_data.get("node_type", "entity"),
            "ontology_class": node_data.get("ontology_class", "Unknown"),
            "confidence": 0.8 if node_data.get("node_type") == "entity" else 0.9
        }
    
    predictions = []
    for node in G.nodes():
        node_type = node_features[node]["type"]
        if node_type == "entity":
            predictions.append(1)
        elif node_type == "literal":
            predictions.append(2)
        else:
            predictions.append(0)
    
    return G, node_features, predictions

class NaturalLanguageQueryProcessor:
    def __init__(self, ontology_learner, db_manager):
        self.ontology_learner = ontology_learner
        self.db_manager = db_manager
        
        # ì¿¼ë¦¬ íŒ¨í„´ ë§¤í•‘ (ë°ì´í„°ë² ì´ìŠ¤ ì§€ì› ì¶”ê°€)
        self.query_patterns = {
            "show_all": [
                r"show\s+all\s+(\w+)",
                r"list\s+all\s+(\w+)",
                r"find\s+all\s+(\w+)",
                r"get\s+all\s+(\w+)"
            ],
            "filter_by_confidence": [
                r"(?:show|find|list)\s+(\w+)\s+with\s+(?:high|good)\s+confidence",
                r"(?:show|find|list)\s+high\s+confidence\s+(\w+)",
                r"(\w+)\s+with\s+confidence\s+>\s*(\d+\.?\d*)"
            ],
            "filter_by_class": [
                r"(?:show|find|list)\s+(\w+)\s+(?:of\s+type|class)\s+(\w+)",
                r"(\w+)\s+that\s+are\s+(\w+)",
                r"all\s+(\w+)\s+(\w+)"
            ],
            "count": [
                r"(?:how\s+many|count)\s+(\w+)",
                r"number\s+of\s+(\w+)",
                r"total\s+(\w+)"
            ],
            "stats": [
                r"(?:statistics|stats)\s+(?:for|of|about)?\s*(\w+)?",
                r"(?:summary|overview)\s+(?:of|about)?\s*(\w+)?",
                r"(?:analyze|analysis)\s+(\w+)?",
                r"database\s+(?:stats|statistics)"
            ],
            "relationships": [
                r"(?:show|find|list)\s+(?:relationships?|relations?|connections?)",
                r"what\s+is\s+connected\s+to\s+(\w+)",
                r"(\w+)\s+(?:connected|related|linked)\s+to\s+(\w+)"
            ],
            "recent": [
                r"(?:recent|latest|newest)\s+(\w+)",
                r"last\s+(\d+)\s+(\w+)",
                r"show\s+recent\s+(?:documents|files)"
            ],
            "search": [
                r"search\s+(?:for\s+)?(.+)",
                r"find\s+(.+)\s+in\s+database",
                r"lookup\s+(.+)"
            ]
        }
        
        # ë™ì˜ì–´ ë§¤í•‘
        self.synonyms = {
            "equipment": ["equipment", "machine", "device", "pump", "motor"],
            "project": ["project", "job", "document", "doc"],
            "requirement": ["requirement", "spec", "specification", "parameter"],
            "person": ["person", "people", "engineer", "reviewer", "checker"],
            "entity": ["entity", "entities", "item", "object", "node"],
            "literal": ["literal", "value", "data", "text"],
            "confidence": ["confidence", "certainty", "reliability", "accuracy"],
            "documents": ["documents", "files", "pdfs", "papers"]
        }
    
    def process_query(self, query, graph_data=None):
        """ìì—°ì–´ ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•˜ì—¬ ê²°ê³¼ ë°˜í™˜ (ë°ì´í„°ë² ì´ìŠ¤ í†µí•©)"""
        query = query.lower().strip()
        print(f"ğŸ” Processing enhanced query: '{query}'")
        
        # ì¿¼ë¦¬ íƒ€ì… ì‹ë³„
        query_type, matches = self._identify_query_type(query)
        print(f"ğŸ¯ Query type: {query_type}, matches: {matches}")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ìš°ì„  ì¿¼ë¦¬ë“¤
        if query_type in ["stats", "recent", "search"]:
            return self._handle_database_query(query_type, matches, graph_data)
        
        # ë©”ëª¨ë¦¬ ë°ì´í„° ì¿¼ë¦¬ë“¤ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
        if graph_data:
            nodes = graph_data.get("nodes", [])
            edges = graph_data.get("edges", [])
            stats = graph_data.get("stats", {})
            learning_results = graph_data.get("learning_results", {})
            
            if query_type == "show_all":
                return self._handle_show_all(matches, nodes, edges)
            elif query_type == "filter_by_confidence":
                return self._handle_confidence_filter(matches, nodes)
            elif query_type == "filter_by_class":
                return self._handle_class_filter(matches, nodes)
            elif query_type == "count":
                return self._handle_count(matches, nodes)
            elif query_type == "relationships":
                return self._handle_relationships(matches, nodes, edges)
        
        # Fallback: ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰
        return self._handle_database_fallback(query)
    
    def _handle_database_query(self, query_type, matches, graph_data):
        """ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ì¿¼ë¦¬ ì²˜ë¦¬"""
        try:
            if query_type == "stats":
                # ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ í†µê³„
                db_stats = self.db_manager.get_database_stats()
                
                stats_data = {
                    "Total Documents": db_stats.get("total_documents", 0),
                    "Total Nodes": db_stats.get("total_nodes", 0),
                    "Total Edges": db_stats.get("total_edges", 0),
                    "Total Patterns": db_stats.get("total_patterns", 0),
                    "High Confidence Entities": db_stats.get("high_confidence_entities", 0)
                }
                
                return {
                    "type": "stat",
                    "data": stats_data,
                    "message": "Database statistics across all processed documents"
                }
            
            elif query_type == "recent":
                # ìµœê·¼ ë¬¸ì„œë“¤
                recent_query = """
                    SELECT filename, upload_time, content_length, processing_time
                    FROM documents 
                    ORDER BY upload_time DESC 
                    LIMIT 10
                """
                
                result_df = self.db_manager.execute_query(recent_query)
                
                if not result_df.empty:
                    # ì‹œê°„ í˜•ì‹ ì •ë¦¬
                    result_df['upload_time'] = result_df['upload_time'].apply(
                        lambda x: x.split('T')[0] if 'T' in str(x) else str(x)
                    )
                    
                    return {
                        "type": "table",
                        "data": result_df.to_dict('records'),
                        "message": f"Found {len(result_df)} recent documents",
                        "columns": ["filename", "upload_time", "content_length", "processing_time"]
                    }
                else:
                    return {
                        "type": "table",
                        "data": [],
                        "message": "No documents found in database"
                    }
            
            elif query_type == "search":
                # í…ìŠ¤íŠ¸ ê²€ìƒ‰
                search_term = matches[0] if matches else ""
                
                search_query = f"""
                    SELECT DISTINCT n.id, n.node_type, n.ontology_class, n.confidence, d.filename
                    FROM nodes n
                    JOIN documents d ON n.document_id = d.id
                    WHERE LOWER(n.id) LIKE '%{search_term}%' 
                       OR LOWER(n.ontology_class) LIKE '%{search_term}%'
                       OR LOWER(d.filename) LIKE '%{search_term}%'
                    ORDER BY n.confidence DESC
                    LIMIT 20
                """
                
                result_df = self.db_manager.execute_query(search_query)
                
                return {
                    "type": "table",
                    "data": result_df.to_dict('records') if not result_df.empty else [],
                    "message": f"Found {len(result_df)} items matching '{search_term}'",
                    "columns": ["id", "node_type", "ontology_class", "confidence", "filename"]
                }
        
        except Exception as e:
            print(f"âŒ Database query error: {e}")
            return {
                "type": "error",
                "message": f"Database query failed: {str(e)}"
            }
    
    def _handle_database_fallback(self, query):
        """ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ fallback ê²€ìƒ‰"""
        try:
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = [word for word in query.split() if len(word) > 2]
            
            if not keywords:
                return {
                    "type": "suggestions",
                    "data": [
                        "database statistics",
                        "show recent documents", 
                        "search for equipment",
                        "count all nodes",
                        "show all entities"
                    ],
                    "message": "Try these database queries:"
                }
            
            # ë‹¤ì¤‘ í‚¤ì›Œë“œ ê²€ìƒ‰
            keyword_conditions = " OR ".join([
                f"LOWER(n.id) LIKE '%{kw}%' OR LOWER(n.ontology_class) LIKE '%{kw}%'"
                for kw in keywords[:3]  # ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œ
            ])
            
            search_query = f"""
                SELECT n.id, n.node_type, n.ontology_class, n.confidence, d.filename
                FROM nodes n
                JOIN documents d ON n.document_id = d.id
                WHERE {keyword_conditions}
                ORDER BY n.confidence DESC
                LIMIT 15
            """
            
            result_df = self.db_manager.execute_query(search_query)
            
            if not result_df.empty:
                return {
                    "type": "table",
                    "data": result_df.to_dict('records'),
                    "message": f"Found {len(result_df)} items in database matching your query",
                    "columns": ["id", "node_type", "ontology_class", "confidence", "filename"]
                }
            else:
                return {
                    "type": "suggestions",
                    "data": [
                        "database statistics",
                        "show recent documents",
                        "search for equipment",
                        "count entities"
                    ],
                    "message": "No matches found. Try these queries:"
                }
        
        except Exception as e:
            print(f"âŒ Fallback search error: {e}")
            return {
                "type": "error", 
                "message": "Search failed. Try 'database statistics' or 'show recent documents'"
            }
    
    def _identify_query_type(self, query):
        """ì¿¼ë¦¬ íƒ€ì…ê³¼ ë§¤ì¹­ëœ ê·¸ë£¹ ì‹ë³„"""
        for query_type, patterns in self.query_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, query, re.IGNORECASE)
                if match:
                    return query_type, match.groups()
        return "unknown", ()
    
    def _normalize_term(self, term):
        """ìš©ì–´ ì •ê·œí™” (ë™ì˜ì–´ ì²˜ë¦¬)"""
        if not term:
            return term
            
        term = term.lower()
        for canonical, synonyms in self.synonyms.items():
            if term in synonyms:
                return canonical
        return term
    
    def _handle_show_all(self, matches, nodes, edges):
        """'show all X' íƒ€ì… ì¿¼ë¦¬ ì²˜ë¦¬"""
        if not matches:
            return {"type": "table", "data": nodes[:10], "message": "Showing all nodes (limited to 10)"}
        
        target = self._normalize_term(matches[0])
        
        if target in ["entity", "entities"]:
            entities = [n for n in nodes if n.get("type") == "entity"]
            return {
                "type": "table", 
                "data": entities,
                "message": f"Found {len(entities)} entities",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in ["literal", "literals", "value", "values"]:
            literals = [n for n in nodes if n.get("type") == "literal"]
            return {
                "type": "table", 
                "data": literals,
                "message": f"Found {len(literals)} literals",
                "columns": ["id", "ontology_class", "confidence"]
            }
        elif target in self.synonyms.get("equipment", []):
            equipment = [n for n in nodes if n.get("ontology_class", "").lower() == "equipment"]
            return {
                "type": "table", 
                "data": equipment,
                "message": f"Found {len(equipment)} equipment entities",
                "columns": ["id", "confidence"]
            }
        elif target in self.synonyms.get("project", []):
            projects = [n for n in nodes if n.get("ontology_class", "").lower() == "project"]
            return {
                "type": "table", 
                "data": projects,
                "message": f"Found {len(projects)} project entities",
                "columns": ["id", "confidence"]
            }
        else:
            # ì¼ë°˜ì ì¸ ê²€ìƒ‰
            filtered = [n for n in nodes if target in n.get("id", "").lower() or 
                       target in n.get("ontology_class", "").lower()]
            return {
                "type": "table", 
                "data": filtered,
                "message": f"Found {len(filtered)} items matching '{target}'",
                "columns": ["id", "type", "ontology_class", "confidence"]
            }
    
    def _handle_confidence_filter(self, matches, nodes):
        """ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§"""
        if len(matches) >= 2 and matches[1].replace('.', '').isdigit():
            # íŠ¹ì • ì„ê³„ê°’ ì§€ì •ëœ ê²½ìš°
            threshold = float(matches[1])
            target = self._normalize_term(matches[0])
        else:
            # 'high confidence' ë“±ì˜ ê²½ìš°
            threshold = 0.8
            target = self._normalize_term(matches[0]) if matches else "entity"
        
        filtered_nodes = []
        for node in nodes:
            confidence = node.get("confidence", 0)
            if confidence > threshold:
                if target == "entity" and node.get("type") == "entity":
                    filtered_nodes.append(node)
                elif target in node.get("ontology_class", "").lower():
                    filtered_nodes.append(node)
                elif target in ["all", "any", ""]:
                    filtered_nodes.append(node)
        
        return {
            "type": "table",
            "data": filtered_nodes,
            "message": f"Found {len(filtered_nodes)} items with confidence > {threshold}",
            "columns": ["id", "type", "ontology_class", "confidence"]
        }
    
    def _handle_class_filter(self, matches, nodes):
        """í´ë˜ìŠ¤/íƒ€ì… ê¸°ë°˜ í•„í„°ë§"""
        if len(matches) >= 2:
            item_type = self._normalize_term(matches[0])
            class_name = self._normalize_term(matches[1])
            
            filtered = [n for n in nodes if 
                       class_name in n.get("ontology_class", "").lower() or
                       class_name in n.get("type", "").lower()]
            
            return {
                "type": "table",
                "data": filtered,
                "message": f"Found {len(filtered)} {item_type} of type {class_name}",
                "columns": ["id", "type", "ontology_class", "confidence"]
            }
        
        return {"type": "error", "message": "Could not parse class filter query"}
    
    def _handle_count(self, matches, nodes):
        """ê°œìˆ˜ ì„¸ê¸°"""
        if not matches:
            total = len(nodes)
            return {
                "type": "stat",
                "data": {"Total Nodes": total},
                "message": f"Total count: {total}"
            }
        
        target = self._normalize_term(matches[0])
        
        if target in ["entity", "entities"]:
            count = sum(1 for n in nodes if n.get("type") == "entity")
        elif target in ["literal", "literals"]:
            count = sum(1 for n in nodes if n.get("type") == "literal")
        elif target in self.synonyms.get("equipment", []):
            count = sum(1 for n in nodes if n.get("ontology_class", "").lower() == "equipment")
        elif target in self.synonyms.get("project", []):
            count = sum(1 for n in nodes if n.get("ontology_class", "").lower() == "project")
        else:
            count = sum(1 for n in nodes if target in n.get("ontology_class", "").lower())
        
        return {
            "type": "stat",
            "data": {f"{target.title()} Count": count},
            "message": f"Count of {target}: {count}"
        }
    
    def _handle_relationships(self, matches, nodes, edges):
        """ê´€ê³„ ì •ë³´ ì²˜ë¦¬"""
        if not edges:
            return {
                "type": "table",
                "data": [],
                "message": "No relationships found in the current graph"
            }
        
        # ê´€ê³„ í†µê³„
        relation_counts = {}
        for edge in edges:
            rel = edge.get("relation", "unknown")
            relation_counts[rel] = relation_counts.get(rel, 0) + 1
        
        relationship_data = [
            {"Relationship Type": rel, "Count": count}
            for rel, count in relation_counts.items()
        ]
        
        return {
            "type": "table",
            "data": relationship_data,
            "message": f"Found {len(edges)} relationships of {len(relation_counts)} types",
            "columns": ["Relationship Type", "Count"]
        }
    
# ê¸€ë¡œë²Œ ë³€ìˆ˜ (ë°ì´í„°ë² ì´ìŠ¤ í†µí•©)
ontology_learner = AdvancedOntologyLearner()
db_manager = DatabaseManager()
query_processor = NaturalLanguageQueryProcessor(ontology_learner, db_manager)
graph_data_store = {}

# Dash ì•± ìƒì„±
app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])

app.layout = dbc.Container([
    html.H1("ğŸ­ Industrial Knowledge Graph Analyzer", className="text-primary mb-4"),
    
    dbc.Row([
        # ì™¼ìª½ íŒ¨ë„
        dbc.Col([
            dbc.Card([
                dbc.CardBody([
                    html.H5("ğŸ“„ Document Upload"),
                    dcc.Upload(
                        id="upload-pdf",
                        children=html.Div([
                            "Drag and drop PDF files or ",
                            html.A("click to select", style={"color": "#007bff"})
                        ]),
                        style={
                            'width': '100%',
                            'height': '60px',
                            'lineHeight': '60px',
                            'borderWidth': '2px',
                            'borderStyle': 'dashed',
                            'borderRadius': '5px',
                            'textAlign': 'center',
                            'backgroundColor': '#f8f9fa'
                        },
                        multiple=False
                    )
                ])
            ], width=4),
        
        # ì˜¤ë¥¸ìª½ íŒ¨ë„
        dbc.Col([
            # ìì—°ì–´ ì¿¼ë¦¬ ì¸í„°í˜ì´ìŠ¤ ì¶”ê°€
            dbc.Card([
                dbc.CardBody([
                    html.H5("ğŸ—£ï¸ Natural Language Query"),
                    dbc.InputGroup([
                        dbc.Input(
                            id="nl-query-input",
                            placeholder="e.g., 'show all equipment with high confidence', 'database statistics', 'search for pump'",
                            type="text"
                        ),
                        dbc.Button("ğŸ” Query", id="query-btn", color="primary")
                    ]),
                    html.Div(id="query-output", className="mt-3")
                ])
            ], className="mb-4"),
            
            html.H4("ğŸ“Š Node Classification"),
            dcc.Graph(id="pie-chart", style={'height': '400px'}),
            
            html.Hr(),
            
            html.H4("ğŸ•¸ï¸ Knowledge Graph"),
            dcc.Graph(id="network-graph", style={'height': '500px'})
        ], width=8)
    ]),
    
    # ìˆ¨ê²¨ì§„ ë°ì´í„° ì €ì¥ì†Œ
    dcc.Store(id="data-store", data={})
], fluid=True)

# ì½œë°±ë“¤
@app.callback(
    [Output("status-output", "children"), Output("data-store", "data")],
    [Input("test-btn", "n_clicks"), Input("sample-btn", "n_clicks"), Input("upload-pdf", "contents")],
    [State("upload-pdf", "filename")],
    prevent_initial_call=True
)
def handle_all_inputs(test_clicks, sample_clicks, pdf_contents, pdf_filename):
    from dash import ctx
    
    trigger_id = ctx.triggered[0]['prop_id'].split('.')[0] if ctx.triggered else None
    print(f"ğŸ”¥ Input callback triggered: {trigger_id}")
    
    if trigger_id == "test-btn":
        print("âœ… Test button clicked")
        return dbc.Alert("âœ… Connection test successful! Database ready.", color="success"), {}
    
    elif trigger_id == "sample-btn":
        print("ğŸ“Š Sample button clicked - generating data...")
        
        G, node_features, predictions = create_sample_graph()
        
        graph_data = {
            "nodes": [{"id": node, **node_features[node]} for node in G.nodes()],
            "edges": [{"source": u, "target": v, "relation": data.get("relation", "unknown")} 
                     for u, v, data in G.edges(data=True)],
            "predictions": predictions,
            "stats": {
                "total_nodes": G.number_of_nodes(),
                "total_edges": G.number_of_edges(),
                "source": "sample"
            }
        }
        
        print(f"âœ… Sample data created: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
        
        message = dbc.Alert([
            html.H5("âœ… Sample data loaded!", className="alert-heading"),
            html.P(f"Created {G.number_of_nodes()} nodes and {G.number_of_edges()} edges"),
            html.P("Click 'Update Charts' to see visualizations!")
        ], color="success")
        
        return message, graph_data
    
    elif trigger_id == "upload-pdf" and pdf_contents:
        print(f"ğŸ“„ PDF uploaded: {pdf_filename}")
        
        try:
            if not pdf_filename or not pdf_filename.lower().endswith('.pdf'):
                return dbc.Alert("âŒ Please select a PDF file", color="warning"), {}
            
            content_type, content_string = pdf_contents.split(',')
            decoded = base64.b64decode(content_string)
            
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(decoded)
                tmp_file_path = tmp_file.name
            
            try:
                # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
                pdf_text = extract_pdf_text(tmp_file_path)
                print(f"ğŸ“ Extracted {len(pdf_text)} characters from PDF")
                
                if len(pdf_text.strip()) < 10:
                    return dbc.Alert("âŒ No meaningful text found in PDF", color="warning"), {}
                
                # ê·¸ë˜í”„ ìƒì„± (í–¥ìƒëœ ë²„ì „)
                import time
                start_time = time.time()
                
                G, node_features, predictions, learning_results = process_pdf_to_graph(pdf_text, pdf_filename)
                
                # ê·¸ë˜í”„ ë°ì´í„° êµ¬ì„±
                graph_data = {
                    "nodes": [{"id": node, **node_features[node]} for node in G.nodes()],
                    "edges": [{"source": u, "target": v, "relation": data.get("relation", "unknown")} 
                             for u, v, data in G.edges(data=True)],
                    "predictions": predictions,
                    "learning_results": learning_results,
                    "stats": {
                        "total_nodes": G.number_of_nodes(),
                        "total_edges": G.number_of_edges(),
                        "source": "pdf",
                        "filename": pdf_filename,
                        "text_length": len(pdf_text),
                        "patterns_found": len(learning_results.get("patterns", {})),
                        "entities_found": sum(len(v) for v in learning_results.get("new_entities", {}).values()),
                        "relations_found": len(learning_results.get("new_relations", [])),
                        "avg_confidence": sum(learning_results.get("confidence_scores", {}).values()) / 
                                        max(len(learning_results.get("confidence_scores", {})), 1),
                        "domain_insights": learning_results.get("domain_insights", {})
                    }
                }
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                processing_time = time.time() - start_time
                doc_id = db_manager.save_processing_results(
                    pdf_filename, pdf_text, graph_data, learning_results, 
                    processing_time=processing_time
                )
                
                print(f"âœ… PDF processed and saved to database: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
                
                # ìƒì„¸í•œ ì„±ê³µ ë©”ì‹œì§€
                stats = graph_data["stats"]
                domain_insights = stats.get("domain_insights", {})
                
                message = dbc.Alert([
                    html.H5(f"âœ… PDF '{pdf_filename}' processed with advanced ontology learning!", className="alert-heading"),
                    dbc.Row([
                        dbc.Col([
                            html.P(f"ğŸ“Š Text: {stats['text_length']} characters", className="mb-1"),
                            html.P(f"ğŸ¯ Patterns: {stats['patterns_found']} types", className="mb-1"),
                            html.P(f"ğŸ·ï¸ Entities: {stats['entities_found']} found", className="mb-1"),
                        ], width=6),
                        dbc.Col([
                            html.P(f"ğŸ”— Relations: {stats['relations_found']} identified", className="mb-1"),
                            html.P(f"ğŸ•¸ï¸ Graph: {stats['total_nodes']} nodes, {stats['total_edges']} edges", className="mb-1"),
                            html.P(f"ğŸ–ï¸ Confidence: {stats['avg_confidence']:.2f}", className="mb-1"),
                        ], width=6),
                    ]),
                    html.Hr(),
                    html.P([
                        "ğŸ§  Domain: ", 
                        html.Strong(domain_insights.get("industry_domain", "Unknown")),
                        f" | Complexity: {domain_insights.get('complexity_score', 0):.1f}/5.0",
                        f" | DB ID: {doc_id}" if doc_id else ""
                    ], className="mb-2"),
                    html.P("ğŸ’¾ Results saved to database! Try querying: 'database statistics' or 'show recent documents'", className="mb-0")
                ], color="success")
                
                return message, graph_data
                
            finally:
                if os.path.exists(tmp_file_path):
                    os.unlink(tmp_file_path)
        
        except Exception as e:
            print(f"âŒ PDF processing error: {e}")
            return dbc.Alert(f"âŒ Error processing PDF: {str(e)}", color="danger"), {}
    
    return "", {} className="mb-3"),
            
            dbc.Card([
                dbc.CardBody([
                    html.H5("ğŸ¯ Quick Actions"),
                    dbc.Button("ğŸ§ª Test Connection", id="test-btn", color="secondary", className="w-100 mb-2", size="sm"),
                    dbc.Button("ğŸ“Š Load Sample Data", id="sample-btn", color="info", className="w-100 mb-2", size="sm"),
                    dbc.Button("ğŸ”„ Update Charts", id="update-btn", color="warning", className="w-100 mb-2", size="sm"),
                ])
            ], className="mb-4"),
            
            html.Div(id="status-output", className="mb-4"),
            
            dbc.Card([
                dbc.CardBody([
                    html.H6("ğŸ“ˆ System Info"),
                    html.P("âœ… Ontology Classes: 7", className="mb-1"),
                    html.P("âœ… Object Properties: 7", className="mb-1"),
                    html.P("âœ… Datatype Properties: 6", className="mb-1"),
                    html.P("ğŸ§  Advanced Learning: Enabled", className="mb-1"),
                    html.P("ğŸ’¾ Database: Connected", className="mb-0"),
                ])
            ]),
            
            # í•™ìŠµ í†µê³„ ì¹´ë“œ ì¶”ê°€
            html.Div(id="learning-stats", className="mt-3")
        ], width=4)

@app.callback(
    Output("pie-chart", "figure"),
    [Input("update-btn", "n_clicks")],
    [State("data-store", "data")]
)
def update_pie_chart(n_clicks, stored_data):
    print(f"ğŸ¯ Pie chart callback - clicks: {n_clicks}, data: {bool(stored_data)}")
    
    if n_clicks and stored_data and "predictions" in stored_data:
        predictions = stored_data["predictions"]
        pred_counts = Counter(predictions)
        
        labels = ["Class", "Entity", "Literal"]
        values = [pred_counts.get(i, 0) for i in range(3)]
        colors = ["#ff9999", "#66b3ff", "#99ff99"]
        
        fig = go.Figure()
        fig.add_trace(go.Pie(
            labels=labels,
            values=values,
            hole=0.3,
            textinfo="label+percent+value",
            marker=dict(colors=colors)
        ))
        
        fig.update_layout(
            title=f"Node Classification Results (Total: {sum(values)})",
            height=400,
            showlegend=True
        )
        
        print(f"âœ… Pie chart created: {values}")
        return fig
    
    return go.Figure().add_annotation(
        text="Load sample data or upload PDF, then click 'Update Charts'",
        showarrow=False, x=0.5, y=0.5
    )

@app.callback(
    Output("network-graph", "figure"),
    [Input("update-btn", "n_clicks")],
    [State("data-store", "data")]
)
def update_network_graph(n_clicks, stored_data):
    print(f"ğŸ•¸ï¸ Network graph callback - clicks: {n_clicks}, data: {bool(stored_data)}")
    
    if n_clicks and stored_data and "nodes" in stored_data:
        nodes = stored_data["nodes"]
        edges = stored_data["edges"]
        
        G = nx.DiGraph()
        for node in nodes:
            G.add_node(node["id"])
        for edge in edges:
            G.add_edge(edge["source"], edge["target"])
        
        pos = nx.spring_layout(G, k=2, iterations=50)
        
        edge_x, edge_y = [], []
        for edge in G.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
        
        node_x = [pos[node][0] for node in G.nodes()]
        node_y = [pos[node][1] for node in G.nodes()]
        node_text = [node[:15] + "..." if len(node) > 15 else node for node in G.nodes()]
        
        # ë…¸ë“œ ìƒ‰ìƒ ë° í¬ê¸° (ì‹ ë¢°ë„ ê¸°ë°˜)
        node_colors = []
        node_sizes = []
        hover_text = []
        
        for node in G.nodes():
            node_info = next((n for n in nodes if n["id"] == node), {})
            confidence = node_info.get("confidence", 0.5)
            node_type = node_info.get("type", "unknown")
            ontology_class = node_info.get("ontology_class", "Unknown")
            
            # ì‹ ë¢°ë„ì— ë”°ë¥¸ í¬ê¸°
            node_sizes.append(15 + confidence * 25)
            
            # íƒ€ì…ì— ë”°ë¥¸ ìƒ‰ìƒ
            if node_type == "entity":
                base_color = [70, 130, 180]  # ìŠ¤í‹¸ë¸”ë£¨
            elif node_type == "literal":
                base_color = [60, 179, 113]  # ë¯¸ë””ì—„ì”¨ê·¸ë¦°
            else:
                base_color = [255, 99, 71]   # í† ë§ˆí† 
            
            # ì‹ ë¢°ë„ì— ë”°ë¥¸ íˆ¬ëª…ë„
            alpha = 0.4 + confidence * 0.6
            color = f"rgba({base_color[0]}, {base_color[1]}, {base_color[2]}, {alpha})"
            node_colors.append(color)
            
            # í˜¸ë²„ í…ìŠ¤íŠ¸
            hover_text.append(
                f"<b>{node}</b><br>" +
                f"Type: {node_type}<br>" +
                f"Class: {ontology_class}<br>" +
                f"Confidence: {confidence:.3f}"
            )
        
        fig = go.Figure()
        
        # ì—£ì§€ ì¶”ê°€
        fig.add_trace(go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=1.5, color='rgba(125,125,125,0.5)'),
            mode='lines',
            showlegend=False,
            hoverinfo='none'
        ))
        
        # ë…¸ë“œ ì¶”ê°€ (ì‹ ë¢°ë„ ê¸°ë°˜ ì‹œê°í™”)
        fig.add_trace(go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            text=node_text,
            textposition="middle center",
            textfont=dict(size=8, color="white"),
            marker=dict(
                size=node_sizes,
                color=node_colors,
                line=dict(width=1, color='rgba(50,50,50,0.8)')
            ),
            showlegend=False,
            hoverinfo='text',
            hovertext=hover_text
        ))
        
        fig.update_layout(
            title=f"Advanced Knowledge Graph ({G.number_of_nodes()} nodes, {G.number_of_edges()} edges)<br><sub>Node size = confidence, Color = type (Blue=Entity, Green=Literal, Red=Other)</sub>",
            height=500,
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            showlegend=False,
            plot_bgcolor='rgba(240,240,240,0.2)',
            margin=dict(l=20, r=20, t=80, b=20)
        )
        
        print(f"âœ… Enhanced network graph created: {G.number_of_nodes()} nodes")
        return fig
    
    return go.Figure().add_annotation(
        text="Load sample data or upload PDF, then click 'Update Charts'",
        showarrow=False, x=0.5, y=0.5
    )

@app.callback(
    Output("learning-stats", "children"),
    [Input("data-store", "data")]
)
def update_learning_stats(stored_data):
    """ì˜¨í†¨ë¡œì§€ í•™ìŠµ í†µê³„ í‘œì‹œ"""
    if not stored_data or "learning_results" not in stored_data:
        return ""
    
    learning_results = stored_data["learning_results"]
    stats = stored_data.get("stats", {})
    
    confidence_scores = learning_results.get("confidence_scores", {})
    domain_insights = learning_results.get("domain_insights", {})
    
    if not confidence_scores and not domain_insights:
        return ""
    
    # ì‹ ë¢°ë„ ë¶„í¬
    high_conf = sum(1 for c in confidence_scores.values() if c > 0.8)
    med_conf = sum(1 for c in confidence_scores.values() if 0.5 < c <= 0.8)
    low_conf = sum(1 for c in confidence_scores.values() if c <= 0.5)
    
    return dbc.Card([
        dbc.CardBody([
            html.H6("ğŸ§  Learning Analytics", className="card-title"),
            html.P(f"ğŸ–ï¸ High Confidence: {high_conf}", className="mb-1"),
            html.P(f"ğŸƒ Medium Confidence: {med_conf}", className="mb-1"),
            html.P(f"ğŸ¤” Low Confidence: {low_conf}", className="mb-1"),
            html.Hr(className="my-2"),
            html.P(f"ğŸ“ˆ Technical Density: {domain_insights.get('technical_density', 0):.3f}", className="mb-1"),
            html.P(f"ğŸ”¬ Document Type: {domain_insights.get('document_type', 'Unknown')}", className="mb-0"),
        ])
    ], className="border-info")

@app.callback(
    Output("query-output", "children"),
    [Input("query-btn", "n_clicks")],
    [State("nl-query-input", "value"), State("data-store", "data")]
)
def handle_natural_language_query(n_clicks, query_text, stored_data):
    """ìì—°ì–´ ì¿¼ë¦¬ ì²˜ë¦¬ (ë°ì´í„°ë² ì´ìŠ¤ í†µí•©)"""
    if not n_clicks or not query_text:
        return html.Div([
            html.P("ğŸ’¡ Query Examples:", className="mb-2 font-weight-bold"),
            html.Ul([
                html.Li("database statistics"),
                html.Li("show recent documents"),
                html.Li("search for equipment"),
                html.Li("show all entities with high confidence"),
                html.Li("count all nodes"),
                html.Li("show relationships")
            ], className="mb-0")
        ])
    
    print(f"ğŸ—£ï¸ Natural language query: '{query_text}'")
    
    try:
        # ì¿¼ë¦¬ ì²˜ë¦¬ (ë°ì´í„°ë² ì´ìŠ¤ ìš°ì„ )
        result = query_processor.process_query(query_text, stored_data)
        
        # ê²°ê³¼ ë Œë”ë§
        return render_query_result(result)
        
    except Exception as e:
        print(f"âŒ Query processing error: {e}")
        return dbc.Alert(f"âŒ Error processing query: {str(e)}", color="danger")

def render_query_result(result):
    """ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ë Œë”ë§"""
    result_type = result.get("type", "error")
    message = result.get("message", "")
    data = result.get("data", [])
    
    components = [
        html.P(f"âœ… {message}", className="text-success font-weight-bold mb-3")
    ]
    
    if result_type == "table" and data:
        # í…Œì´ë¸” í˜•íƒœë¡œ ê²°ê³¼ í‘œì‹œ
        columns = result.get("columns", [])
        if not columns and data:
            columns = list(data[0].keys()) if data else []
        
        # ë°ì´í„° ì •ë¦¬ (ì†Œìˆ˜ì  ë°˜ì˜¬ë¦¼)
        clean_data = []
        for item in data[:20]:  # ìµœëŒ€ 20ê°œë§Œ í‘œì‹œ
            clean_item = {}
            for key, value in item.items():
                if isinstance(value, float):
                    clean_item[key] = round(value, 3)
                else:
                    clean_item[key] = str(value)[:50]  # ê¸¸ì´ ì œí•œ
            clean_data.append(clean_item)
        
        if clean_data:
            df = pd.DataFrame(clean_data)
            table = dbc.Table.from_dataframe(
                df, 
                striped=True, 
                bordered=True, 
                hover=True, 
                responsive=True,
                size="sm"
            )
            components.append(table)
            
            if len(data) > 20:
                components.append(
                    html.P(f"... and {len(data) - 20} more items", className="text-muted")
                )
    
    elif result_type == "stat" and data:
        # í†µê³„ í˜•íƒœë¡œ ê²°ê³¼ í‘œì‹œ
        stat_cards = []
        for key, value in data.items():
            stat_cards.append(
                dbc.Col([
                    dbc.Card([
                        dbc.CardBody([
                            html.H5(str(value), className="text-primary"),
                            html.P(key, className="mb-0")
                        ])
                    ], className="text-center")
                ], width=3)
            )
        
        components.append(dbc.Row(stat_cards[:4]))  # ìµœëŒ€ 4ê°œì”©
        
        if len(stat_cards) > 4:
            components.append(dbc.Row(stat_cards[4:8], className="mt-2"))
    
    elif result_type == "suggestions" and data:
        # ì œì•ˆì‚¬í•­ í‘œì‹œ
        components.extend([
            html.P("ğŸ’¡ Try these queries:", className="font-weight-bold"),
            html.Ul([html.Li(suggestion) for suggestion in data])
        ])
    
    elif result_type == "error":
        components = [dbc.Alert(f"âŒ {message}", color="danger")]
    
    else:
        components.append(html.P("No results found."))
    
    return html.Div(components)

if __name__ == "__main__":
    print("ğŸš€ Starting Advanced Industrial Knowledge Graph Analyzer...")
    print("ğŸ§  Enhanced ontology learning enabled!")
    print("ğŸ’¾ Database system integrated!")
    print("ğŸ” Natural language queries supported!")
    print("ğŸŒ Open http://127.0.0.1:8050 in your browser")
    
    app.run(debug=True, port=8050)

    